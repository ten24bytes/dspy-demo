{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2759f4",
   "metadata": {},
   "source": [
    "# Building AI Applications by Customizing DSPy Modules\n",
    "\n",
    "This notebook demonstrates how to create custom DSPy modules for specialized AI applications. We'll cover:\n",
    "\n",
    "1. **Understanding DSPy Modules**: Basic concepts and architecture\n",
    "2. **Creating Custom Modules**: Building from scratch\n",
    "3. **Composing Modules**: Combining modules for complex workflows\n",
    "4. **State Management**: Handling state across module calls\n",
    "5. **Error Handling**: Robust error handling in custom modules\n",
    "6. **Performance Optimization**: Best practices for efficiency\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of DSPy concepts\n",
    "- Python programming experience\n",
    "- Familiarity with language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure DSPy with OpenAI (or your preferred model)\n",
    "# Make sure to set your OPENAI_API_KEY environment variable\n",
    "lm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv('OPENAI_API_KEY'))\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec0f303",
   "metadata": {},
   "source": [
    "## 1. Understanding DSPy Modules\n",
    "\n",
    "DSPy modules are the building blocks of AI applications. They encapsulate:\n",
    "- **Signatures**: Define input/output interfaces\n",
    "- **Logic**: Processing and transformation logic\n",
    "- **State**: Maintain context between calls\n",
    "- **Optimization**: Can be optimized with DSPy optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic module structure\n",
    "class BasicModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize any sub-modules or components\n",
    "        self.predictor = dspy.Predict(\"question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        # Main processing logic\n",
    "        result = self.predictor(question=question)\n",
    "        return result\n",
    "\n",
    "# Test the basic module\n",
    "basic_module = BasicModule()\n",
    "result = basic_module(\"What is machine learning?\")\n",
    "print(f\"Answer: {result.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42671c22",
   "metadata": {},
   "source": [
    "## 2. Creating Custom Modules\n",
    "\n",
    "Let's create more sophisticated custom modules for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a30bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures for our modules\n",
    "@dataclass\n",
    "class ProcessingResult:\n",
    "    content: str\n",
    "    confidence: float\n",
    "    metadata: Dict[str, Any]\n",
    "    timestamp: datetime\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    is_valid: bool\n",
    "    issues: List[str]\n",
    "    suggestions: List[str]\n",
    "\n",
    "# Content Analysis Module\n",
    "class ContentAnalyzer(dspy.Module):\n",
    "    \"\"\"Analyzes content for sentiment, topics, and quality.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sentiment_analyzer = dspy.ChainOfThought(\"text -> sentiment, confidence_score\")\n",
    "        self.topic_extractor = dspy.ChainOfThought(\"text -> topics, relevance_scores\")\n",
    "        self.quality_assessor = dspy.ChainOfThought(\"text -> quality_score, quality_factors\")\n",
    "    \n",
    "    def forward(self, text: str) -> ProcessingResult:\n",
    "        # Analyze sentiment\n",
    "        sentiment_result = self.sentiment_analyzer(text=text)\n",
    "        \n",
    "        # Extract topics\n",
    "        topic_result = self.topic_extractor(text=text)\n",
    "        \n",
    "        # Assess quality\n",
    "        quality_result = self.quality_assessor(text=text)\n",
    "        \n",
    "        # Combine results\n",
    "        metadata = {\n",
    "            'sentiment': sentiment_result.sentiment,\n",
    "            'sentiment_confidence': float(sentiment_result.confidence_score),\n",
    "            'topics': topic_result.topics.split(', ') if hasattr(topic_result, 'topics') else [],\n",
    "            'quality_score': float(quality_result.quality_score),\n",
    "            'quality_factors': quality_result.quality_factors.split(', ') if hasattr(quality_result, 'quality_factors') else []\n",
    "        }\n",
    "        \n",
    "        return ProcessingResult(\n",
    "            content=text,\n",
    "            confidence=metadata['sentiment_confidence'],\n",
    "            metadata=metadata,\n",
    "            timestamp=datetime.now()\n",
    "        )\n",
    "\n",
    "# Test the content analyzer\n",
    "analyzer = ContentAnalyzer()\n",
    "sample_text = \"I absolutely love this new AI technology! It's revolutionizing how we approach data science and machine learning.\"\n",
    "result = analyzer(sample_text)\n",
    "\n",
    "print(f\"Content: {result.content[:50]}...\")\n",
    "print(f\"Confidence: {result.confidence}\")\n",
    "print(f\"Sentiment: {result.metadata['sentiment']}\")\n",
    "print(f\"Topics: {result.metadata['topics']}\")\n",
    "print(f\"Quality Score: {result.metadata['quality_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d45905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content Validator Module\n",
    "class ContentValidator(dspy.Module):\n",
    "    \"\"\"Validates content for various criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_rules: List[str] = None):\n",
    "        super().__init__()\n",
    "        self.validation_rules = validation_rules or [\n",
    "            \"appropriate language\",\n",
    "            \"factual accuracy\",\n",
    "            \"coherence\",\n",
    "            \"completeness\"\n",
    "        ]\n",
    "        \n",
    "        # Create signature for validation\n",
    "        rules_str = \", \".join(self.validation_rules)\n",
    "        self.validator = dspy.ChainOfThought(\n",
    "            f\"text, rules: {rules_str} -> is_valid: bool, issues: list, suggestions: list\"\n",
    "        )\n",
    "    \n",
    "    def forward(self, text: str) -> ValidationResult:\n",
    "        rules_str = \", \".join(self.validation_rules)\n",
    "        result = self.validator(text=text, rules=rules_str)\n",
    "        \n",
    "        # Parse the result\n",
    "        is_valid = result.is_valid.lower() in ['true', 'yes', '1'] if hasattr(result, 'is_valid') else True\n",
    "        \n",
    "        issues = []\n",
    "        if hasattr(result, 'issues') and result.issues:\n",
    "            issues = result.issues.split(', ') if isinstance(result.issues, str) else []\n",
    "        \n",
    "        suggestions = []\n",
    "        if hasattr(result, 'suggestions') and result.suggestions:\n",
    "            suggestions = result.suggestions.split(', ') if isinstance(result.suggestions, str) else []\n",
    "        \n",
    "        return ValidationResult(\n",
    "            is_valid=is_valid,\n",
    "            issues=issues,\n",
    "            suggestions=suggestions\n",
    "        )\n",
    "\n",
    "# Test the validator\n",
    "validator = ContentValidator()\n",
    "validation_result = validator(sample_text)\n",
    "\n",
    "print(f\"Is Valid: {validation_result.is_valid}\")\n",
    "print(f\"Issues: {validation_result.issues}\")\n",
    "print(f\"Suggestions: {validation_result.suggestions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c87e965",
   "metadata": {},
   "source": [
    "## 3. Composing Modules\n",
    "\n",
    "Now let's create a composite module that combines multiple custom modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content Enhancement Module\n",
    "class ContentEnhancer(dspy.Module):\n",
    "    \"\"\"Enhances content based on analysis and validation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enhancer = dspy.ChainOfThought(\n",
    "            \"original_text, issues, suggestions -> enhanced_text, improvements_made\"\n",
    "        )\n",
    "    \n",
    "    def forward(self, text: str, validation_result: ValidationResult) -> str:\n",
    "        if validation_result.is_valid:\n",
    "            return text  # No enhancement needed\n",
    "        \n",
    "        issues_str = \"; \".join(validation_result.issues)\n",
    "        suggestions_str = \"; \".join(validation_result.suggestions)\n",
    "        \n",
    "        result = self.enhancer(\n",
    "            original_text=text,\n",
    "            issues=issues_str,\n",
    "            suggestions=suggestions_str\n",
    "        )\n",
    "        \n",
    "        return result.enhanced_text if hasattr(result, 'enhanced_text') else text\n",
    "\n",
    "# Comprehensive Content Processing Pipeline\n",
    "class ContentProcessingPipeline(dspy.Module):\n",
    "    \"\"\"Complete content processing pipeline combining analysis, validation, and enhancement.\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_rules: List[str] = None):\n",
    "        super().__init__()\n",
    "        self.analyzer = ContentAnalyzer()\n",
    "        self.validator = ContentValidator(validation_rules)\n",
    "        self.enhancer = ContentEnhancer()\n",
    "        \n",
    "        # Pipeline state\n",
    "        self.processing_history = []\n",
    "    \n",
    "    def forward(self, text: str, enhance_if_needed: bool = True) -> Dict[str, Any]:\n",
    "        # Step 1: Analyze content\n",
    "        analysis_result = self.analyzer(text)\n",
    "        \n",
    "        # Step 2: Validate content\n",
    "        validation_result = self.validator(text)\n",
    "        \n",
    "        # Step 3: Enhance if needed and requested\n",
    "        final_text = text\n",
    "        if enhance_if_needed and not validation_result.is_valid:\n",
    "            final_text = self.enhancer(text, validation_result)\n",
    "        \n",
    "        # Compile results\n",
    "        pipeline_result = {\n",
    "            'original_text': text,\n",
    "            'final_text': final_text,\n",
    "            'was_enhanced': final_text != text,\n",
    "            'analysis': analysis_result,\n",
    "            'validation': validation_result,\n",
    "            'processing_timestamp': datetime.now()\n",
    "        }\n",
    "        \n",
    "        # Store in history\n",
    "        self.processing_history.append(pipeline_result)\n",
    "        \n",
    "        return pipeline_result\n",
    "    \n",
    "    def get_processing_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about processed content.\"\"\"\n",
    "        if not self.processing_history:\n",
    "            return {'total_processed': 0}\n",
    "        \n",
    "        total = len(self.processing_history)\n",
    "        enhanced = sum(1 for result in self.processing_history if result['was_enhanced'])\n",
    "        avg_confidence = sum(\n",
    "            result['analysis'].confidence for result in self.processing_history\n",
    "        ) / total\n",
    "        \n",
    "        return {\n",
    "            'total_processed': total,\n",
    "            'enhanced_count': enhanced,\n",
    "            'enhancement_rate': enhanced / total,\n",
    "            'average_confidence': avg_confidence\n",
    "        }\n",
    "\n",
    "# Test the complete pipeline\n",
    "pipeline = ContentProcessingPipeline()\n",
    "\n",
    "# Test with different types of content\n",
    "test_texts = [\n",
    "    \"This AI is amazing and will change everything!\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"Machine learning algorithms can be complex but are very powerful tools for data analysis.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    result = pipeline(text)\n",
    "    \n",
    "    print(f\"Original: {result['original_text']}\")\n",
    "    print(f\"Enhanced: {result['was_enhanced']}\")\n",
    "    if result['was_enhanced']:\n",
    "        print(f\"Final: {result['final_text']}\")\n",
    "    \n",
    "    print(f\"Sentiment: {result['analysis'].metadata['sentiment']}\")\n",
    "    print(f\"Valid: {result['validation'].is_valid}\")\n",
    "\n",
    "print(f\"\\n--- Pipeline Statistics ---\")\n",
    "stats = pipeline.get_processing_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6413d17",
   "metadata": {},
   "source": [
    "## 4. State Management\n",
    "\n",
    "Advanced modules often need to maintain state across multiple calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stateful Conversation Module\n",
    "class ConversationManager(dspy.Module):\n",
    "    \"\"\"Manages conversation state and context.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_history: int = 10):\n",
    "        super().__init__()\n",
    "        self.max_history = max_history\n",
    "        self.conversation_history = []\n",
    "        self.conversation_summary = \"\"\n",
    "        \n",
    "        # Modules for conversation management\n",
    "        self.responder = dspy.ChainOfThought(\n",
    "            \"conversation_history, current_message -> response, confidence\"\n",
    "        )\n",
    "        self.summarizer = dspy.ChainOfThought(\n",
    "            \"conversation_history -> summary\"\n",
    "        )\n",
    "    \n",
    "    def forward(self, message: str, user_id: str = \"user\") -> Dict[str, Any]:\n",
    "        # Add message to history\n",
    "        self.conversation_history.append({\n",
    "            'user_id': user_id,\n",
    "            'message': message,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "        \n",
    "        # Maintain history size\n",
    "        if len(self.conversation_history) > self.max_history:\n",
    "            # Summarize older conversation\n",
    "            old_messages = self.conversation_history[:-self.max_history//2]\n",
    "            old_text = \"\\n\".join([f\"{msg['user_id']}: {msg['message']}\" for msg in old_messages])\n",
    "            \n",
    "            summary_result = self.summarizer(conversation_history=old_text)\n",
    "            self.conversation_summary = summary_result.summary if hasattr(summary_result, 'summary') else \"\"\n",
    "            \n",
    "            # Keep recent messages\n",
    "            self.conversation_history = self.conversation_history[-self.max_history//2:]\n",
    "        \n",
    "        # Prepare context for response\n",
    "        context_parts = []\n",
    "        if self.conversation_summary:\n",
    "            context_parts.append(f\"Previous conversation summary: {self.conversation_summary}\")\n",
    "        \n",
    "        recent_messages = \"\\n\".join([\n",
    "            f\"{msg['user_id']}: {msg['message']}\" \n",
    "            for msg in self.conversation_history[-5:]  # Last 5 messages\n",
    "        ])\n",
    "        context_parts.append(f\"Recent conversation:\\n{recent_messages}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate response\n",
    "        response_result = self.responder(\n",
    "            conversation_history=context,\n",
    "            current_message=message\n",
    "        )\n",
    "        \n",
    "        response = response_result.response if hasattr(response_result, 'response') else \"I understand.\"\n",
    "        confidence = float(response_result.confidence) if hasattr(response_result, 'confidence') else 0.8\n",
    "        \n",
    "        # Add response to history\n",
    "        self.conversation_history.append({\n",
    "            'user_id': 'assistant',\n",
    "            'message': response,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'response': response,\n",
    "            'confidence': confidence,\n",
    "            'conversation_length': len(self.conversation_history),\n",
    "            'has_summary': bool(self.conversation_summary)\n",
    "        }\n",
    "    \n",
    "    def get_conversation_state(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current conversation state.\"\"\"\n",
    "        return {\n",
    "            'history_length': len(self.conversation_history),\n",
    "            'summary_exists': bool(self.conversation_summary),\n",
    "            'last_message_time': self.conversation_history[-1]['timestamp'] if self.conversation_history else None\n",
    "        }\n",
    "    \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Reset conversation state.\"\"\"\n",
    "        self.conversation_history.clear()\n",
    "        self.conversation_summary = \"\"\n",
    "\n",
    "# Test the conversation manager\n",
    "conv_manager = ConversationManager(max_history=6)\n",
    "\n",
    "# Simulate a conversation\n",
    "conversation_flow = [\n",
    "    \"Hello, I'm interested in learning about machine learning.\",\n",
    "    \"Can you explain what neural networks are?\",\n",
    "    \"How do they differ from traditional algorithms?\",\n",
    "    \"What are some practical applications?\",\n",
    "    \"Thank you for the explanations!\"\n",
    "]\n",
    "\n",
    "for message in conversation_flow:\n",
    "    print(f\"\\nUser: {message}\")\n",
    "    result = conv_manager(message)\n",
    "    print(f\"Assistant: {result['response']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "    \n",
    "    state = conv_manager.get_conversation_state()\n",
    "    print(f\"State: {state['history_length']} messages, Summary: {state['summary_exists']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1b92c",
   "metadata": {},
   "source": [
    "## 5. Error Handling and Robustness\n",
    "\n",
    "Building robust modules with proper error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust processing module with error handling\n",
    "class RobustProcessor(dspy.Module):\n",
    "    \"\"\"A robust module with comprehensive error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, retry_attempts: int = 3):\n",
    "        super().__init__()\n",
    "        self.retry_attempts = retry_attempts\n",
    "        self.error_count = 0\n",
    "        self.success_count = 0\n",
    "        \n",
    "        # Different processing strategies\n",
    "        self.primary_processor = dspy.ChainOfThought(\"input -> processed_output, confidence\")\n",
    "        self.fallback_processor = dspy.Predict(\"input -> simple_output\")\n",
    "        self.error_handler = dspy.Predict(\"error_description, input -> recovery_suggestion\")\n",
    "    \n",
    "    def forward(self, input_text: str, processing_mode: str = \"normal\") -> Dict[str, Any]:\n",
    "        result = {\n",
    "            'input': input_text,\n",
    "            'output': None,\n",
    "            'success': False,\n",
    "            'error': None,\n",
    "            'attempts': 0,\n",
    "            'processing_mode': processing_mode,\n",
    "            'fallback_used': False\n",
    "        }\n",
    "        \n",
    "        # Input validation\n",
    "        if not input_text or not isinstance(input_text, str):\n",
    "            result['error'] = \"Invalid input: must be non-empty string\"\n",
    "            self.error_count += 1\n",
    "            return result\n",
    "        \n",
    "        # Try primary processing with retries\n",
    "        for attempt in range(self.retry_attempts):\n",
    "            result['attempts'] = attempt + 1\n",
    "            \n",
    "            try:\n",
    "                if processing_mode == \"detailed\":\n",
    "                    # More complex processing\n",
    "                    processed = self.primary_processor(input=input_text)\n",
    "                    result['output'] = processed.processed_output if hasattr(processed, 'processed_output') else str(processed)\n",
    "                    result['confidence'] = float(processed.confidence) if hasattr(processed, 'confidence') else 0.8\n",
    "                else:\n",
    "                    # Simple processing\n",
    "                    processed = self.fallback_processor(input=input_text)\n",
    "                    result['output'] = processed.simple_output if hasattr(processed, 'simple_output') else str(processed)\n",
    "                    result['confidence'] = 0.7  # Lower confidence for simple mode\n",
    "                \n",
    "                result['success'] = True\n",
    "                self.success_count += 1\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                logger.warning(f\"Processing attempt {attempt + 1} failed: {error_msg}\")\n",
    "                \n",
    "                if attempt == self.retry_attempts - 1:\n",
    "                    # Last attempt failed, try fallback\n",
    "                    try:\n",
    "                        fallback_result = self.fallback_processor(input=input_text)\n",
    "                        result['output'] = fallback_result.simple_output if hasattr(fallback_result, 'simple_output') else str(fallback_result)\n",
    "                        result['confidence'] = 0.5  # Low confidence for fallback\n",
    "                        result['success'] = True\n",
    "                        result['fallback_used'] = True\n",
    "                        self.success_count += 1\n",
    "                        \n",
    "                        # Get recovery suggestion\n",
    "                        try:\n",
    "                            recovery = self.error_handler(error_description=error_msg, input=input_text)\n",
    "                            result['recovery_suggestion'] = recovery.recovery_suggestion if hasattr(recovery, 'recovery_suggestion') else None\n",
    "                        except:\n",
    "                            pass  # Recovery suggestion is optional\n",
    "                        \n",
    "                    except Exception as fallback_error:\n",
    "                        result['error'] = f\"All processing failed. Last error: {str(fallback_error)}\"\n",
    "                        self.error_count += 1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_performance_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance statistics.\"\"\"\n",
    "        total = self.success_count + self.error_count\n",
    "        return {\n",
    "            'total_requests': total,\n",
    "            'success_count': self.success_count,\n",
    "            'error_count': self.error_count,\n",
    "            'success_rate': self.success_count / total if total > 0 else 0.0\n",
    "        }\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset performance statistics.\"\"\"\n",
    "        self.success_count = 0\n",
    "        self.error_count = 0\n",
    "\n",
    "# Test the robust processor\n",
    "processor = RobustProcessor(retry_attempts=2)\n",
    "\n",
    "# Test with various inputs\n",
    "test_cases = [\n",
    "    (\"Analyze the benefits of renewable energy\", \"detailed\"),\n",
    "    (\"What is Python?\", \"normal\"),\n",
    "    (\"\", \"normal\"),  # Invalid input\n",
    "    (\"Explain quantum computing in simple terms\", \"detailed\"),\n",
    "    (None, \"normal\")  # Invalid input\n",
    "]\n",
    "\n",
    "for i, (text, mode) in enumerate(test_cases, 1):\n",
    "    print(f\"\\n--- Test Case {i} ---\")\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Mode: {mode}\")\n",
    "    \n",
    "    result = processor(text, mode)\n",
    "    \n",
    "    print(f\"Success: {result['success']}\")\n",
    "    print(f\"Attempts: {result['attempts']}\")\n",
    "    print(f\"Fallback used: {result['fallback_used']}\")\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"Output: {result['output'][:100]}...\" if len(str(result['output'])) > 100 else f\"Output: {result['output']}\")\n",
    "        if 'confidence' in result:\n",
    "            print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    \n",
    "    if 'recovery_suggestion' in result:\n",
    "        print(f\"Recovery suggestion: {result['recovery_suggestion']}\")\n",
    "\n",
    "print(f\"\\n--- Performance Statistics ---\")\n",
    "stats = processor.get_performance_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fcbbad",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization\n",
    "\n",
    "Techniques for optimizing custom module performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import lru_cache\n",
    "from typing import Tuple\n",
    "\n",
    "# Optimized module with caching and batching\n",
    "class OptimizedProcessor(dspy.Module):\n",
    "    \"\"\"Optimized module with caching, batching, and performance monitoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_size: int = 100, batch_size: int = 5):\n",
    "        super().__init__()\n",
    "        self.cache_size = cache_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.processing_times = []\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        \n",
    "        # Processing modules\n",
    "        self.single_processor = dspy.Predict(\"text -> processed_text\")\n",
    "        self.batch_processor = dspy.Predict(\"text_batch -> processed_batch\")\n",
    "        \n",
    "        # Simple cache for demonstration\n",
    "        self._cache = {}\n",
    "    \n",
    "    def _get_cache_key(self, text: str) -> str:\n",
    "        \"\"\"Generate cache key for text.\"\"\"\n",
    "        return hash(text.strip().lower())\n",
    "    \n",
    "    def _cache_get(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Get from cache.\"\"\"\n",
    "        if key in self._cache:\n",
    "            self.cache_hits += 1\n",
    "            return self._cache[key]\n",
    "        self.cache_misses += 1\n",
    "        return None\n",
    "    \n",
    "    def _cache_set(self, key: str, value: str):\n",
    "        \"\"\"Set in cache with size management.\"\"\"\n",
    "        if len(self._cache) >= self.cache_size:\n",
    "            # Simple LRU: remove oldest entry\n",
    "            oldest_key = next(iter(self._cache))\n",
    "            del self._cache[oldest_key]\n",
    "        \n",
    "        self._cache[key] = value\n",
    "    \n",
    "    def forward(self, texts: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a list of texts with optimization.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = []\n",
    "        cache_results = {}\n",
    "        texts_to_process = []\n",
    "        \n",
    "        # Check cache first\n",
    "        for i, text in enumerate(texts):\n",
    "            cache_key = self._get_cache_key(text)\n",
    "            cached_result = self._cache_get(cache_key)\n",
    "            \n",
    "            if cached_result:\n",
    "                cache_results[i] = {\n",
    "                    'original': text,\n",
    "                    'processed': cached_result,\n",
    "                    'cached': True,\n",
    "                    'processing_time': 0.0\n",
    "                }\n",
    "            else:\n",
    "                texts_to_process.append((i, text, cache_key))\n",
    "        \n",
    "        # Process uncached texts\n",
    "        if texts_to_process:\n",
    "            if len(texts_to_process) <= self.batch_size:\n",
    "                # Process as batch\n",
    "                batch_texts = [item[1] for item in texts_to_process]\n",
    "                batch_start = time.time()\n",
    "                \n",
    "                try:\n",
    "                    batch_input = \"\\n---\\n\".join(batch_texts)\n",
    "                    batch_result = self.batch_processor(text_batch=batch_input)\n",
    "                    batch_outputs = batch_result.processed_batch.split(\"\\n---\\n\") if hasattr(batch_result, 'processed_batch') else batch_texts\n",
    "                    \n",
    "                    batch_time = time.time() - batch_start\n",
    "                    avg_time = batch_time / len(texts_to_process)\n",
    "                    \n",
    "                    for (i, original_text, cache_key), processed_text in zip(texts_to_process, batch_outputs):\n",
    "                        self._cache_set(cache_key, processed_text)\n",
    "                        cache_results[i] = {\n",
    "                            'original': original_text,\n",
    "                            'processed': processed_text,\n",
    "                            'cached': False,\n",
    "                            'processing_time': avg_time,\n",
    "                            'batch_processed': True\n",
    "                        }\n",
    "                \n",
    "                except Exception as e:\n",
    "                    # Fallback to individual processing\n",
    "                    logger.warning(f\"Batch processing failed, falling back to individual processing: {e}\")\n",
    "                    for i, text, cache_key in texts_to_process:\n",
    "                        individual_start = time.time()\n",
    "                        try:\n",
    "                            result = self.single_processor(text=text)\n",
    "                            processed = result.processed_text if hasattr(result, 'processed_text') else text\n",
    "                            individual_time = time.time() - individual_start\n",
    "                            \n",
    "                            self._cache_set(cache_key, processed)\n",
    "                            cache_results[i] = {\n",
    "                                'original': text,\n",
    "                                'processed': processed,\n",
    "                                'cached': False,\n",
    "                                'processing_time': individual_time,\n",
    "                                'batch_processed': False\n",
    "                            }\n",
    "                        except Exception as individual_error:\n",
    "                            cache_results[i] = {\n",
    "                                'original': text,\n",
    "                                'processed': text,  # Return original on error\n",
    "                                'cached': False,\n",
    "                                'processing_time': time.time() - individual_start,\n",
    "                                'error': str(individual_error)\n",
    "                            }\n",
    "            else:\n",
    "                # Process individually for large batches\n",
    "                for i, text, cache_key in texts_to_process:\n",
    "                    individual_start = time.time()\n",
    "                    try:\n",
    "                        result = self.single_processor(text=text)\n",
    "                        processed = result.processed_text if hasattr(result, 'processed_text') else text\n",
    "                        individual_time = time.time() - individual_start\n",
    "                        \n",
    "                        self._cache_set(cache_key, processed)\n",
    "                        cache_results[i] = {\n",
    "                            'original': text,\n",
    "                            'processed': processed,\n",
    "                            'cached': False,\n",
    "                            'processing_time': individual_time,\n",
    "                            'batch_processed': False\n",
    "                        }\n",
    "                    except Exception as e:\n",
    "                        cache_results[i] = {\n",
    "                            'original': text,\n",
    "                            'processed': text,\n",
    "                            'cached': False,\n",
    "                            'processing_time': time.time() - individual_start,\n",
    "                            'error': str(e)\n",
    "                        }\n",
    "        \n",
    "        # Assemble results in original order\n",
    "        results = [cache_results[i] for i in range(len(texts))]\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        self.processing_times.append(total_time)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_performance_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance metrics.\"\"\"\n",
    "        total_requests = self.cache_hits + self.cache_misses\n",
    "        \n",
    "        return {\n",
    "            'cache_hit_rate': self.cache_hits / total_requests if total_requests > 0 else 0.0,\n",
    "            'cache_hits': self.cache_hits,\n",
    "            'cache_misses': self.cache_misses,\n",
    "            'cache_size': len(self._cache),\n",
    "            'avg_processing_time': sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0.0,\n",
    "            'total_batches_processed': len(self.processing_times)\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the cache.\"\"\"\n",
    "        self._cache.clear()\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "\n",
    "# Test the optimized processor\n",
    "optimized_processor = OptimizedProcessor(cache_size=50, batch_size=3)\n",
    "\n",
    "# Test data with some repetition to demonstrate caching\n",
    "test_texts = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain machine learning concepts\",\n",
    "    \"What is artificial intelligence?\",  # Duplicate for cache test\n",
    "    \"How do neural networks work?\",\n",
    "    \"Explain machine learning concepts\",  # Another duplicate\n",
    "    \"What are the benefits of cloud computing?\",\n",
    "    \"Describe data science workflows\"\n",
    "]\n",
    "\n",
    "print(\"Processing batch 1...\")\n",
    "results1 = optimized_processor(test_texts)\n",
    "\n",
    "for i, result in enumerate(results1):\n",
    "    print(f\"\\nText {i+1}: {result['original'][:50]}...\")\n",
    "    print(f\"Cached: {result['cached']}\")\n",
    "    print(f\"Processing time: {result['processing_time']:.3f}s\")\n",
    "    if 'batch_processed' in result:\n",
    "        print(f\"Batch processed: {result['batch_processed']}\")\n",
    "    if 'error' in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "\n",
    "print(\"\\nProcessing same batch again (should hit cache more)...\")\n",
    "results2 = optimized_processor(test_texts)\n",
    "\n",
    "print(f\"\\n--- Performance Metrics ---\")\n",
    "metrics = optimized_processor.get_performance_metrics()\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6936ea",
   "metadata": {},
   "source": [
    "## 7. Advanced Module Composition\n",
    "\n",
    "Building complex applications by composing multiple specialized modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced application combining all our custom modules\n",
    "class AdvancedContentManager(dspy.Module):\n",
    "    \"\"\"Advanced content management system combining multiple specialized modules.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize all component modules\n",
    "        self.content_pipeline = ContentProcessingPipeline([\n",
    "            \"appropriate language\", \"factual accuracy\", \"coherence\", \"completeness\"\n",
    "        ])\n",
    "        self.conversation_manager = ConversationManager(max_history=8)\n",
    "        self.robust_processor = RobustProcessor(retry_attempts=2)\n",
    "        self.optimized_processor = OptimizedProcessor(cache_size=100, batch_size=4)\n",
    "        \n",
    "        # System state\n",
    "        self.session_data = {\n",
    "            'total_processed': 0,\n",
    "            'session_start': datetime.now(),\n",
    "            'processing_modes': ['conversation', 'batch', 'analysis', 'robust']\n",
    "        }\n",
    "    \n",
    "    def process_conversation(self, message: str, user_id: str = \"user\") -> Dict[str, Any]:\n",
    "        \"\"\"Process conversational input.\"\"\"\n",
    "        conv_result = self.conversation_manager(message, user_id)\n",
    "        \n",
    "        # Enhance response quality\n",
    "        enhanced_result = self.content_pipeline(conv_result['response'], enhance_if_needed=True)\n",
    "        \n",
    "        self.session_data['total_processed'] += 1\n",
    "        \n",
    "        return {\n",
    "            'mode': 'conversation',\n",
    "            'response': enhanced_result['final_text'],\n",
    "            'original_response': conv_result['response'],\n",
    "            'was_enhanced': enhanced_result['was_enhanced'],\n",
    "            'confidence': conv_result['confidence'],\n",
    "            'conversation_state': self.conversation_manager.get_conversation_state()\n",
    "        }\n",
    "    \n",
    "    def process_batch(self, texts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Process multiple texts efficiently.\"\"\"\n",
    "        batch_results = self.optimized_processor(texts)\n",
    "        \n",
    "        # Analyze each result\n",
    "        analyzed_results = []\n",
    "        for result in batch_results:\n",
    "            if not result.get('error'):\n",
    "                analysis = self.content_pipeline(result['processed'], enhance_if_needed=False)\n",
    "                analyzed_results.append({\n",
    "                    **result,\n",
    "                    'analysis': analysis['analysis'].metadata,\n",
    "                    'validation': {\n",
    "                        'is_valid': analysis['validation'].is_valid,\n",
    "                        'issues_count': len(analysis['validation'].issues)\n",
    "                    }\n",
    "                })\n",
    "            else:\n",
    "                analyzed_results.append(result)\n",
    "        \n",
    "        self.session_data['total_processed'] += len(texts)\n",
    "        \n",
    "        return {\n",
    "            'mode': 'batch',\n",
    "            'results': analyzed_results,\n",
    "            'batch_size': len(texts),\n",
    "            'performance': self.optimized_processor.get_performance_metrics()\n",
    "        }\n",
    "    \n",
    "    def process_robust(self, text: str, processing_mode: str = \"detailed\") -> Dict[str, Any]:\n",
    "        \"\"\"Process with robust error handling.\"\"\"\n",
    "        robust_result = self.robust_processor(text, processing_mode)\n",
    "        \n",
    "        # If successful, run additional analysis\n",
    "        if robust_result['success']:\n",
    "            analysis_result = self.content_pipeline(robust_result['output'], enhance_if_needed=True)\n",
    "            robust_result['enhanced_analysis'] = {\n",
    "                'final_content': analysis_result['final_text'],\n",
    "                'was_enhanced': analysis_result['was_enhanced'],\n",
    "                'sentiment': analysis_result['analysis'].metadata.get('sentiment'),\n",
    "                'quality_score': analysis_result['analysis'].metadata.get('quality_score')\n",
    "            }\n",
    "        \n",
    "        self.session_data['total_processed'] += 1\n",
    "        \n",
    "        return {\n",
    "            'mode': 'robust',\n",
    "            **robust_result\n",
    "        }\n",
    "    \n",
    "    def get_system_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive system status.\"\"\"\n",
    "        return {\n",
    "            'session_duration': str(datetime.now() - self.session_data['session_start']),\n",
    "            'total_processed': self.session_data['total_processed'],\n",
    "            'pipeline_stats': self.content_pipeline.get_processing_stats(),\n",
    "            'conversation_state': self.conversation_manager.get_conversation_state(),\n",
    "            'robust_processor_stats': self.robust_processor.get_performance_stats(),\n",
    "            'optimization_metrics': self.optimized_processor.get_performance_metrics()\n",
    "        }\n",
    "    \n",
    "    def reset_session(self):\n",
    "        \"\"\"Reset session data and clear caches.\"\"\"\n",
    "        self.session_data['total_processed'] = 0\n",
    "        self.session_data['session_start'] = datetime.now()\n",
    "        self.conversation_manager.reset_conversation()\n",
    "        self.robust_processor.reset_stats()\n",
    "        self.optimized_processor.clear_cache()\n",
    "\n",
    "# Demonstrate the advanced content manager\n",
    "content_manager = AdvancedContentManager()\n",
    "\n",
    "print(\"=== Advanced Content Manager Demo ===\")\n",
    "\n",
    "# Test conversation mode\n",
    "print(\"\\n1. Conversation Mode:\")\n",
    "conv_result = content_manager.process_conversation(\n",
    "    \"Hello! I'd like to learn about the latest developments in artificial intelligence.\"\n",
    ")\n",
    "print(f\"Response: {conv_result['response'][:100]}...\")\n",
    "print(f\"Enhanced: {conv_result['was_enhanced']}\")\n",
    "print(f\"Confidence: {conv_result['confidence']:.2f}\")\n",
    "\n",
    "# Test batch mode\n",
    "print(\"\\n2. Batch Processing Mode:\")\n",
    "batch_texts = [\n",
    "    \"Explain the concept of machine learning\",\n",
    "    \"What are the benefits of cloud computing?\",\n",
    "    \"Describe data visualization techniques\"\n",
    "]\n",
    "batch_result = content_manager.process_batch(batch_texts)\n",
    "print(f\"Processed {batch_result['batch_size']} texts\")\n",
    "print(f\"Cache hit rate: {batch_result['performance']['cache_hit_rate']:.2f}\")\n",
    "\n",
    "for i, result in enumerate(batch_result['results']):\n",
    "    print(f\"  Text {i+1}: Success={not result.get('error')}, Cached={result.get('cached', False)}\")\n",
    "\n",
    "# Test robust mode\n",
    "print(\"\\n3. Robust Processing Mode:\")\n",
    "robust_result = content_manager.process_robust(\n",
    "    \"Analyze the environmental impact of renewable energy technologies\",\n",
    "    \"detailed\"\n",
    ")\n",
    "print(f\"Success: {robust_result['success']}\")\n",
    "print(f\"Attempts: {robust_result['attempts']}\")\n",
    "if robust_result['success'] and 'enhanced_analysis' in robust_result:\n",
    "    analysis = robust_result['enhanced_analysis']\n",
    "    print(f\"Enhanced: {analysis['was_enhanced']}\")\n",
    "    print(f\"Sentiment: {analysis['sentiment']}\")\n",
    "\n",
    "# System status\n",
    "print(\"\\n4. System Status:\")\n",
    "status = content_manager.get_system_status()\n",
    "print(f\"Session duration: {status['session_duration']}\")\n",
    "print(f\"Total processed: {status['total_processed']}\")\n",
    "print(f\"Pipeline success rate: {status['pipeline_stats'].get('enhancement_rate', 0):.2f}\")\n",
    "print(f\"Robust processor success rate: {status['robust_processor_stats']['success_rate']:.2f}\")\n",
    "print(f\"Cache hit rate: {status['optimization_metrics']['cache_hit_rate']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d47d5db",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated how to build sophisticated AI applications by customizing DSPy modules:\n",
    "\n",
    "### Key Concepts Covered:\n",
    "\n",
    "1. **Module Architecture**: Understanding the structure and components of DSPy modules\n",
    "2. **Custom Module Creation**: Building specialized modules for specific tasks\n",
    "3. **Module Composition**: Combining multiple modules into complex workflows\n",
    "4. **State Management**: Maintaining context and state across module calls\n",
    "5. **Error Handling**: Building robust modules with proper error handling and fallbacks\n",
    "6. **Performance Optimization**: Implementing caching, batching, and performance monitoring\n",
    "7. **Advanced Integration**: Creating comprehensive systems that leverage multiple specialized modules\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- **Modularity**: Keep modules focused on specific tasks\n",
    "- **Robustness**: Always include error handling and fallback mechanisms\n",
    "- **Performance**: Use caching and batching for efficiency\n",
    "- **Monitoring**: Track performance metrics and system health\n",
    "- **Composability**: Design modules to work well together\n",
    "- **State Management**: Carefully manage state to avoid conflicts\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Experiment with different module combinations\n",
    "- Add custom optimizers for your specific use cases\n",
    "- Implement domain-specific modules for your applications\n",
    "- Explore advanced features like streaming and async processing\n",
    "- Integrate with external APIs and databases\n",
    "\n",
    "This foundation provides the building blocks for creating sophisticated AI applications with DSPy's powerful module system."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
