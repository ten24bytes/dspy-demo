{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a72fa519",
      "metadata": {},
      "source": [
        "# Privacy-Conscious Delegation with DSPy\n",
        "\n",
        "This notebook demonstrates how to build privacy-conscious AI systems using DSPy that can delegate sensitive tasks while maintaining data privacy.\n",
        "\n",
        "Based on the DSPy tutorial: [Privacy-Conscious Delegation](https://dspy.ai/tutorials/papillon/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f68997",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary libraries and configure the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c77574d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('../../')\n",
        "\n",
        "import dspy\n",
        "from utils import setup_default_lm, print_step, print_result, print_error\n",
        "from dotenv import load_dotenv\n",
        "import hashlib\n",
        "import uuid\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv('../../.env')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c7e1bc",
      "metadata": {},
      "source": [
        "## Language Model Configuration\n",
        "\n",
        "Set up DSPy with a language model for privacy-conscious processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e924ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(\"Setting up Language Model\", \"Configuring DSPy for privacy-conscious delegation\")\n",
        "\n",
        "try:\n",
        "    # Use a privacy-focused model configuration\n",
        "    lm = setup_default_lm(provider=\"openai\", model=\"gpt-4o\", max_tokens=1000)\n",
        "    dspy.configure(lm=lm)\n",
        "    print_result(\"Language model configured successfully!\", \"Status\")\n",
        "except Exception as e:\n",
        "    print_error(f\"Failed to configure language model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd1f378a",
      "metadata": {},
      "source": [
        "## Privacy-Preserving Data Handler\n",
        "\n",
        "Create a class to handle sensitive data with privacy protection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73bf0e28",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PrivacyHandler:\n",
        "    \"\"\"Handles privacy-sensitive data processing.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.sensitive_data_map = {}\n",
        "        self.anonymization_map = {}\n",
        "    \n",
        "    def anonymize_text(self, text: str, sensitive_entities: list) -> str:\n",
        "        \"\"\"Replace sensitive entities with anonymized placeholders.\"\"\"\n",
        "        anonymized_text = text\n",
        "        \n",
        "        for entity in sensitive_entities:\n",
        "            if entity not in self.anonymization_map:\n",
        "                # Generate a unique placeholder\n",
        "                placeholder = f\"[ENTITY_{len(self.anonymization_map)}]\"\n",
        "                self.anonymization_map[entity] = placeholder\n",
        "                self.sensitive_data_map[placeholder] = entity\n",
        "            \n",
        "            anonymized_text = anonymized_text.replace(entity, self.anonymization_map[entity])\n",
        "        \n",
        "        return anonymized_text\n",
        "    \n",
        "    def deanonymize_text(self, text: str) -> str:\n",
        "        \"\"\"Restore original entities from anonymized text.\"\"\"\n",
        "        deanonymized_text = text\n",
        "        \n",
        "        for placeholder, original in self.sensitive_data_map.items():\n",
        "            deanonymized_text = deanonymized_text.replace(placeholder, original)\n",
        "        \n",
        "        return deanonymized_text\n",
        "\n",
        "# Test the privacy handler\n",
        "privacy_handler = PrivacyHandler()\n",
        "\n",
        "# Example sensitive data\n",
        "sensitive_text = \"John Smith works at Google and his email is john.smith@gmail.com\"\n",
        "sensitive_entities = [\"John Smith\", \"Google\", \"john.smith@gmail.com\"]\n",
        "\n",
        "anonymized = privacy_handler.anonymize_text(sensitive_text, sensitive_entities)\n",
        "print_result(f\"Original: {sensitive_text}\")\n",
        "print_result(f\"Anonymized: {anonymized}\")\n",
        "print_result(f\"Restored: {privacy_handler.deanonymize_text(anonymized)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb472e0c",
      "metadata": {},
      "source": [
        "## Privacy-Conscious Processing Signatures\n",
        "\n",
        "Define signatures for privacy-aware text processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9591eacc",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SensitiveDataDetection(dspy.Signature):\n",
        "    \"\"\"Detect sensitive information in text without exposing the actual data.\"\"\"\n",
        "    \n",
        "    text = dspy.InputField(desc=\"Text to analyze for sensitive information\")\n",
        "    sensitive_types = dspy.OutputField(desc=\"Types of sensitive information detected (without revealing actual values)\")\n",
        "\n",
        "class PrivacyAnalysis(dspy.Signature):\n",
        "    \"\"\"Analyze anonymized text for insights while preserving privacy.\"\"\"\n",
        "    \n",
        "    anonymized_text = dspy.InputField(desc=\"Text with sensitive information anonymized\")\n",
        "    analysis = dspy.OutputField(desc=\"Analysis and insights from the anonymized text\")\n",
        "\n",
        "class SecureRecommendation(dspy.Signature):\n",
        "    \"\"\"Provide recommendations based on anonymized analysis.\"\"\"\n",
        "    \n",
        "    analysis = dspy.InputField(desc=\"Analysis of anonymized data\")\n",
        "    recommendations = dspy.OutputField(desc=\"Privacy-safe recommendations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d172b7a",
      "metadata": {},
      "source": [
        "## Privacy-Conscious Processing Module\n",
        "\n",
        "Create a module that processes sensitive data while maintaining privacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3f98b3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PrivacyConsciousProcessor(dspy.Module):\n",
        "    \"\"\"A module for processing sensitive data with privacy protection.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.privacy_handler = PrivacyHandler()\n",
        "        self.detect_sensitive = dspy.ChainOfThought(SensitiveDataDetection)\n",
        "        self.analyze_privacy = dspy.ChainOfThought(PrivacyAnalysis)\n",
        "        self.secure_recommend = dspy.ChainOfThought(SecureRecommendation)\n",
        "    \n",
        "    def forward(self, text: str, sensitive_entities: list = None):\n",
        "        \"\"\"Process text while maintaining privacy.\"\"\"\n",
        "        \n",
        "        # Step 1: Detect sensitive information types\n",
        "        print_step(\"Step 1: Sensitive Data Detection\")\n",
        "        detection_result = self.detect_sensitive(text=text)\n",
        "        print_result(detection_result.sensitive_types, \"Detected Sensitive Types\")\n",
        "        \n",
        "        # Step 2: Anonymize the text if sensitive entities are provided\n",
        "        if sensitive_entities:\n",
        "            anonymized_text = self.privacy_handler.anonymize_text(text, sensitive_entities)\n",
        "            print_step(\"Step 2: Text Anonymization\")\n",
        "            print_result(anonymized_text, \"Anonymized Text\")\n",
        "        else:\n",
        "            anonymized_text = text\n",
        "        \n",
        "        # Step 3: Analyze the anonymized text\n",
        "        print_step(\"Step 3: Privacy-Safe Analysis\")\n",
        "        analysis_result = self.analyze_privacy(anonymized_text=anonymized_text)\n",
        "        print_result(analysis_result.analysis, \"Analysis\")\n",
        "        \n",
        "        # Step 4: Generate secure recommendations\n",
        "        print_step(\"Step 4: Secure Recommendations\")\n",
        "        recommendation_result = self.secure_recommend(analysis=analysis_result.analysis)\n",
        "        print_result(recommendation_result.recommendations, \"Recommendations\")\n",
        "        \n",
        "        return dspy.Prediction(\n",
        "            sensitive_types=detection_result.sensitive_types,\n",
        "            anonymized_text=anonymized_text,\n",
        "            analysis=analysis_result.analysis,\n",
        "            recommendations=recommendation_result.recommendations\n",
        "        )\n",
        "\n",
        "# Initialize the processor\n",
        "processor = PrivacyConsciousProcessor()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1310b82",
      "metadata": {},
      "source": [
        "## Example: Processing Employee Feedback\n",
        "\n",
        "Let's process employee feedback that contains sensitive information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9354a76a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example employee feedback with sensitive information\n",
        "employee_feedback = \"\"\"\n",
        "Hi, this is Sarah Johnson from the Marketing Department. \n",
        "I wanted to provide feedback about our recent project with Microsoft. \n",
        "My manager, David Wilson, has been very supportive, but I think we need \n",
        "better tools for collaboration. My work email is sarah.j@company.com \n",
        "and I've been working on the Adobe Creative Suite integration project.\n",
        "The project budget was $150,000 and we completed it on March 15, 2024.\n",
        "\"\"\"\n",
        "\n",
        "# Identify sensitive entities\n",
        "sensitive_entities = [\n",
        "    \"Sarah Johnson\", \n",
        "    \"David Wilson\", \n",
        "    \"Microsoft\", \n",
        "    \"sarah.j@company.com\", \n",
        "    \"$150,000\", \n",
        "    \"March 15, 2024\"\n",
        "]\n",
        "\n",
        "print_step(\"Processing Employee Feedback\", \"Analyzing feedback while protecting privacy\")\n",
        "\n",
        "# Process the feedback\n",
        "result = processor(text=employee_feedback, sensitive_entities=sensitive_entities)\n",
        "\n",
        "print_step(\"Final Results Summary\")\n",
        "print(\"✓ Sensitive information detected and protected\")\n",
        "print(\"✓ Analysis completed on anonymized data\")\n",
        "print(\"✓ Secure recommendations generated\")\n",
        "print(\"✓ Original data privacy maintained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee1b1ae6",
      "metadata": {},
      "source": [
        "## Example: Customer Service Data Protection\n",
        "\n",
        "Process customer service interactions while protecting customer privacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "496de337",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customer service interaction with PII\n",
        "customer_interaction = \"\"\"\n",
        "Customer: Hi, I'm having issues with my order. My name is Emily Chen \n",
        "and my order number is ORD-12345-XYZ. I live at 123 Main Street, \n",
        "San Francisco, CA 94102. My phone number is (555) 123-4567 and \n",
        "email is emily.chen@email.com. The order was placed on my credit \n",
        "card ending in 1234.\n",
        "\"\"\"\n",
        "\n",
        "customer_sensitive_entities = [\n",
        "    \"Emily Chen\",\n",
        "    \"ORD-12345-XYZ\", \n",
        "    \"123 Main Street, San Francisco, CA 94102\",\n",
        "    \"(555) 123-4567\",\n",
        "    \"emily.chen@email.com\",\n",
        "    \"1234\"\n",
        "]\n",
        "\n",
        "print_step(\"Processing Customer Service Data\", \"Protecting customer privacy\")\n",
        "\n",
        "# Process customer data\n",
        "customer_result = processor(text=customer_interaction, sensitive_entities=customer_sensitive_entities)\n",
        "\n",
        "# Show how we can still get insights without exposing customer data\n",
        "print_step(\"Privacy-Protected Customer Insights\")\n",
        "print(\"✓ Customer issue type identified without exposing identity\")\n",
        "print(\"✓ Process improvements suggested based on anonymized patterns\")\n",
        "print(\"✓ Service quality metrics generated safely\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "130ff93d",
      "metadata": {},
      "source": [
        "## Advanced Privacy Features\n",
        "\n",
        "Implement additional privacy-preserving techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e5653c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedPrivacyProcessor(dspy.Module):\n",
        "    \"\"\"Advanced privacy-conscious processing with differential privacy concepts.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.basic_processor = PrivacyConsciousProcessor()\n",
        "    \n",
        "    def add_noise_to_metrics(self, metrics: dict, noise_level: float = 0.1):\n",
        "        \"\"\"Add noise to numerical metrics for differential privacy.\"\"\"\n",
        "        import random\n",
        "        \n",
        "        noisy_metrics = {}\n",
        "        for key, value in metrics.items():\n",
        "            if isinstance(value, (int, float)):\n",
        "                noise = random.gauss(0, noise_level * abs(value))\n",
        "                noisy_metrics[key] = max(0, value + noise)  # Ensure non-negative\n",
        "            else:\n",
        "                noisy_metrics[key] = value\n",
        "        \n",
        "        return noisy_metrics\n",
        "    \n",
        "    def k_anonymize_groups(self, data_groups: list, k: int = 3):\n",
        "        \"\"\"Implement k-anonymity for grouped data.\"\"\"\n",
        "        # Simple k-anonymity implementation\n",
        "        anonymized_groups = []\n",
        "        \n",
        "        for group in data_groups:\n",
        "            if len(group) >= k:\n",
        "                anonymized_groups.append(f\"Group of {len(group)} entities\")\n",
        "            else:\n",
        "                anonymized_groups.append(f\"Small group (< {k} entities)\")\n",
        "        \n",
        "        return anonymized_groups\n",
        "\n",
        "# Example of advanced privacy features\n",
        "advanced_processor = AdvancyPrivacyProcessor()\n",
        "\n",
        "# Simulate some metrics\n",
        "sample_metrics = {\n",
        "    \"response_time\": 2.5,\n",
        "    \"satisfaction_score\": 4.2,\n",
        "    \"resolution_rate\": 0.85,\n",
        "    \"category\": \"technical_support\"\n",
        "}\n",
        "\n",
        "print_step(\"Advanced Privacy Features\")\n",
        "noisy_metrics = advanced_processor.add_noise_to_metrics(sample_metrics)\n",
        "print_result(f\"Original metrics: {sample_metrics}\")\n",
        "print_result(f\"Privacy-protected metrics: {noisy_metrics}\")\n",
        "\n",
        "# K-anonymity example\n",
        "data_groups = [\n",
        "    [\"user1\", \"user2\", \"user3\", \"user4\"],  # Group of 4\n",
        "    [\"user5\", \"user6\"],  # Group of 2\n",
        "    [\"user7\", \"user8\", \"user9\", \"user10\", \"user11\"]  # Group of 5\n",
        "]\n",
        "\n",
        "k_anon_groups = advanced_processor.k_anonymize_groups(data_groups, k=3)\n",
        "print_result(f\"K-anonymized groups: {k_anon_groups}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a002686e",
      "metadata": {},
      "source": [
        "## Best Practices for Privacy-Conscious AI\n",
        "\n",
        "Key principles for building privacy-aware AI systems:\n",
        "\n",
        "1. **Data Minimization**: Only collect and process necessary data\n",
        "2. **Purpose Limitation**: Use data only for stated purposes\n",
        "3. **Anonymization**: Remove or mask identifying information\n",
        "4. **Differential Privacy**: Add noise to protect individual privacy\n",
        "5. **Secure Processing**: Use encrypted and secure computation methods\n",
        "6. **Transparency**: Be clear about data usage and privacy protections\n",
        "7. **Regular Audits**: Monitor and audit privacy protections\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated how to build privacy-conscious AI systems using DSPy that can:\n",
        "\n",
        "- Detect sensitive information automatically\n",
        "- Anonymize data while preserving utility\n",
        "- Generate insights without exposing private information\n",
        "- Implement advanced privacy techniques like differential privacy\n",
        "- Maintain transparency about privacy protections\n",
        "\n",
        "These techniques are essential for building AI systems that comply with privacy regulations like GDPR, CCPA, and HIPAA."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
