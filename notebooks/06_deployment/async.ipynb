{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721b5b32",
   "metadata": {},
   "source": [
    "# Async Processing with DSPy\n",
    "\n",
    "This notebook demonstrates asynchronous processing capabilities in DSPy for building scalable, non-blocking AI applications.\n",
    "\n",
    "## What You'll Learn:\n",
    "- Setting up async DSPy modules\n",
    "- Concurrent request processing\n",
    "- Batch processing with async patterns\n",
    "- Error handling in async contexts\n",
    "- Performance optimization for async operations\n",
    "\n",
    "Based on the DSPy tutorial: [Async](https://dspy.ai/tutorials/async/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcba490",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26308df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import dspy\n",
    "from utils import setup_default_lm, print_step, print_result, print_error\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional, Coroutine\n",
    "import json\n",
    "from datetime import datetime\n",
    "import aiohttp\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import functools\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c2cde",
   "metadata": {},
   "source": [
    "## Language Model Configuration for Async Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a38627",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Setting up Language Model\", \"Configuring DSPy for async operations\")\n",
    "\n",
    "try:\n",
    "    # Configure with async-capable model\n",
    "    lm = setup_default_lm(\n",
    "        provider=\"openai\", \n",
    "        model=\"gpt-4o-mini\", \n",
    "        max_tokens=1000\n",
    "    )\n",
    "    dspy.configure(lm=lm)\n",
    "    print_result(\"Async-capable language model configured successfully!\", \"Status\")\n",
    "except Exception as e:\n",
    "    print_error(f\"Failed to configure language model: {e}\")\n",
    "    # Continue with demo using fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcae8bb",
   "metadata": {},
   "source": [
    "## Basic Async DSPy Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncDSPyModule(dspy.Module):\n",
    "    \"\"\"Basic async-enabled DSPy module.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predictor = dspy.Predict(\"prompt -> response\")\n",
    "        self.chain_of_thought = dspy.ChainOfThought(\"question -> answer\")\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        \"\"\"Synchronous forward pass.\"\"\"\n",
    "        return self.predictor(**kwargs)\n",
    "    \n",
    "    async def async_forward(self, **kwargs):\n",
    "        \"\"\"Asynchronous forward pass.\"\"\"\n",
    "        # Run DSPy prediction in thread pool to avoid blocking\n",
    "        loop = asyncio.get_event_loop()\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            result = await loop.run_in_executor(\n",
    "                executor, \n",
    "                lambda: self.predictor(**kwargs)\n",
    "            )\n",
    "        return result\n",
    "    \n",
    "    async def async_chain_of_thought(self, question: str):\n",
    "        \"\"\"Async chain of thought reasoning.\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            result = await loop.run_in_executor(\n",
    "                executor,\n",
    "                lambda: self.chain_of_thought(question=question)\n",
    "            )\n",
    "        return result\n",
    "\n",
    "# Initialize async module\n",
    "async_module = AsyncDSPyModule()\n",
    "print_result(\"Async DSPy module initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5568530",
   "metadata": {},
   "source": [
    "## Testing Basic Async Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Testing Basic Async Operations\", \"Comparing sync vs async performance\")\n",
    "\n",
    "async def test_basic_async():\n",
    "    \"\"\"Test basic asynchronous operations.\"\"\"\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"Explain machine learning\",\n",
    "        \"What is artificial intelligence?\",\n",
    "        \"Describe neural networks\"\n",
    "    ]\n",
    "    \n",
    "    # Test 1: Synchronous processing\n",
    "    print(\"\\n1. Synchronous Processing:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    sync_start = time.time()\n",
    "    sync_results = []\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"Processing prompt {i+1}: {prompt}\")\n",
    "        result = async_module.forward(prompt=prompt)\n",
    "        sync_results.append(result.response[:100] + \"...\")\n",
    "        print(f\"Result: {sync_results[-1]}\\n\")\n",
    "    \n",
    "    sync_time = time.time() - sync_start\n",
    "    print(f\"Synchronous processing time: {sync_time:.2f} seconds\")\n",
    "    \n",
    "    # Test 2: Asynchronous processing\n",
    "    print(\"\\n2. Asynchronous Processing:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    async_start = time.time()\n",
    "    \n",
    "    # Create async tasks\n",
    "    tasks = []\n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        task = asyncio.create_task(\n",
    "            async_module.async_forward(prompt=prompt),\n",
    "            name=f\"task_{i+1}\"\n",
    "        )\n",
    "        tasks.append(task)\n",
    "        print(f\"Started task {i+1}: {prompt}\")\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    async_results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    async_time = time.time() - async_start\n",
    "    \n",
    "    print(\"\\nAsync Results:\")\n",
    "    for i, result in enumerate(async_results):\n",
    "        print(f\"Task {i+1}: {result.response[:100]}...\")\n",
    "    \n",
    "    print(f\"\\nAsynchronous processing time: {async_time:.2f} seconds\")\n",
    "    print(f\"Speedup: {sync_time/async_time:.2f}x\")\n",
    "\n",
    "# Run async test\n",
    "try:\n",
    "    await test_basic_async()\n",
    "except Exception as e:\n",
    "    print_error(f\"Error in basic async test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee5c6e",
   "metadata": {},
   "source": [
    "## Advanced Async Processing with Batch Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48aa166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncBatchProcessor(dspy.Module):\n",
    "    \"\"\"Advanced async batch processing module.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_concurrent: int = 5):\n",
    "        super().__init__()\n",
    "        self.qa_module = dspy.ChainOfThought(\"question -> answer\")\n",
    "        self.summarizer = dspy.Predict(\"text -> summary\")\n",
    "        self.classifier = dspy.Predict(\"text -> category\")\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def process_single_item(self, item: Dict[str, Any], task_type: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single item asynchronously.\"\"\"\n",
    "        \n",
    "        async with self.semaphore:  # Limit concurrent operations\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Run in thread pool to avoid blocking\n",
    "                loop = asyncio.get_event_loop()\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    if task_type == \"qa\":\n",
    "                        result = await loop.run_in_executor(\n",
    "                            executor,\n",
    "                            lambda: self.qa_module(question=item[\"question\"])\n",
    "                        )\n",
    "                        output = result.answer\n",
    "                    elif task_type == \"summarize\":\n",
    "                        result = await loop.run_in_executor(\n",
    "                            executor,\n",
    "                            lambda: self.summarizer(text=item[\"text\"])\n",
    "                        )\n",
    "                        output = result.summary\n",
    "                    elif task_type == \"classify\":\n",
    "                        result = await loop.run_in_executor(\n",
    "                            executor,\n",
    "                            lambda: self.classifier(text=item[\"text\"])\n",
    "                        )\n",
    "                        output = result.category\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "                \n",
    "                return {\n",
    "                    \"id\": item.get(\"id\", \"unknown\"),\n",
    "                    \"input\": item,\n",
    "                    \"output\": output,\n",
    "                    \"task_type\": task_type,\n",
    "                    \"processing_time\": time.time() - start_time,\n",
    "                    \"status\": \"success\",\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"id\": item.get(\"id\", \"unknown\"),\n",
    "                    \"input\": item,\n",
    "                    \"output\": None,\n",
    "                    \"task_type\": task_type,\n",
    "                    \"processing_time\": time.time() - start_time,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "    \n",
    "    async def batch_process(self, items: List[Dict[str, Any]], task_type: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process multiple items asynchronously.\"\"\"\n",
    "        \n",
    "        print(f\"Processing {len(items)} items of type '{task_type}' with max concurrency {self.max_concurrent}\")\n",
    "        \n",
    "        # Create tasks for all items\n",
    "        tasks = [\n",
    "            asyncio.create_task(\n",
    "                self.process_single_item(item, task_type),\n",
    "                name=f\"{task_type}_{item.get('id', i)}\"\n",
    "            )\n",
    "            for i, item in enumerate(items)\n",
    "        ]\n",
    "        \n",
    "        # Process tasks as they complete\n",
    "        results = []\n",
    "        completed = 0\n",
    "        \n",
    "        for task in asyncio.as_completed(tasks):\n",
    "            result = await task\n",
    "            results.append(result)\n",
    "            completed += 1\n",
    "            \n",
    "            if completed % 2 == 0 or completed == len(tasks):\n",
    "                print(f\"Completed {completed}/{len(tasks)} tasks\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def mixed_batch_process(self, task_batches: Dict[str, List[Dict[str, Any]]]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Process multiple batches of different task types concurrently.\"\"\"\n",
    "        \n",
    "        print(f\"Processing mixed batches: {list(task_batches.keys())}\")\n",
    "        \n",
    "        # Create tasks for each batch type\n",
    "        batch_tasks = {\n",
    "            task_type: asyncio.create_task(\n",
    "                self.batch_process(items, task_type),\n",
    "                name=f\"batch_{task_type}\"\n",
    "            )\n",
    "            for task_type, items in task_batches.items()\n",
    "        }\n",
    "        \n",
    "        # Wait for all batches to complete\n",
    "        batch_results = {}\n",
    "        for task_type, task in batch_tasks.items():\n",
    "            batch_results[task_type] = await task\n",
    "        \n",
    "        return batch_results\n",
    "\n",
    "# Initialize batch processor\n",
    "batch_processor = AsyncBatchProcessor(max_concurrent=3)\n",
    "print_result(\"Async batch processor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619cbde",
   "metadata": {},
   "source": [
    "## Testing Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57416d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Testing Batch Processing\", \"Demonstrating concurrent batch operations\")\n",
    "\n",
    "async def test_batch_processing():\n",
    "    \"\"\"Test batch processing capabilities.\"\"\"\n",
    "    \n",
    "    # Prepare test data\n",
    "    qa_items = [\n",
    "        {\"id\": \"qa_1\", \"question\": \"What is machine learning?\"},\n",
    "        {\"id\": \"qa_2\", \"question\": \"How does deep learning work?\"},\n",
    "        {\"id\": \"qa_3\", \"question\": \"What are neural networks?\"},\n",
    "        {\"id\": \"qa_4\", \"question\": \"Explain artificial intelligence\"},\n",
    "    ]\n",
    "    \n",
    "    summarization_items = [\n",
    "        {\n",
    "            \"id\": \"sum_1\", \n",
    "            \"text\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can access data and use it to learn for themselves.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"sum_2\", \n",
    "            \"text\": \"Deep learning is part of a broader family of machine learning methods based on artificial neural networks. It uses multiple layers to progressively extract higher-level features from raw input.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"sum_3\", \n",
    "            \"text\": \"Natural language processing combines computational linguistics with statistical, machine learning, and deep learning models to help computers understand and communicate with human language.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    classification_items = [\n",
    "        {\"id\": \"cls_1\", \"text\": \"The stock market reached new highs today\"},\n",
    "        {\"id\": \"cls_2\", \"text\": \"Scientists discovered a new species of bird\"},\n",
    "        {\"id\": \"cls_3\", \"text\": \"The new smartphone has amazing camera features\"},\n",
    "        {\"id\": \"cls_4\", \"text\": \"Climate change affects global weather patterns\"}\n",
    "    ]\n",
    "    \n",
    "    # Test 1: Single batch processing\n",
    "    print(\"\\n1. Single Batch Processing (Q&A):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    qa_start = time.time()\n",
    "    qa_results = await batch_processor.batch_process(qa_items, \"qa\")\n",
    "    qa_time = time.time() - qa_start\n",
    "    \n",
    "    print(f\"\\nQ&A Batch Results (took {qa_time:.2f}s):\")\n",
    "    for result in qa_results:\n",
    "        status_icon = \"âœ…\" if result[\"status\"] == \"success\" else \"âŒ\"\n",
    "        print(f\"{status_icon} {result['id']}: {result['output'][:80]}...\")\n",
    "        print(f\"   Processing time: {result['processing_time']:.2f}s\")\n",
    "    \n",
    "    # Test 2: Mixed batch processing\n",
    "    print(\"\\n\\n2. Mixed Batch Processing:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    mixed_start = time.time()\n",
    "    mixed_batches = {\n",
    "        \"summarize\": summarization_items,\n",
    "        \"classify\": classification_items\n",
    "    }\n",
    "    \n",
    "    mixed_results = await batch_processor.mixed_batch_process(mixed_batches)\n",
    "    mixed_time = time.time() - mixed_start\n",
    "    \n",
    "    print(f\"\\nMixed Batch Results (took {mixed_time:.2f}s):\")\n",
    "    \n",
    "    for batch_type, results in mixed_results.items():\n",
    "        print(f\"\\n{batch_type.upper()} Results:\")\n",
    "        for result in results:\n",
    "            status_icon = \"âœ…\" if result[\"status\"] == \"success\" else \"âŒ\"\n",
    "            print(f\"  {status_icon} {result['id']}: {result['output'][:60]}...\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\nðŸ“Š Performance Summary:\")\n",
    "    total_items = len(qa_items) + len(summarization_items) + len(classification_items)\n",
    "    total_time = qa_time + mixed_time\n",
    "    avg_time_per_item = total_time / total_items\n",
    "    \n",
    "    print(f\"Total items processed: {total_items}\")\n",
    "    print(f\"Total processing time: {total_time:.2f}s\")\n",
    "    print(f\"Average time per item: {avg_time_per_item:.2f}s\")\n",
    "\n",
    "# Run batch processing test\n",
    "try:\n",
    "    await test_batch_processing()\n",
    "except Exception as e:\n",
    "    print_error(f\"Error in batch processing test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74824417",
   "metadata": {},
   "source": [
    "## Async Error Handling and Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d871885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustAsyncModule(dspy.Module):\n",
    "    \"\"\"Async module with robust error handling and retry logic.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3, retry_delay: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.predictor = dspy.Predict(\"prompt -> response\")\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"retries_performed\": 0\n",
    "        }\n",
    "    \n",
    "    async def robust_forward(self, prompt: str, timeout: float = 30.0) -> Dict[str, Any]:\n",
    "        \"\"\"Forward with error handling, retries, and timeout.\"\"\"\n",
    "        \n",
    "        self.stats[\"total_requests\"] += 1\n",
    "        \n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            try:\n",
    "                # Run with timeout\n",
    "                result = await asyncio.wait_for(\n",
    "                    self._async_predict(prompt),\n",
    "                    timeout=timeout\n",
    "                )\n",
    "                \n",
    "                self.stats[\"successful_requests\"] += 1\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"result\": result,\n",
    "                    \"attempts\": attempt + 1,\n",
    "                    \"prompt\": prompt[:50] + \"...\",\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "            except asyncio.TimeoutError:\n",
    "                error_msg = f\"Timeout after {timeout}s on attempt {attempt + 1}\"\n",
    "                if attempt < self.max_retries:\n",
    "                    print(f\"â° {error_msg}, retrying...\")\n",
    "                    self.stats[\"retries_performed\"] += 1\n",
    "                    await asyncio.sleep(self.retry_delay * (attempt + 1))  # Exponential backoff\n",
    "                    continue\n",
    "                else:\n",
    "                    self.stats[\"failed_requests\"] += 1\n",
    "                    return {\n",
    "                        \"success\": False,\n",
    "                        \"error\": error_msg,\n",
    "                        \"attempts\": attempt + 1,\n",
    "                        \"prompt\": prompt[:50] + \"...\",\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error on attempt {attempt + 1}: {str(e)}\"\n",
    "                if attempt < self.max_retries:\n",
    "                    print(f\"âŒ {error_msg}, retrying...\")\n",
    "                    self.stats[\"retries_performed\"] += 1\n",
    "                    await asyncio.sleep(self.retry_delay * (attempt + 1))\n",
    "                    continue\n",
    "                else:\n",
    "                    self.stats[\"failed_requests\"] += 1\n",
    "                    return {\n",
    "                        \"success\": False,\n",
    "                        \"error\": error_msg,\n",
    "                        \"attempts\": attempt + 1,\n",
    "                        \"prompt\": prompt[:50] + \"...\",\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "    \n",
    "    async def _async_predict(self, prompt: str):\n",
    "        \"\"\"Run prediction in thread pool.\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            result = await loop.run_in_executor(\n",
    "                executor,\n",
    "                lambda: self.predictor(prompt=prompt)\n",
    "            )\n",
    "        return result\n",
    "    \n",
    "    async def batch_robust_forward(self, prompts: List[str], max_concurrent: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process multiple prompts with error handling.\"\"\"\n",
    "        \n",
    "        semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        \n",
    "        async def process_with_semaphore(prompt: str) -> Dict[str, Any]:\n",
    "            async with semaphore:\n",
    "                return await self.robust_forward(prompt)\n",
    "        \n",
    "        # Create tasks\n",
    "        tasks = [\n",
    "            asyncio.create_task(process_with_semaphore(prompt))\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "        \n",
    "        # Wait for all to complete (don't fail on individual errors)\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Handle any task-level exceptions\n",
    "        processed_results = []\n",
    "        for i, result in enumerate(results):\n",
    "            if isinstance(result, Exception):\n",
    "                processed_results.append({\n",
    "                    \"success\": False,\n",
    "                    \"error\": f\"Task exception: {str(result)}\",\n",
    "                    \"prompt\": prompts[i][:50] + \"...\",\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                })\n",
    "            else:\n",
    "                processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance and reliability statistics.\"\"\"\n",
    "        success_rate = (self.stats[\"successful_requests\"] / max(1, self.stats[\"total_requests\"])) * 100\n",
    "        \n",
    "        return {\n",
    "            **self.stats,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"avg_retries_per_request\": self.stats[\"retries_performed\"] / max(1, self.stats[\"total_requests\"])\n",
    "        }\n",
    "\n",
    "# Initialize robust async module\n",
    "robust_module = RobustAsyncModule(max_retries=2, retry_delay=0.5)\n",
    "print_result(\"Robust async module initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20590d",
   "metadata": {},
   "source": [
    "## Testing Error Handling and Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdaea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Testing Error Handling\", \"Demonstrating robust async processing\")\n",
    "\n",
    "async def test_error_handling():\n",
    "    \"\"\"Test error handling and retry mechanisms.\"\"\"\n",
    "    \n",
    "    # Test prompts (some might cause issues)\n",
    "    test_prompts = [\n",
    "        \"Explain the concept of machine learning\",\n",
    "        \"What is artificial intelligence?\",\n",
    "        \"Describe neural network architectures\",\n",
    "        \"How does deep learning work?\",\n",
    "        \"What are the applications of AI?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nTesting robust processing with {len(test_prompts)} prompts...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process all prompts\n",
    "    results = await robust_module.batch_robust_forward(\n",
    "        test_prompts, \n",
    "        max_concurrent=2\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Analyze results\n",
    "    successful_results = [r for r in results if r.get(\"success\", False)]\n",
    "    failed_results = [r for r in results if not r.get(\"success\", False)]\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Processing Results:\")\n",
    "    print(f\"Total prompts: {len(test_prompts)}\")\n",
    "    print(f\"Successful: {len(successful_results)}\")\n",
    "    print(f\"Failed: {len(failed_results)}\")\n",
    "    print(f\"Processing time: {processing_time:.2f}s\")\n",
    "    \n",
    "    # Show individual results\n",
    "    print(f\"\\nâœ… Successful Results:\")\n",
    "    for result in successful_results:\n",
    "        attempts = result.get(\"attempts\", 1)\n",
    "        response_preview = result[\"result\"].response[:80] + \"...\" if hasattr(result[\"result\"], 'response') else \"No response\"\n",
    "        print(f\"  â€¢ {result['prompt']} (attempts: {attempts})\")\n",
    "        print(f\"    Response: {response_preview}\")\n",
    "    \n",
    "    if failed_results:\n",
    "        print(f\"\\nâŒ Failed Results:\")\n",
    "        for result in failed_results:\n",
    "            attempts = result.get(\"attempts\", 1)\n",
    "            print(f\"  â€¢ {result['prompt']} (attempts: {attempts})\")\n",
    "            print(f\"    Error: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Show module statistics\n",
    "    stats = robust_module.get_stats()\n",
    "    print(f\"\\nðŸ“ˆ Module Statistics:\")\n",
    "    print(f\"Total requests: {stats['total_requests']}\")\n",
    "    print(f\"Success rate: {stats['success_rate']:.1f}%\")\n",
    "    print(f\"Retries performed: {stats['retries_performed']}\")\n",
    "    print(f\"Average retries per request: {stats['avg_retries_per_request']:.2f}\")\n",
    "\n",
    "# Run error handling test\n",
    "try:\n",
    "    await test_error_handling()\n",
    "except Exception as e:\n",
    "    print_error(f\"Error in error handling test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595e726",
   "metadata": {},
   "source": [
    "## Performance Monitoring for Async Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncPerformanceMonitor:\n",
    "    \"\"\"Monitor performance of async DSPy operations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"request_times\": [],\n",
    "            \"queue_times\": [],\n",
    "            \"concurrent_requests\": [],\n",
    "            \"throughput_samples\": [],\n",
    "            \"error_counts\": 0,\n",
    "            \"total_requests\": 0\n",
    "        }\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    async def monitored_request(self, async_func, *args, **kwargs):\n",
    "        \"\"\"Wrap an async function with performance monitoring.\"\"\"\n",
    "        \n",
    "        queue_start = time.time()\n",
    "        request_start = time.time()\n",
    "        \n",
    "        self.metrics[\"total_requests\"] += 1\n",
    "        \n",
    "        try:\n",
    "            result = await async_func(*args, **kwargs)\n",
    "            request_time = time.time() - request_start\n",
    "            \n",
    "            self.metrics[\"request_times\"].append(request_time)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics[\"error_counts\"] += 1\n",
    "            raise e\n",
    "    \n",
    "    async def benchmark_concurrency(self, async_func, test_data: List[Any], \n",
    "                                  concurrency_levels: List[int]) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Benchmark different concurrency levels.\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for concurrency in concurrency_levels:\n",
    "            print(f\"\\nTesting concurrency level: {concurrency}\")\n",
    "            \n",
    "            # Reset metrics for this test\n",
    "            self.metrics[\"request_times\"] = []\n",
    "            self.metrics[\"error_counts\"] = 0\n",
    "            self.metrics[\"total_requests\"] = 0\n",
    "            \n",
    "            semaphore = asyncio.Semaphore(concurrency)\n",
    "            \n",
    "            async def limited_request(data):\n",
    "                async with semaphore:\n",
    "                    return await self.monitored_request(async_func, data)\n",
    "            \n",
    "            # Run benchmark\n",
    "            start_time = time.time()\n",
    "            \n",
    "            tasks = [asyncio.create_task(limited_request(data)) for data in test_data]\n",
    "            await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if self.metrics[\"request_times\"]:\n",
    "                avg_request_time = sum(self.metrics[\"request_times\"]) / len(self.metrics[\"request_times\"])\n",
    "                throughput = len(test_data) / total_time\n",
    "                success_rate = ((self.metrics[\"total_requests\"] - self.metrics[\"error_counts\"]) / \n",
    "                              self.metrics[\"total_requests\"]) * 100\n",
    "            else:\n",
    "                avg_request_time = 0\n",
    "                throughput = 0\n",
    "                success_rate = 0\n",
    "            \n",
    "            results[f\"concurrency_{concurrency}\"] = {\n",
    "                \"total_time\": total_time,\n",
    "                \"avg_request_time\": avg_request_time,\n",
    "                \"throughput\": throughput,\n",
    "                \"success_rate\": success_rate,\n",
    "                \"total_requests\": self.metrics[\"total_requests\"],\n",
    "                \"errors\": self.metrics[\"error_counts\"]\n",
    "            }\n",
    "            \n",
    "            print(f\"  Total time: {total_time:.2f}s\")\n",
    "            print(f\"  Throughput: {throughput:.2f} requests/second\")\n",
    "            print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive performance summary.\"\"\"\n",
    "        \n",
    "        if not self.metrics[\"request_times\"]:\n",
    "            return {\"status\": \"no_data\"}\n",
    "        \n",
    "        request_times = self.metrics[\"request_times\"]\n",
    "        \n",
    "        return {\n",
    "            \"total_requests\": self.metrics[\"total_requests\"],\n",
    "            \"total_errors\": self.metrics[\"error_counts\"],\n",
    "            \"success_rate\": ((self.metrics[\"total_requests\"] - self.metrics[\"error_counts\"]) / \n",
    "                           self.metrics[\"total_requests\"]) * 100,\n",
    "            \"avg_request_time\": sum(request_times) / len(request_times),\n",
    "            \"min_request_time\": min(request_times),\n",
    "            \"max_request_time\": max(request_times),\n",
    "            \"total_runtime\": time.time() - self.start_time\n",
    "        }\n",
    "\n",
    "# Initialize performance monitor\n",
    "perf_monitor = AsyncPerformanceMonitor()\n",
    "print_result(\"Performance monitor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69693a8e",
   "metadata": {},
   "source": [
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54213c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Performance Benchmarking\", \"Testing different concurrency levels\")\n",
    "\n",
    "async def benchmark_async_performance():\n",
    "    \"\"\"Benchmark async performance across different concurrency levels.\"\"\"\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_prompts = [\n",
    "        \"Explain machine learning concepts\",\n",
    "        \"What is artificial intelligence?\",\n",
    "        \"Describe neural networks\",\n",
    "        \"How does deep learning work?\",\n",
    "        \"What are transformers in AI?\",\n",
    "        \"Explain computer vision\",\n",
    "        \"What is natural language processing?\",\n",
    "        \"Describe reinforcement learning\"\n",
    "    ]\n",
    "    \n",
    "    # Test different concurrency levels\n",
    "    concurrency_levels = [1, 2, 4]\n",
    "    \n",
    "    print(f\"Benchmarking with {len(test_prompts)} prompts across {len(concurrency_levels)} concurrency levels...\")\n",
    "    \n",
    "    # Run benchmark\n",
    "    benchmark_results = await perf_monitor.benchmark_concurrency(\n",
    "        async_module.async_forward,\n",
    "        [{\"prompt\": prompt} for prompt in test_prompts],\n",
    "        concurrency_levels\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nðŸ“Š Benchmark Results:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Concurrency':<12} {'Time(s)':<10} {'Throughput':<12} {'Success%':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for level_name, metrics in benchmark_results.items():\n",
    "        concurrency = level_name.split('_')[1]\n",
    "        print(f\"{concurrency:<12} {metrics['total_time']:<10.2f} \"\n",
    "              f\"{metrics['throughput']:<12.2f} {metrics['success_rate']:<10.1f}\")\n",
    "    \n",
    "    # Find optimal concurrency\n",
    "    best_throughput = max(benchmark_results.values(), key=lambda x: x['throughput'])\n",
    "    best_level = [k for k, v in benchmark_results.items() if v == best_throughput][0]\n",
    "    \n",
    "    print(f\"\\nðŸ† Best Performance:\")\n",
    "    print(f\"Optimal concurrency: {best_level.split('_')[1]}\")\n",
    "    print(f\"Peak throughput: {best_throughput['throughput']:.2f} requests/second\")\n",
    "    print(f\"Total time: {best_throughput['total_time']:.2f}s\")\n",
    "    \n",
    "    # Performance recommendations\n",
    "    print(f\"\\nðŸ’¡ Performance Recommendations:\")\n",
    "    \n",
    "    if len(benchmark_results) > 1:\n",
    "        concurrent_best = max([v for k, v in benchmark_results.items() if 'concurrency_1' not in k], \n",
    "                            key=lambda x: x['throughput'], default=None)\n",
    "        sequential = benchmark_results.get('concurrency_1')\n",
    "        \n",
    "        if concurrent_best and sequential:\n",
    "            speedup = concurrent_best['throughput'] / sequential['throughput']\n",
    "            print(f\"â€¢ Concurrency provides {speedup:.1f}x speedup over sequential processing\")\n",
    "            \n",
    "            if speedup > 2:\n",
    "                print(\"â€¢ High concurrency benefit - consider increasing concurrent requests\")\n",
    "            elif speedup < 1.5:\n",
    "                print(\"â€¢ Low concurrency benefit - may be limited by API rate limits\")\n",
    "            else:\n",
    "                print(\"â€¢ Moderate concurrency benefit - current setup is reasonable\")\n",
    "\n",
    "# Run performance benchmark\n",
    "try:\n",
    "    await benchmark_async_performance()\n",
    "except Exception as e:\n",
    "    print_error(f\"Error in performance benchmark: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f3f40",
   "metadata": {},
   "source": [
    "## Conclusion and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aafe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Summary\", \"Async Processing with DSPy - Key Takeaways\")\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸŽ‰ Async Processing Tutorial Complete!\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "\n",
    "1. **Basic Async Operations**: Converting sync DSPy modules to async\n",
    "2. **Batch Processing**: Concurrent processing of multiple requests\n",
    "3. **Error Handling**: Robust retry logic and timeout management\n",
    "4. **Performance Monitoring**: Metrics and benchmarking tools\n",
    "5. **Concurrency Control**: Semaphores and rate limiting\n",
    "\n",
    "### DSPy Async Integration:\n",
    "\n",
    "- **Thread Pool Execution**: Running DSPy predictions in thread pools\n",
    "- **Async Wrappers**: Converting sync modules to async interfaces\n",
    "- **Batch Operations**: Processing multiple inputs concurrently\n",
    "- **Error Recovery**: Handling failures gracefully with retries\n",
    "\n",
    "### Performance Benefits:\n",
    "\n",
    "- **Throughput**: Significant improvement with concurrent processing\n",
    "- **Resource Utilization**: Better CPU and network efficiency\n",
    "- **Scalability**: Handle larger workloads without blocking\n",
    "- **Responsiveness**: Non-blocking operations for better UX\n",
    "\n",
    "### Best Practices for Production:\n",
    "\n",
    "1. **Concurrency Limits**: Use semaphores to control concurrent requests\n",
    "2. **Error Handling**: Implement retry logic with exponential backoff\n",
    "3. **Timeout Management**: Set appropriate timeouts for operations\n",
    "4. **Resource Monitoring**: Track performance metrics and system resources\n",
    "5. **Rate Limiting**: Respect API limits and implement backpressure\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Batch Processing**: Large-scale data processing workflows\n",
    "- **Real-time Applications**: Chat systems and interactive applications\n",
    "- **ETL Pipelines**: Data transformation and analysis\n",
    "- **API Services**: High-throughput API endpoints\n",
    "- **Background Processing**: Asynchronous task processing\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Final performance summary\n",
    "final_summary = perf_monitor.get_performance_summary()\n",
    "if final_summary.get(\"status\") != \"no_data\":\n",
    "    print(f\"\\nðŸ“ˆ Final Performance Summary:\")\n",
    "    print(f\"Total requests processed: {final_summary['total_requests']}\")\n",
    "    print(f\"Overall success rate: {final_summary['success_rate']:.1f}%\")\n",
    "    print(f\"Average request time: {final_summary['avg_request_time']:.3f}s\")\n",
    "    print(f\"Total runtime: {final_summary['total_runtime']:.2f}s\")\n",
    "\n",
    "print(\"\\nðŸš€ You're now ready to build scalable async DSPy applications!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
