{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4cb67c0",
   "metadata": {},
   "source": [
    "# Output Refinement: Best-of-N and Refine\n",
    "\n",
    "This tutorial demonstrates how to use DSPy's output refinement techniques to improve the quality of generated responses.\n",
    "\n",
    "## What is Output Refinement?\n",
    "\n",
    "Output refinement involves generating multiple candidate outputs and either selecting the best one (Best-of-N) or iteratively improving an output (Refine). These techniques help improve response quality and consistency.\n",
    "\n",
    "## Key Techniques:\n",
    "- **Best-of-N**: Generate N outputs and select the best one\n",
    "- **Refine**: Iteratively improve an output through multiple passes\n",
    "- **Hybrid approaches**: Combine both techniques for optimal results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c44a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "try:\n",
    "    import dspy\n",
    "except ImportError:\n",
    "    install_package(\"dspy\")\n",
    "    import dspy\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d3853",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d97b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DSPy\n",
    "lm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv('OPENAI_API_KEY'))\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(\"DSPy configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db296a",
   "metadata": {},
   "source": [
    "## Best-of-N Sampling\n",
    "\n",
    "The Best-of-N technique generates multiple candidate outputs and selects the best one based on a quality metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseGenerationSignature(dspy.Signature):\n",
    "    \"\"\"Generate a high-quality response to a question.\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(desc=\"The question to answer\")\n",
    "    response: str = dspy.OutputField(desc=\"A comprehensive and accurate response\")\n",
    "\n",
    "class ResponseEvaluationSignature(dspy.Signature):\n",
    "    \"\"\"Evaluate the quality of a response.\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(desc=\"The original question\")\n",
    "    response: str = dspy.InputField(desc=\"The response to evaluate\")\n",
    "    score: float = dspy.OutputField(desc=\"Quality score from 0.0 to 1.0\")\n",
    "    reasoning: str = dspy.OutputField(desc=\"Explanation for the score\")\n",
    "\n",
    "class BestOfNGenerator(dspy.Module):\n",
    "    \"\"\"Generate N responses and select the best one.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_candidates: int = 3):\n",
    "        super().__init__()\n",
    "        self.n_candidates = n_candidates\n",
    "        self.generator = dspy.ChainOfThought(ResponseGenerationSignature)\n",
    "        self.evaluator = dspy.ChainOfThought(ResponseEvaluationSignature)\n",
    "    \n",
    "    def forward(self, question: str) -> dspy.Prediction:\n",
    "        # Generate N candidate responses\n",
    "        candidates = []\n",
    "        for i in range(self.n_candidates):\n",
    "            try:\n",
    "                response = self.generator(question=question)\n",
    "                candidates.append(response.response)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating candidate {i}: {e}\")\n",
    "        \n",
    "        if not candidates:\n",
    "            return dspy.Prediction(best_response=\"Error: No candidates generated\")\n",
    "        \n",
    "        # Evaluate each candidate\n",
    "        evaluations = []\n",
    "        for candidate in candidates:\n",
    "            try:\n",
    "                eval_result = self.evaluator(question=question, response=candidate)\n",
    "                evaluations.append({\n",
    "                    'response': candidate,\n",
    "                    'score': float(eval_result.score),\n",
    "                    'reasoning': eval_result.reasoning\n",
    "                })\n",
    "            except Exception as e:\n",
    "                evaluations.append({\n",
    "                    'response': candidate,\n",
    "                    'score': 0.0,\n",
    "                    'reasoning': f\"Evaluation error: {e}\"\n",
    "                })\n",
    "        \n",
    "        # Select the best candidate\n",
    "        best_candidate = max(evaluations, key=lambda x: x['score'])\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            best_response=best_candidate['response'],\n",
    "            best_score=best_candidate['score'],\n",
    "            all_candidates=evaluations,\n",
    "            selection_reasoning=best_candidate['reasoning']\n",
    "        )\n",
    "\n",
    "# Test Best-of-N\n",
    "best_of_n = BestOfNGenerator(n_candidates=3)\n",
    "\n",
    "test_question = \"What are the key benefits of renewable energy sources?\"\n",
    "result = best_of_n(question=test_question)\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"\\nBest Response (Score: {result.best_score}):\")\n",
    "print(result.best_response)\n",
    "print(f\"\\nSelection Reasoning: {result.selection_reasoning}\")\n",
    "print(f\"\\nTotal candidates evaluated: {len(result.all_candidates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb2c109",
   "metadata": {},
   "source": [
    "## Refine Technique\n",
    "\n",
    "The Refine technique iteratively improves an initial response through multiple refinement passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefinementSignature(dspy.Signature):\n",
    "    \"\"\"Refine and improve a response based on feedback.\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(desc=\"The original question\")\n",
    "    current_response: str = dspy.InputField(desc=\"The current response to improve\")\n",
    "    feedback: str = dspy.InputField(desc=\"Specific feedback for improvement\")\n",
    "    refined_response: str = dspy.OutputField(desc=\"The improved response\")\n",
    "\n",
    "class FeedbackGenerationSignature(dspy.Signature):\n",
    "    \"\"\"Generate specific feedback for improving a response.\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(desc=\"The original question\")\n",
    "    response: str = dspy.InputField(desc=\"The response to provide feedback on\")\n",
    "    feedback: str = dspy.OutputField(desc=\"Specific, actionable feedback for improvement\")\n",
    "\n",
    "class IterativeRefiner(dspy.Module):\n",
    "    \"\"\"Iteratively refine responses through multiple passes.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        super().__init__()\n",
    "        self.max_iterations = max_iterations\n",
    "        self.generator = dspy.ChainOfThought(ResponseGenerationSignature)\n",
    "        self.feedback_generator = dspy.ChainOfThought(FeedbackGenerationSignature)\n",
    "        self.refiner = dspy.ChainOfThought(RefinementSignature)\n",
    "        self.evaluator = dspy.ChainOfThought(ResponseEvaluationSignature)\n",
    "    \n",
    "    def forward(self, question: str) -> dspy.Prediction:\n",
    "        # Generate initial response\n",
    "        initial_response = self.generator(question=question)\n",
    "        current_response = initial_response.response\n",
    "        \n",
    "        refinement_history = [\n",
    "            {\n",
    "                'iteration': 0,\n",
    "                'response': current_response,\n",
    "                'feedback': 'Initial generation'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Iterative refinement\n",
    "        for iteration in range(1, self.max_iterations + 1):\n",
    "            # Generate feedback\n",
    "            feedback_result = self.feedback_generator(\n",
    "                question=question,\n",
    "                response=current_response\n",
    "            )\n",
    "            \n",
    "            # Refine response based on feedback\n",
    "            refinement_result = self.refiner(\n",
    "                question=question,\n",
    "                current_response=current_response,\n",
    "                feedback=feedback_result.feedback\n",
    "            )\n",
    "            \n",
    "            current_response = refinement_result.refined_response\n",
    "            \n",
    "            refinement_history.append({\n",
    "                'iteration': iteration,\n",
    "                'response': current_response,\n",
    "                'feedback': feedback_result.feedback\n",
    "            })\n",
    "        \n",
    "        # Evaluate final response\n",
    "        final_evaluation = self.evaluator(\n",
    "            question=question,\n",
    "            response=current_response\n",
    "        )\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            final_response=current_response,\n",
    "            final_score=float(final_evaluation.score),\n",
    "            evaluation_reasoning=final_evaluation.reasoning,\n",
    "            refinement_history=refinement_history\n",
    "        )\n",
    "\n",
    "# Test Iterative Refinement\n",
    "refiner = IterativeRefiner(max_iterations=2)\n",
    "\n",
    "test_question = \"Explain the concept of machine learning and its applications.\"\n",
    "refined_result = refiner(question=test_question)\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"\\nFinal Response (Score: {refined_result.final_score}):\")\n",
    "print(refined_result.final_response)\n",
    "print(f\"\\nEvaluation: {refined_result.evaluation_reasoning}\")\n",
    "\n",
    "print(\"\\nRefinement History:\")\n",
    "for step in refined_result.refinement_history:\n",
    "    print(f\"\\nIteration {step['iteration']}:\")\n",
    "    print(f\"Feedback: {step['feedback']}\")\n",
    "    print(f\"Response: {step['response'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a184f30",
   "metadata": {},
   "source": [
    "## Hybrid Approach: Best-of-N + Refine\n",
    "\n",
    "Combine both techniques for maximum quality improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRefinementSystem(dspy.Module):\n",
    "    \"\"\"Combine Best-of-N sampling with iterative refinement.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_candidates: int = 3, max_refinements: int = 2):\n",
    "        super().__init__()\n",
    "        self.n_candidates = n_candidates\n",
    "        self.max_refinements = max_refinements\n",
    "        self.best_of_n = BestOfNGenerator(n_candidates)\n",
    "        self.refiner = IterativeRefiner(max_refinements)\n",
    "        self.evaluator = dspy.ChainOfThought(ResponseEvaluationSignature)\n",
    "    \n",
    "    def forward(self, question: str) -> dspy.Prediction:\n",
    "        # Step 1: Generate best initial response using Best-of-N\n",
    "        best_initial = self.best_of_n(question=question)\n",
    "        \n",
    "        # Step 2: Refine the best initial response\n",
    "        # Create a custom refiner that starts with the best response\n",
    "        generator = dspy.ChainOfThought(ResponseGenerationSignature)\n",
    "        feedback_generator = dspy.ChainOfThought(FeedbackGenerationSignature)\n",
    "        refiner_module = dspy.ChainOfThought(RefinementSignature)\n",
    "        \n",
    "        current_response = best_initial.best_response\n",
    "        refinement_steps = []\n",
    "        \n",
    "        for iteration in range(self.max_refinements):\n",
    "            # Generate feedback\n",
    "            feedback_result = feedback_generator(\n",
    "                question=question,\n",
    "                response=current_response\n",
    "            )\n",
    "            \n",
    "            # Refine response\n",
    "            refinement_result = refiner_module(\n",
    "                question=question,\n",
    "                current_response=current_response,\n",
    "                feedback=feedback_result.feedback\n",
    "            )\n",
    "            \n",
    "            current_response = refinement_result.refined_response\n",
    "            \n",
    "            refinement_steps.append({\n",
    "                'iteration': iteration + 1,\n",
    "                'feedback': feedback_result.feedback,\n",
    "                'response': current_response\n",
    "            })\n",
    "        \n",
    "        # Final evaluation\n",
    "        final_evaluation = self.evaluator(\n",
    "            question=question,\n",
    "            response=current_response\n",
    "        )\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            final_response=current_response,\n",
    "            final_score=float(final_evaluation.score),\n",
    "            initial_best_score=best_initial.best_score,\n",
    "            improvement=float(final_evaluation.score) - best_initial.best_score,\n",
    "            initial_candidates=best_initial.all_candidates,\n",
    "            refinement_steps=refinement_steps,\n",
    "            evaluation_reasoning=final_evaluation.reasoning\n",
    "        )\n",
    "\n",
    "# Test Hybrid System\n",
    "hybrid_system = HybridRefinementSystem(n_candidates=3, max_refinements=2)\n",
    "\n",
    "test_question = \"What are the ethical implications of artificial intelligence in healthcare?\"\n",
    "hybrid_result = hybrid_system(question=test_question)\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"\\nInitial Best Score: {hybrid_result.initial_best_score}\")\n",
    "print(f\"Final Score: {hybrid_result.final_score}\")\n",
    "print(f\"Improvement: {hybrid_result.improvement:.3f}\")\n",
    "print(f\"\\nFinal Response:\")\n",
    "print(hybrid_result.final_response)\n",
    "print(f\"\\nEvaluation: {hybrid_result.evaluation_reasoning}\")\n",
    "\n",
    "print(f\"\\nInitial candidates considered: {len(hybrid_result.initial_candidates)}\")\n",
    "print(f\"Refinement steps: {len(hybrid_result.refinement_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb46be2",
   "metadata": {},
   "source": [
    "## Advanced Refinement Strategies\n",
    "\n",
    "Let's explore more sophisticated refinement strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiCriteriaEvaluationSignature(dspy.Signature):\n",
    "    \"\"\"Evaluate response on multiple criteria.\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(desc=\"The original question\")\n",
    "    response: str = dspy.InputField(desc=\"The response to evaluate\")\n",
    "    accuracy_score: float = dspy.OutputField(desc=\"Accuracy score (0.0-1.0)\")\n",
    "    clarity_score: float = dspy.OutputField(desc=\"Clarity score (0.0-1.0)\")\n",
    "    completeness_score: float = dspy.OutputField(desc=\"Completeness score (0.0-1.0)\")\n",
    "    overall_score: float = dspy.OutputField(desc=\"Overall score (0.0-1.0)\")\n",
    "\n",
    "class TargetedRefinementSignature(dspy.Signature):\n",
    "    \"\"\"Refine response focusing on specific aspects.\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(desc=\"The original question\")\n",
    "    current_response: str = dspy.InputField(desc=\"Current response\")\n",
    "    focus_area: str = dspy.InputField(desc=\"Specific aspect to improve (accuracy, clarity, completeness)\")\n",
    "    refined_response: str = dspy.OutputField(desc=\"Response refined for the focus area\")\n",
    "\n",
    "class AdvancedRefinementSystem(dspy.Module):\n",
    "    \"\"\"Advanced refinement with multi-criteria evaluation and targeted improvements.\"\"\"\n",
    "    \n",
    "    def __init__(self, improvement_threshold: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.improvement_threshold = improvement_threshold\n",
    "        self.generator = dspy.ChainOfThought(ResponseGenerationSignature)\n",
    "        self.multi_evaluator = dspy.ChainOfThought(MultiCriteriaEvaluationSignature)\n",
    "        self.targeted_refiner = dspy.ChainOfThought(TargetedRefinementSignature)\n",
    "    \n",
    "    def forward(self, question: str) -> dspy.Prediction:\n",
    "        # Generate initial response\n",
    "        initial_response = self.generator(question=question)\n",
    "        current_response = initial_response.response\n",
    "        \n",
    "        # Initial evaluation\n",
    "        current_eval = self.multi_evaluator(\n",
    "            question=question,\n",
    "            response=current_response\n",
    "        )\n",
    "        \n",
    "        refinement_log = [{\n",
    "            'step': 0,\n",
    "            'response': current_response,\n",
    "            'accuracy': float(current_eval.accuracy_score),\n",
    "            'clarity': float(current_eval.clarity_score),\n",
    "            'completeness': float(current_eval.completeness_score),\n",
    "            'overall': float(current_eval.overall_score),\n",
    "            'focus_area': 'initial'\n",
    "        }]\n",
    "        \n",
    "        # Identify areas needing improvement\n",
    "        criteria_scores = {\n",
    "            'accuracy': float(current_eval.accuracy_score),\n",
    "            'clarity': float(current_eval.clarity_score),\n",
    "            'completeness': float(current_eval.completeness_score)\n",
    "        }\n",
    "        \n",
    "        max_iterations = 3\n",
    "        iteration = 0\n",
    "        \n",
    "        while iteration < max_iterations:\n",
    "            # Find the lowest scoring criterion\n",
    "            focus_area = min(criteria_scores, key=criteria_scores.get)\n",
    "            \n",
    "            # If all scores are good enough, stop\n",
    "            if criteria_scores[focus_area] > 0.8:\n",
    "                break\n",
    "            \n",
    "            # Refine focusing on the weakest area\n",
    "            refined_result = self.targeted_refiner(\n",
    "                question=question,\n",
    "                current_response=current_response,\n",
    "                focus_area=focus_area\n",
    "            )\n",
    "            \n",
    "            # Evaluate the refined response\n",
    "            new_eval = self.multi_evaluator(\n",
    "                question=question,\n",
    "                response=refined_result.refined_response\n",
    "            )\n",
    "            \n",
    "            new_overall = float(new_eval.overall_score)\n",
    "            \n",
    "            # Check if improvement is significant\n",
    "            if new_overall > float(current_eval.overall_score) + self.improvement_threshold:\n",
    "                current_response = refined_result.refined_response\n",
    "                current_eval = new_eval\n",
    "                criteria_scores = {\n",
    "                    'accuracy': float(new_eval.accuracy_score),\n",
    "                    'clarity': float(new_eval.clarity_score),\n",
    "                    'completeness': float(new_eval.completeness_score)\n",
    "                }\n",
    "                \n",
    "                refinement_log.append({\n",
    "                    'step': iteration + 1,\n",
    "                    'response': current_response,\n",
    "                    'accuracy': criteria_scores['accuracy'],\n",
    "                    'clarity': criteria_scores['clarity'],\n",
    "                    'completeness': criteria_scores['completeness'],\n",
    "                    'overall': new_overall,\n",
    "                    'focus_area': focus_area\n",
    "                })\n",
    "            else:\n",
    "                # No significant improvement, stop\n",
    "                break\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            final_response=current_response,\n",
    "            final_scores=criteria_scores,\n",
    "            overall_score=float(current_eval.overall_score),\n",
    "            refinement_log=refinement_log,\n",
    "            iterations_performed=len(refinement_log) - 1\n",
    "        )\n",
    "\n",
    "# Test Advanced Refinement\n",
    "advanced_refiner = AdvancedRefinementSystem(improvement_threshold=0.05)\n",
    "\n",
    "complex_question = \"Analyze the environmental, economic, and social impacts of transitioning to renewable energy sources.\"\n",
    "advanced_result = advanced_refiner(question=complex_question)\n",
    "\n",
    "print(f\"Question: {complex_question}\")\n",
    "print(f\"\\nFinal Response:\")\n",
    "print(advanced_result.final_response)\n",
    "print(f\"\\nFinal Scores:\")\n",
    "for criterion, score in advanced_result.final_scores.items():\n",
    "    print(f\"  {criterion.capitalize()}: {score:.3f}\")\n",
    "print(f\"  Overall: {advanced_result.overall_score:.3f}\")\n",
    "print(f\"\\nIterations performed: {advanced_result.iterations_performed}\")\n",
    "\n",
    "print(\"\\nRefinement Progress:\")\n",
    "for step in advanced_result.refinement_log:\n",
    "    print(f\"Step {step['step']} (Focus: {step['focus_area']}): Overall = {step['overall']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b80b7",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the performance of different refinement approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed82fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_refinement_methods(question: str):\n",
    "    \"\"\"Compare different refinement methods on the same question.\"\"\"\n",
    "    \n",
    "    methods = {\n",
    "        'Baseline': dspy.ChainOfThought(ResponseGenerationSignature),\n",
    "        'Best-of-3': BestOfNGenerator(n_candidates=3),\n",
    "        'Iterative Refine': IterativeRefiner(max_iterations=2),\n",
    "        'Hybrid': HybridRefinementSystem(n_candidates=3, max_refinements=2),\n",
    "        'Advanced': AdvancedRefinementSystem(improvement_threshold=0.05)\n",
    "    }\n",
    "    \n",
    "    evaluator = dspy.ChainOfThought(ResponseEvaluationSignature)\n",
    "    results = {}\n",
    "    \n",
    "    for method_name, method in methods.items():\n",
    "        print(f\"\\nTesting {method_name}...\")\n",
    "        \n",
    "        try:\n",
    "            if method_name == 'Baseline':\n",
    "                result = method(question=question)\n",
    "                response = result.response\n",
    "            elif method_name == 'Best-of-3':\n",
    "                result = method(question=question)\n",
    "                response = result.best_response\n",
    "            elif method_name == 'Iterative Refine':\n",
    "                result = method(question=question)\n",
    "                response = result.final_response\n",
    "            elif method_name == 'Hybrid':\n",
    "                result = method(question=question)\n",
    "                response = result.final_response\n",
    "            elif method_name == 'Advanced':\n",
    "                result = method(question=question)\n",
    "                response = result.final_response\n",
    "            \n",
    "            # Evaluate the response\n",
    "            evaluation = evaluator(question=question, response=response)\n",
    "            \n",
    "            results[method_name] = {\n",
    "                'response': response,\n",
    "                'score': float(evaluation.score),\n",
    "                'reasoning': evaluation.reasoning\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method_name}: {e}\")\n",
    "            results[method_name] = {\n",
    "                'response': f\"Error: {e}\",\n",
    "                'score': 0.0,\n",
    "                'reasoning': \"Method failed\"\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "comparison_question = \"What are the key challenges in implementing sustainable urban planning?\"\n",
    "comparison_results = compare_refinement_methods(comparison_question)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"REFINEMENT METHOD COMPARISON\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Question: {comparison_question}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Sort by score\n",
    "sorted_results = sorted(comparison_results.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "\n",
    "for rank, (method, result) in enumerate(sorted_results, 1):\n",
    "    print(f\"\\n#{rank}. {method} (Score: {result['score']:.3f})\")\n",
    "    print(f\"Response: {result['response'][:200]}...\")\n",
    "    print(f\"Reasoning: {result['reasoning']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c3d8c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated various output refinement techniques in DSPy:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Best-of-N Sampling**: Generate multiple candidates and select the best one\n",
    "2. **Iterative Refinement**: Improve responses through multiple feedback cycles\n",
    "3. **Hybrid Approaches**: Combine techniques for maximum quality improvement\n",
    "4. **Multi-Criteria Evaluation**: Assess responses on multiple dimensions\n",
    "5. **Targeted Refinement**: Focus improvements on specific weak areas\n",
    "\n",
    "### When to Use Each Technique:\n",
    "\n",
    "- **Best-of-N**: When you need consistent quality and have computational budget\n",
    "- **Iterative Refinement**: When responses need deep, thoughtful improvement\n",
    "- **Hybrid**: When maximum quality is required regardless of cost\n",
    "- **Advanced**: When you need fine-grained control over specific quality aspects\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. Balance quality improvement with computational cost\n",
    "2. Use appropriate evaluation metrics for your domain\n",
    "3. Set reasonable improvement thresholds to avoid over-refinement\n",
    "4. Consider caching refined responses for repeated queries\n",
    "5. Monitor performance across different types of questions\n",
    "\n",
    "These techniques can significantly improve the quality and consistency of your DSPy applications, especially for complex reasoning tasks and high-stakes applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
