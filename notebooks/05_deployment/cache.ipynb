{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1f4a1e",
   "metadata": {},
   "source": [
    "# DSPy Cache Management\n",
    "\n",
    "This tutorial demonstrates how to use DSPy's caching capabilities to improve performance and reduce API costs.\n",
    "\n",
    "## What is Caching in DSPy?\n",
    "\n",
    "Caching in DSPy stores the results of language model calls to avoid redundant API requests. This is especially useful during development, testing, and when you have repeated queries.\n",
    "\n",
    "## Benefits:\n",
    "- **Faster development**: Avoid waiting for repeated API calls\n",
    "- **Cost reduction**: Reduce API usage and costs\n",
    "- **Consistent results**: Get the same output for identical inputs\n",
    "- **Offline development**: Work with cached responses when offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8972406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "try:\n",
    "    import dspy\n",
    "except ImportError:\n",
    "    install_package(\"dspy\")\n",
    "    import dspy\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb67cf6",
   "metadata": {},
   "source": [
    "## Basic Cache Configuration\n",
    "\n",
    "DSPy provides built-in caching functionality that can be configured in several ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facac6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DSPy with caching\n",
    "lm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Enable built-in caching\n",
    "cache_dir = Path(\"dspy_cache\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Configure cache using DSPy's built-in cache function\n",
    "dspy.configure(\n",
    "    lm=lm,\n",
    "    cache=True  # Enable caching\n",
    ")\n",
    "\n",
    "print(\"DSPy configured with caching enabled\")\n",
    "print(f\"Cache directory: {cache_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f4256",
   "metadata": {},
   "source": [
    "## Testing Cache Performance\n",
    "\n",
    "Let's create a simple module to test caching performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleQASignature(dspy.Signature):\n",
    "    \"\"\"Answer a question concisely.\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(desc=\"The question to answer\")\n",
    "    answer: str = dspy.OutputField(desc=\"A concise answer\")\n",
    "\n",
    "class CachedQASystem(dspy.Module):\n",
    "    \"\"\"Simple QA system to test caching.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qa = dspy.ChainOfThought(SimpleQASignature)\n",
    "    \n",
    "    def forward(self, question: str) -> dspy.Prediction:\n",
    "        return self.qa(question=question)\n",
    "\n",
    "# Create the system\n",
    "qa_system = CachedQASystem()\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "    \"What is the largest planet in our solar system?\"\n",
    "]\n",
    "\n",
    "print(\"Testing cache performance...\")\n",
    "\n",
    "# First run (no cache)\n",
    "print(\"\\nFirst run (populating cache):\")\n",
    "first_run_times = []\n",
    "\n",
    "for question in test_questions:\n",
    "    start_time = time.time()\n",
    "    result = qa_system(question=question)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    duration = end_time - start_time\n",
    "    first_run_times.append(duration)\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result.answer}\")\n",
    "    print(f\"Time: {duration:.2f}s\\n\")\n",
    "\n",
    "# Second run (with cache)\n",
    "print(\"Second run (using cache):\")\n",
    "second_run_times = []\n",
    "\n",
    "for question in test_questions:\n",
    "    start_time = time.time()\n",
    "    result = qa_system(question=question)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    duration = end_time - start_time\n",
    "    second_run_times.append(duration)\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result.answer}\")\n",
    "    print(f\"Time: {duration:.2f}s\\n\")\n",
    "\n",
    "# Performance comparison\n",
    "avg_first_run = sum(first_run_times) / len(first_run_times)\n",
    "avg_second_run = sum(second_run_times) / len(second_run_times)\n",
    "speedup = avg_first_run / avg_second_run if avg_second_run > 0 else float('inf')\n",
    "\n",
    "print(f\"Performance Summary:\")\n",
    "print(f\"Average time (first run): {avg_first_run:.2f}s\")\n",
    "print(f\"Average time (cached): {avg_second_run:.2f}s\")\n",
    "print(f\"Speedup: {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6947e68",
   "metadata": {},
   "source": [
    "## Advanced Cache Management\n",
    "\n",
    "Let's implement more sophisticated cache management with custom cache handlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCacheManager:\n",
    "    \"\"\"Advanced cache manager with TTL, size limits, and statistics.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"advanced_cache\", \n",
    "                 max_size_mb: int = 100, default_ttl_hours: int = 24):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.max_size_bytes = max_size_mb * 1024 * 1024\n",
    "        self.default_ttl_seconds = default_ttl_hours * 3600\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'hits': 0,\n",
    "            'misses': 0,\n",
    "            'evictions': 0,\n",
    "            'total_requests': 0\n",
    "        }\n",
    "        \n",
    "        self.metadata_file = self.cache_dir / \"cache_metadata.json\"\n",
    "        self.load_metadata()\n",
    "    \n",
    "    def _generate_cache_key(self, inputs: dict) -> str:\n",
    "        \"\"\"Generate a unique cache key from inputs.\"\"\"\n",
    "        # Create a consistent string representation\n",
    "        sorted_inputs = json.dumps(inputs, sort_keys=True)\n",
    "        return hashlib.md5(sorted_inputs.encode()).hexdigest()\n",
    "    \n",
    "    def load_metadata(self):\n",
    "        \"\"\"Load cache metadata.\"\"\"\n",
    "        if self.metadata_file.exists():\n",
    "            with open(self.metadata_file, 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "        else:\n",
    "            self.metadata = {}\n",
    "    \n",
    "    def save_metadata(self):\n",
    "        \"\"\"Save cache metadata.\"\"\"\n",
    "        with open(self.metadata_file, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "    \n",
    "    def get(self, inputs: dict) -> dict:\n",
    "        \"\"\"Get cached result if available and not expired.\"\"\"\n",
    "        self.stats['total_requests'] += 1\n",
    "        \n",
    "        cache_key = self._generate_cache_key(inputs)\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.json\"\n",
    "        \n",
    "        if not cache_file.exists():\n",
    "            self.stats['misses'] += 1\n",
    "            return None\n",
    "        \n",
    "        # Check if expired\n",
    "        if cache_key in self.metadata:\n",
    "            created_at = self.metadata[cache_key]['created_at']\n",
    "            ttl = self.metadata[cache_key].get('ttl', self.default_ttl_seconds)\n",
    "            \n",
    "            if time.time() - created_at > ttl:\n",
    "                # Expired, remove\n",
    "                cache_file.unlink()\n",
    "                del self.metadata[cache_key]\n",
    "                self.save_metadata()\n",
    "                self.stats['misses'] += 1\n",
    "                return None\n",
    "        \n",
    "        # Load cached result\n",
    "        try:\n",
    "            with open(cache_file, 'r') as f:\n",
    "                cached_data = json.load(f)\n",
    "            \n",
    "            self.stats['hits'] += 1\n",
    "            \n",
    "            # Update access time\n",
    "            if cache_key in self.metadata:\n",
    "                self.metadata[cache_key]['last_accessed'] = time.time()\n",
    "                self.save_metadata()\n",
    "            \n",
    "            return cached_data\n",
    "        \n",
    "        except (json.JSONDecodeError, FileNotFoundError):\n",
    "            self.stats['misses'] += 1\n",
    "            return None\n",
    "    \n",
    "    def put(self, inputs: dict, result: dict, ttl_hours: int = None):\n",
    "        \"\"\"Cache a result.\"\"\"\n",
    "        cache_key = self._generate_cache_key(inputs)\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.json\"\n",
    "        \n",
    "        # Check cache size and evict if necessary\n",
    "        self._enforce_size_limit()\n",
    "        \n",
    "        # Prepare cache data\n",
    "        cache_data = {\n",
    "            'inputs': inputs,\n",
    "            'result': result,\n",
    "            'cached_at': time.time()\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(cache_data, f, indent=2)\n",
    "        \n",
    "        # Update metadata\n",
    "        ttl_seconds = (ttl_hours * 3600) if ttl_hours else self.default_ttl_seconds\n",
    "        self.metadata[cache_key] = {\n",
    "            'created_at': time.time(),\n",
    "            'last_accessed': time.time(),\n",
    "            'ttl': ttl_seconds,\n",
    "            'size_bytes': cache_file.stat().st_size\n",
    "        }\n",
    "        self.save_metadata()\n",
    "    \n",
    "    def _enforce_size_limit(self):\n",
    "        \"\"\"Enforce cache size limit by evicting least recently used items.\"\"\"\n",
    "        total_size = sum(\n",
    "            meta['size_bytes'] for meta in self.metadata.values()\n",
    "        )\n",
    "        \n",
    "        if total_size <= self.max_size_bytes:\n",
    "            return\n",
    "        \n",
    "        # Sort by last accessed time (LRU)\n",
    "        sorted_items = sorted(\n",
    "            self.metadata.items(),\n",
    "            key=lambda x: x[1]['last_accessed']\n",
    "        )\n",
    "        \n",
    "        # Remove oldest items until under size limit\n",
    "        for cache_key, meta in sorted_items:\n",
    "            if total_size <= self.max_size_bytes:\n",
    "                break\n",
    "            \n",
    "            # Remove file and metadata\n",
    "            cache_file = self.cache_dir / f\"{cache_key}.json\"\n",
    "            if cache_file.exists():\n",
    "                cache_file.unlink()\n",
    "            \n",
    "            total_size -= meta['size_bytes']\n",
    "            del self.metadata[cache_key]\n",
    "            self.stats['evictions'] += 1\n",
    "        \n",
    "        self.save_metadata()\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all cache.\"\"\"\n",
    "        for cache_file in self.cache_dir.glob(\"*.json\"):\n",
    "            if cache_file.name != \"cache_metadata.json\":\n",
    "                cache_file.unlink()\n",
    "        \n",
    "        self.metadata = {}\n",
    "        self.save_metadata()\n",
    "        \n",
    "        # Reset stats\n",
    "        self.stats = {\n",
    "            'hits': 0,\n",
    "            'misses': 0,\n",
    "            'evictions': 0,\n",
    "            'total_requests': 0\n",
    "        }\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        hit_rate = (self.stats['hits'] / self.stats['total_requests'] \n",
    "                   if self.stats['total_requests'] > 0 else 0)\n",
    "        \n",
    "        total_size = sum(\n",
    "            meta['size_bytes'] for meta in self.metadata.values()\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            **self.stats,\n",
    "            'hit_rate': hit_rate,\n",
    "            'cache_size_bytes': total_size,\n",
    "            'cache_size_mb': total_size / (1024 * 1024),\n",
    "            'cached_items': len(self.metadata)\n",
    "        }\n",
    "\n",
    "# Test advanced cache manager\n",
    "advanced_cache = AdvancedCacheManager(\n",
    "    cache_dir=\"advanced_cache\",\n",
    "    max_size_mb=10,\n",
    "    default_ttl_hours=1\n",
    ")\n",
    "\n",
    "print(\"Advanced cache manager created\")\n",
    "print(f\"Initial stats: {advanced_cache.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226bf4e5",
   "metadata": {},
   "source": [
    "## Cached DSPy Module\n",
    "\n",
    "Let's create a DSPy module that uses our advanced cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b40971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedModule(dspy.Module):\n",
    "    \"\"\"DSPy module with advanced caching capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_manager: AdvancedCacheManager = None, use_cache: bool = True):\n",
    "        super().__init__()\n",
    "        self.cache_manager = cache_manager or AdvancedCacheManager()\n",
    "        self.use_cache = use_cache\n",
    "        self.qa = dspy.ChainOfThought(SimpleQASignature)\n",
    "    \n",
    "    def forward(self, question: str) -> dspy.Prediction:\n",
    "        # Prepare cache inputs\n",
    "        cache_inputs = {\n",
    "            'question': question,\n",
    "            'module': 'CachedModule',\n",
    "            'signature': 'SimpleQASignature'\n",
    "        }\n",
    "        \n",
    "        # Try to get from cache\n",
    "        if self.use_cache:\n",
    "            cached_result = self.cache_manager.get(cache_inputs)\n",
    "            if cached_result:\n",
    "                # Return cached result\n",
    "                return dspy.Prediction(\n",
    "                    answer=cached_result['result']['answer'],\n",
    "                    cached=True\n",
    "                )\n",
    "        \n",
    "        # Generate new result\n",
    "        result = self.qa(question=question)\n",
    "        \n",
    "        # Cache the result\n",
    "        if self.use_cache:\n",
    "            cache_result = {\n",
    "                'answer': result.answer\n",
    "            }\n",
    "            self.cache_manager.put(cache_inputs, cache_result)\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            answer=result.answer,\n",
    "            cached=False\n",
    "        )\n",
    "\n",
    "# Test cached module\n",
    "cached_qa = CachedModule(advanced_cache)\n",
    "\n",
    "# Test with repeated questions\n",
    "test_questions = [\n",
    "    \"What is the speed of light?\",\n",
    "    \"Who invented the telephone?\",\n",
    "    \"What is the largest ocean?\",\n",
    "    \"What is the speed of light?\",  # Repeat\n",
    "    \"Who invented the telephone?\",  # Repeat\n",
    "]\n",
    "\n",
    "print(\"Testing cached module:\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    start_time = time.time()\n",
    "    result = cached_qa(question=question)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{i}. Q: {question}\")\n",
    "    print(f\"   A: {result.answer}\")\n",
    "    print(f\"   Cached: {result.cached}\")\n",
    "    print(f\"   Time: {end_time - start_time:.3f}s\")\n",
    "\n",
    "# Show cache statistics\n",
    "print(f\"\\nCache Statistics:\")\n",
    "stats = advanced_cache.get_stats()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205477f",
   "metadata": {},
   "source": [
    "## Cache Strategies for Different Use Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e098079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheStrategy:\n",
    "    \"\"\"Base class for cache strategies.\"\"\"\n",
    "    \n",
    "    def should_cache(self, inputs: dict, result: dict) -> bool:\n",
    "        \"\"\"Determine if a result should be cached.\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def get_ttl_hours(self, inputs: dict, result: dict) -> int:\n",
    "        \"\"\"Get TTL for this cache entry.\"\"\"\n",
    "        return 24\n",
    "\n",
    "class DevelopmentCacheStrategy(CacheStrategy):\n",
    "    \"\"\"Aggressive caching for development.\"\"\"\n",
    "    \n",
    "    def should_cache(self, inputs: dict, result: dict) -> bool:\n",
    "        return True  # Cache everything\n",
    "    \n",
    "    def get_ttl_hours(self, inputs: dict, result: dict) -> int:\n",
    "        return 168  # 1 week\n",
    "\n",
    "class ProductionCacheStrategy(CacheStrategy):\n",
    "    \"\"\"Conservative caching for production.\"\"\"\n",
    "    \n",
    "    def should_cache(self, inputs: dict, result: dict) -> bool:\n",
    "        # Only cache if result seems high quality\n",
    "        if 'confidence' in result:\n",
    "            return float(result['confidence']) > 0.8\n",
    "        return len(result.get('answer', '')) > 10  # Meaningful answers\n",
    "    \n",
    "    def get_ttl_hours(self, inputs: dict, result: dict) -> int:\n",
    "        # Shorter TTL for production\n",
    "        return 6  # 6 hours\n",
    "\n",
    "class TestingCacheStrategy(CacheStrategy):\n",
    "    \"\"\"No caching for testing to ensure fresh results.\"\"\"\n",
    "    \n",
    "    def should_cache(self, inputs: dict, result: dict) -> bool:\n",
    "        return False  # Never cache during testing\n",
    "\n",
    "class SmartCachedModule(dspy.Module):\n",
    "    \"\"\"Module with strategy-based caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_manager: AdvancedCacheManager, \n",
    "                 cache_strategy: CacheStrategy):\n",
    "        super().__init__()\n",
    "        self.cache_manager = cache_manager\n",
    "        self.cache_strategy = cache_strategy\n",
    "        self.qa = dspy.ChainOfThought(SimpleQASignature)\n",
    "    \n",
    "    def forward(self, question: str) -> dspy.Prediction:\n",
    "        cache_inputs = {\n",
    "            'question': question,\n",
    "            'module': 'SmartCachedModule'\n",
    "        }\n",
    "        \n",
    "        # Try cache first\n",
    "        cached_result = self.cache_manager.get(cache_inputs)\n",
    "        if cached_result:\n",
    "            return dspy.Prediction(\n",
    "                answer=cached_result['result']['answer'],\n",
    "                cached=True\n",
    "            )\n",
    "        \n",
    "        # Generate new result\n",
    "        result = self.qa(question=question)\n",
    "        \n",
    "        # Check if we should cache this result\n",
    "        result_dict = {'answer': result.answer}\n",
    "        \n",
    "        if self.cache_strategy.should_cache(cache_inputs, result_dict):\n",
    "            ttl_hours = self.cache_strategy.get_ttl_hours(cache_inputs, result_dict)\n",
    "            self.cache_manager.put(cache_inputs, result_dict, ttl_hours)\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            answer=result.answer,\n",
    "            cached=False\n",
    "        )\n",
    "\n",
    "# Test different strategies\n",
    "strategies = {\n",
    "    'development': DevelopmentCacheStrategy(),\n",
    "    'production': ProductionCacheStrategy(),\n",
    "    'testing': TestingCacheStrategy()\n",
    "}\n",
    "\n",
    "print(\"Testing different cache strategies:\")\n",
    "\n",
    "for strategy_name, strategy in strategies.items():\n",
    "    print(f\"\\n=== {strategy_name.upper()} STRATEGY ===\")\n",
    "    \n",
    "    # Create fresh cache for each strategy\n",
    "    strategy_cache = AdvancedCacheManager(\n",
    "        cache_dir=f\"cache_{strategy_name}\",\n",
    "        max_size_mb=5\n",
    "    )\n",
    "    strategy_cache.clear()  # Start fresh\n",
    "    \n",
    "    smart_module = SmartCachedModule(strategy_cache, strategy)\n",
    "    \n",
    "    # Test questions\n",
    "    questions = [\n",
    "        \"What is artificial intelligence?\",\n",
    "        \"Hi\",  # Short answer - might not be cached in production\n",
    "        \"What is artificial intelligence?\"  # Repeat\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        result = smart_module(question=question)\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {result.answer[:50]}...\")\n",
    "        print(f\"Cached: {result.cached}\")\n",
    "    \n",
    "    stats = strategy_cache.get_stats()\n",
    "    print(f\"Strategy stats: {stats['cached_items']} items, \"\n",
    "          f\"{stats['hit_rate']:.1%} hit rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a32bd",
   "metadata": {},
   "source": [
    "## Cache Monitoring and Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7afe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheMonitor:\n",
    "    \"\"\"Monitor and maintain cache health.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_manager: AdvancedCacheManager):\n",
    "        self.cache_manager = cache_manager\n",
    "    \n",
    "    def health_check(self) -> dict:\n",
    "        \"\"\"Perform a comprehensive health check.\"\"\"\n",
    "        stats = self.cache_manager.get_stats()\n",
    "        \n",
    "        health = {\n",
    "            'overall_status': 'healthy',\n",
    "            'issues': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Check hit rate\n",
    "        if stats['hit_rate'] < 0.3 and stats['total_requests'] > 10:\n",
    "            health['issues'].append('Low cache hit rate')\n",
    "            health['recommendations'].append('Consider adjusting TTL or cache strategy')\n",
    "        \n",
    "        # Check cache size\n",
    "        if stats['cache_size_mb'] > 8:  # Assuming 10MB limit\n",
    "            health['issues'].append('Cache approaching size limit')\n",
    "            health['recommendations'].append('Consider increasing cache size or reducing TTL')\n",
    "        \n",
    "        # Check eviction rate\n",
    "        if stats['evictions'] > stats['cached_items']:\n",
    "            health['issues'].append('High eviction rate')\n",
    "            health['recommendations'].append('Increase cache size or optimize cache strategy')\n",
    "        \n",
    "        # Overall status\n",
    "        if health['issues']:\n",
    "            health['overall_status'] = 'needs_attention' if len(health['issues']) < 3 else 'unhealthy'\n",
    "        \n",
    "        return health\n",
    "    \n",
    "    def cleanup_expired(self) -> int:\n",
    "        \"\"\"Clean up expired cache entries.\"\"\"\n",
    "        current_time = time.time()\n",
    "        expired_keys = []\n",
    "        \n",
    "        for cache_key, meta in self.cache_manager.metadata.items():\n",
    "            created_at = meta['created_at']\n",
    "            ttl = meta.get('ttl', self.cache_manager.default_ttl_seconds)\n",
    "            \n",
    "            if current_time - created_at > ttl:\n",
    "                expired_keys.append(cache_key)\n",
    "        \n",
    "        # Remove expired entries\n",
    "        for cache_key in expired_keys:\n",
    "            cache_file = self.cache_manager.cache_dir / f\"{cache_key}.json\"\n",
    "            if cache_file.exists():\n",
    "                cache_file.unlink()\n",
    "            del self.cache_manager.metadata[cache_key]\n",
    "        \n",
    "        if expired_keys:\n",
    "            self.cache_manager.save_metadata()\n",
    "        \n",
    "        return len(expired_keys)\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate a comprehensive cache report.\"\"\"\n",
    "        stats = self.cache_manager.get_stats()\n",
    "        health = self.health_check()\n",
    "        \n",
    "        report = f\"\"\"\n",
    "CACHE PERFORMANCE REPORT\n",
    "========================\n",
    "\n",
    "Statistics:\n",
    "  Total Requests: {stats['total_requests']}\n",
    "  Cache Hits: {stats['hits']}\n",
    "  Cache Misses: {stats['misses']}\n",
    "  Hit Rate: {stats['hit_rate']:.1%}\n",
    "  \n",
    "Storage:\n",
    "  Cached Items: {stats['cached_items']}\n",
    "  Cache Size: {stats['cache_size_mb']:.2f} MB\n",
    "  Evictions: {stats['evictions']}\n",
    "  \n",
    "Health Status: {health['overall_status'].upper()}\n",
    "\"\"\"\n",
    "        \n",
    "        if health['issues']:\n",
    "            report += \"\\nIssues:\\n\"\n",
    "            for issue in health['issues']:\n",
    "                report += f\"  - {issue}\\n\"\n",
    "        \n",
    "        if health['recommendations']:\n",
    "            report += \"\\nRecommendations:\\n\"\n",
    "            for rec in health['recommendations']:\n",
    "                report += f\"  - {rec}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Test cache monitoring\n",
    "monitor = CacheMonitor(advanced_cache)\n",
    "\n",
    "# Generate some cache activity\n",
    "test_module = CachedModule(advanced_cache)\n",
    "for i in range(10):\n",
    "    question = f\"What is the answer to question {i % 3}?\"  # Some repeats\n",
    "    test_module(question=question)\n",
    "\n",
    "# Check health and generate report\n",
    "health = monitor.health_check()\n",
    "print(\"Cache Health Check:\")\n",
    "print(json.dumps(health, indent=2))\n",
    "\n",
    "print(\"\\n\" + monitor.generate_report())\n",
    "\n",
    "# Clean up expired entries\n",
    "expired_count = monitor.cleanup_expired()\n",
    "print(f\"\\nCleaned up {expired_count} expired entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8dba2",
   "metadata": {},
   "source": [
    "## Cache Best Practices and Configuration\n",
    "\n",
    "Let's implement a configuration system for cache management in different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheConfig:\n",
    "    \"\"\"Configuration management for cache settings.\"\"\"\n",
    "    \n",
    "    DEVELOPMENT = {\n",
    "        'enabled': True,\n",
    "        'max_size_mb': 100,\n",
    "        'default_ttl_hours': 168,  # 1 week\n",
    "        'strategy': 'aggressive',\n",
    "        'monitor_interval_minutes': 60\n",
    "    }\n",
    "    \n",
    "    PRODUCTION = {\n",
    "        'enabled': True,\n",
    "        'max_size_mb': 500,\n",
    "        'default_ttl_hours': 6,\n",
    "        'strategy': 'conservative',\n",
    "        'monitor_interval_minutes': 15\n",
    "    }\n",
    "    \n",
    "    TESTING = {\n",
    "        'enabled': False,\n",
    "        'max_size_mb': 10,\n",
    "        'default_ttl_hours': 1,\n",
    "        'strategy': 'none',\n",
    "        'monitor_interval_minutes': 5\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_config(cls, environment: str) -> dict:\n",
    "        \"\"\"Get configuration for environment.\"\"\"\n",
    "        configs = {\n",
    "            'development': cls.DEVELOPMENT,\n",
    "            'production': cls.PRODUCTION,\n",
    "            'testing': cls.TESTING\n",
    "        }\n",
    "        return configs.get(environment, cls.DEVELOPMENT)\n",
    "\n",
    "class ConfigurableCacheManager:\n",
    "    \"\"\"Cache manager with environment-based configuration.\"\"\"\n",
    "    \n",
    "    def __init__(self, environment: str = 'development'):\n",
    "        self.environment = environment\n",
    "        self.config = CacheConfig.get_config(environment)\n",
    "        \n",
    "        if self.config['enabled']:\n",
    "            self.cache_manager = AdvancedCacheManager(\n",
    "                cache_dir=f\"cache_{environment}\",\n",
    "                max_size_mb=self.config['max_size_mb'],\n",
    "                default_ttl_hours=self.config['default_ttl_hours']\n",
    "            )\n",
    "            self.monitor = CacheMonitor(self.cache_manager)\n",
    "        else:\n",
    "            self.cache_manager = None\n",
    "            self.monitor = None\n",
    "    \n",
    "    def is_enabled(self) -> bool:\n",
    "        \"\"\"Check if caching is enabled.\"\"\"\n",
    "        return self.config['enabled']\n",
    "    \n",
    "    def get(self, inputs: dict):\n",
    "        \"\"\"Get from cache if enabled.\"\"\"\n",
    "        if not self.is_enabled():\n",
    "            return None\n",
    "        return self.cache_manager.get(inputs)\n",
    "    \n",
    "    def put(self, inputs: dict, result: dict, ttl_hours: int = None):\n",
    "        \"\"\"Put to cache if enabled.\"\"\"\n",
    "        if not self.is_enabled():\n",
    "            return\n",
    "        self.cache_manager.put(inputs, result, ttl_hours)\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        if not self.is_enabled():\n",
    "            return {'enabled': False}\n",
    "        \n",
    "        stats = self.cache_manager.get_stats()\n",
    "        stats.update({\n",
    "            'environment': self.environment,\n",
    "            'config': self.config\n",
    "        })\n",
    "        return stats\n",
    "    \n",
    "    def health_check(self) -> dict:\n",
    "        \"\"\"Perform health check.\"\"\"\n",
    "        if not self.is_enabled():\n",
    "            return {'enabled': False, 'status': 'disabled'}\n",
    "        return self.monitor.health_check()\n",
    "\n",
    "# Test configurable cache manager\n",
    "environments = ['development', 'production', 'testing']\n",
    "\n",
    "print(\"Testing configurable cache manager:\")\n",
    "\n",
    "for env in environments:\n",
    "    print(f\"\\n=== {env.upper()} ENVIRONMENT ===\")\n",
    "    \n",
    "    cache_mgr = ConfigurableCacheManager(env)\n",
    "    \n",
    "    print(f\"Cache enabled: {cache_mgr.is_enabled()}\")\n",
    "    \n",
    "    if cache_mgr.is_enabled():\n",
    "        # Test cache operations\n",
    "        test_inputs = {'question': f'Test question for {env}'}\n",
    "        test_result = {'answer': f'Test answer for {env}'}\n",
    "        \n",
    "        # Put and get\n",
    "        cache_mgr.put(test_inputs, test_result)\n",
    "        cached = cache_mgr.get(test_inputs)\n",
    "        \n",
    "        print(f\"Cache test successful: {cached is not None}\")\n",
    "        \n",
    "        # Show stats\n",
    "        stats = cache_mgr.get_stats()\n",
    "        print(f\"Cache size limit: {stats['config']['max_size_mb']} MB\")\n",
    "        print(f\"Default TTL: {stats['config']['default_ttl_hours']} hours\")\n",
    "    else:\n",
    "        print(\"Caching disabled for this environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bedb292",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial covered comprehensive caching strategies for DSPy applications:\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "\n",
    "1. **Basic Caching**: Using DSPy's built-in cache functionality\n",
    "2. **Advanced Cache Management**: Custom cache with TTL, size limits, and LRU eviction\n",
    "3. **Cache Strategies**: Different approaches for development, production, and testing\n",
    "4. **Monitoring**: Health checks and performance monitoring\n",
    "5. **Configuration**: Environment-specific cache settings\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Environment-Specific Caching**:\n",
    "   - Development: Aggressive caching for faster iteration\n",
    "   - Production: Conservative caching with shorter TTL\n",
    "   - Testing: Minimal or no caching for consistency\n",
    "\n",
    "2. **Cache Management**:\n",
    "   - Implement size limits to prevent disk space issues\n",
    "   - Use TTL to ensure data freshness\n",
    "   - Monitor hit rates and performance\n",
    "   - Regular cleanup of expired entries\n",
    "\n",
    "3. **Strategy Selection**:\n",
    "   - Cache deterministic results aggressively\n",
    "   - Be cautious with time-sensitive information\n",
    "   - Consider result quality when caching\n",
    "   - Implement cache invalidation for critical updates\n",
    "\n",
    "4. **Performance Optimization**:\n",
    "   - Use appropriate cache keys to maximize hit rates\n",
    "   - Balance cache size with available resources\n",
    "   - Monitor and adjust TTL based on usage patterns\n",
    "   - Consider cache warming for critical paths\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "- **Security**: Encrypt cached data if it contains sensitive information\n",
    "- **Distributed Caching**: Consider Redis or similar for multi-instance deployments\n",
    "- **Backup**: Regular backups of important cached data\n",
    "- **Monitoring**: Real-time alerts for cache performance issues\n",
    "- **Testing**: Validate cache behavior under load\n",
    "\n",
    "Proper caching can significantly improve the performance and cost-effectiveness of your DSPy applications while maintaining result quality and consistency."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
