{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3e8970",
   "metadata": {},
   "source": [
    "# Streaming with DSPy\n",
    "\n",
    "This notebook demonstrates how to implement streaming responses with DSPy for real-time AI applications.\n",
    "\n",
    "## What You'll Learn:\n",
    "- Setting up streaming responses with DSPy\n",
    "- Implementing real-time text generation\n",
    "- Building streaming chat interfaces\n",
    "- Handling streaming with different LM providers\n",
    "- Performance optimization for streaming applications\n",
    "\n",
    "Based on the DSPy tutorial: [Streaming](https://dspy.ai/tutorials/streaming/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e239a",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import dspy\n",
    "from utils import setup_default_lm, print_step, print_result, print_error\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Generator, AsyncGenerator, List, Dict, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "import threading\n",
    "from queue import Queue\n",
    "import functools\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb7f29",
   "metadata": {},
   "source": [
    "## Language Model Configuration for Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Setting up Language Model\", \"Configuring DSPy for streaming responses\")\n",
    "\n",
    "try:\n",
    "    # Configure with streaming-capable model\n",
    "    lm = setup_default_lm(\n",
    "        provider=\"openai\", \n",
    "        model=\"gpt-4o-mini\", \n",
    "        max_tokens=1000,\n",
    "        stream=True  # Enable streaming\n",
    "    )\n",
    "    dspy.configure(lm=lm)\n",
    "    print_result(\"Streaming-enabled language model configured successfully!\", \"Status\")\n",
    "except Exception as e:\n",
    "    print_error(f\"Failed to configure streaming model: {e}\")\n",
    "    # Fallback to non-streaming for demo\n",
    "    try:\n",
    "        lm = setup_default_lm(provider=\"openai\", model=\"gpt-4o-mini\", max_tokens=1000)\n",
    "        dspy.configure(lm=lm)\n",
    "        print_result(\"Non-streaming model configured as fallback\")\n",
    "    except Exception as e2:\n",
    "        print_error(f\"Failed to configure fallback model: {e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ad68b",
   "metadata": {},
   "source": [
    "## Basic Streaming Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2230ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingTextGenerator:\n",
    "    \"\"\"Basic streaming text generator using DSPy.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.predictor = dspy.Predict(\"prompt -> response\")\n",
    "    \n",
    "    def generate_stream(self, prompt: str) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate streaming response for a given prompt.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # For demonstration, we'll simulate streaming by chunking a response\n",
    "            # In practice, you'd use the actual streaming API from your LM provider\n",
    "            \n",
    "            # Get full response first (in real streaming, this would be incremental)\n",
    "            result = self.predictor(prompt=prompt)\n",
    "            response_text = result.response\n",
    "            \n",
    "            # Simulate streaming by yielding chunks\n",
    "            words = response_text.split()\n",
    "            current_chunk = \"\"\n",
    "            \n",
    "            for word in words:\n",
    "                current_chunk += word + \" \"\n",
    "                \n",
    "                # Yield chunks of 3-5 words\n",
    "                if len(current_chunk.split()) >= 3:\n",
    "                    yield current_chunk.strip()\n",
    "                    current_chunk = \"\"\n",
    "                    time.sleep(0.1)  # Simulate network delay\n",
    "            \n",
    "            # Yield remaining text\n",
    "            if current_chunk.strip():\n",
    "                yield current_chunk.strip()\n",
    "                \n",
    "        except Exception as e:\n",
    "            yield f\"Error in streaming: {e}\"\n",
    "    \n",
    "    def generate_complete_stream(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate streaming response with metadata.\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        chunks = list(self.generate_stream(prompt))\n",
    "        end_time = time.time()\n",
    "        \n",
    "        return {\n",
    "            'chunks': chunks,\n",
    "            'full_response': ' '.join(chunks),\n",
    "            'chunk_count': len(chunks),\n",
    "            'duration': end_time - start_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Initialize streaming generator\n",
    "streaming_generator = StreamingTextGenerator()\n",
    "print_result(\"Basic streaming text generator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abeb43",
   "metadata": {},
   "source": [
    "## Testing Basic Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c45407",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Testing Basic Streaming\", \"Demonstrating word-by-word text generation\")\n",
    "\n",
    "test_prompt = \"Explain the concept of machine learning in simple terms.\"\n",
    "\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(\"\\nStreaming Response:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Demonstrate real-time streaming\n",
    "full_response = \"\"\n",
    "chunk_count = 0\n",
    "\n",
    "for chunk in streaming_generator.generate_stream(test_prompt):\n",
    "    print(chunk, end=\" \", flush=True)\n",
    "    full_response += chunk + \" \"\n",
    "    chunk_count += 1\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print_result(f\"Streaming completed in {chunk_count} chunks\")\n",
    "print_result(f\"Full response length: {len(full_response)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957b3d4",
   "metadata": {},
   "source": [
    "## Advanced Streaming with DSPy Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db660ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingSignatures:\n",
    "    \"\"\"DSPy signatures optimized for streaming.\"\"\"\n",
    "    \n",
    "    class StreamingQA(dspy.Signature):\n",
    "        \"\"\"Question answering optimized for streaming response.\"\"\"\n",
    "        question = dspy.InputField(desc=\"User's question\")\n",
    "        answer = dspy.OutputField(desc=\"Comprehensive answer that can be streamed\")\n",
    "    \n",
    "    class StreamingStoryGeneration(dspy.Signature):\n",
    "        \"\"\"Story generation optimized for streaming.\"\"\"\n",
    "        prompt = dspy.InputField(desc=\"Story prompt or theme\")\n",
    "        story = dspy.OutputField(desc=\"Engaging story that unfolds progressively\")\n",
    "    \n",
    "    class StreamingExplanation(dspy.Signature):\n",
    "        \"\"\"Technical explanation optimized for streaming.\"\"\"\n",
    "        topic = dspy.InputField(desc=\"Topic to explain\")\n",
    "        audience = dspy.InputField(desc=\"Target audience level\")\n",
    "        explanation = dspy.OutputField(desc=\"Clear, progressive explanation\")\n",
    "\n",
    "class AdvancedStreamingModule(dspy.Module):\n",
    "    \"\"\"Advanced DSPy module with streaming capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qa_module = dspy.ChainOfThought(StreamingSignatures.StreamingQA)\n",
    "        self.story_module = dspy.ChainOfThought(StreamingSignatures.StreamingStoryGeneration)\n",
    "        self.explanation_module = dspy.ChainOfThought(StreamingSignatures.StreamingExplanation)\n",
    "        \n",
    "        # Streaming configuration\n",
    "        self.chunk_size = 5  # words per chunk\n",
    "        self.delay = 0.08    # seconds between chunks\n",
    "    \n",
    "    def stream_qa(self, question: str) -> Generator[Dict[str, Any], None, None]:\n",
    "        \"\"\"Stream question-answering response with reasoning.\"\"\"\n",
    "        \n",
    "        # Generate reasoning and answer\n",
    "        result = self.qa_module(question=question)\n",
    "        \n",
    "        # Stream reasoning first\n",
    "        if hasattr(result, 'reasoning') and result.reasoning:\n",
    "            yield {\n",
    "                'type': 'reasoning_start',\n",
    "                'content': 'Let me think about this...',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            for chunk in self._chunk_text(result.reasoning):\n",
    "                yield {\n",
    "                    'type': 'reasoning',\n",
    "                    'content': chunk,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                time.sleep(self.delay)\n",
    "        \n",
    "        # Stream answer\n",
    "        yield {\n",
    "            'type': 'answer_start',\n",
    "            'content': 'Here\\'s my answer:',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        for chunk in self._chunk_text(result.answer):\n",
    "            yield {\n",
    "                'type': 'answer',\n",
    "                'content': chunk,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            time.sleep(self.delay)\n",
    "        \n",
    "        yield {\n",
    "            'type': 'complete',\n",
    "            'content': '',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def stream_story(self, prompt: str) -> Generator[Dict[str, Any], None, None]:\n",
    "        \"\"\"Stream story generation.\"\"\"\n",
    "        \n",
    "        result = self.story_module(prompt=prompt)\n",
    "        \n",
    "        yield {\n",
    "            'type': 'story_start',\n",
    "            'content': f'Creating a story about: {prompt}',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        for chunk in self._chunk_text(result.story):\n",
    "            yield {\n",
    "                'type': 'story',\n",
    "                'content': chunk,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            time.sleep(self.delay)\n",
    "        \n",
    "        yield {\n",
    "            'type': 'story_complete',\n",
    "            'content': 'The End.',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def stream_explanation(self, topic: str, audience: str = \"general\") -> Generator[Dict[str, Any], None, None]:\n",
    "        \"\"\"Stream technical explanation.\"\"\"\n",
    "        \n",
    "        result = self.explanation_module(topic=topic, audience=audience)\n",
    "        \n",
    "        yield {\n",
    "            'type': 'explanation_start',\n",
    "            'content': f'Explaining {topic} for {audience} audience:',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        for chunk in self._chunk_text(result.explanation):\n",
    "            yield {\n",
    "                'type': 'explanation',\n",
    "                'content': chunk,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            time.sleep(self.delay)\n",
    "        \n",
    "        yield {\n",
    "            'type': 'explanation_complete',\n",
    "            'content': '',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def _chunk_text(self, text: str) -> Generator[str, None, None]:\n",
    "        \"\"\"Break text into streaming chunks.\"\"\"\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), self.chunk_size):\n",
    "            chunk = ' '.join(words[i:i + self.chunk_size])\n",
    "            yield chunk\n",
    "    \n",
    "    def configure_streaming(self, chunk_size: int = 5, delay: float = 0.08):\n",
    "        \"\"\"Configure streaming parameters.\"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.delay = delay\n",
    "\n",
    "# Initialize advanced streaming module\n",
    "advanced_streaming = AdvancedStreamingModule()\n",
    "print_result(\"Advanced streaming module initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710f720",
   "metadata": {},
   "source": [
    "## Testing Advanced Streaming Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03861f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Testing Advanced Streaming\", \"Demonstrating structured streaming with metadata\")\n",
    "\n",
    "# Test 1: Streaming Q&A with reasoning\n",
    "print(\"\\n1. Streaming Q&A with Reasoning:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "qa_question = \"How do neural networks learn from data?\"\n",
    "print(f\"Question: {qa_question}\\n\")\n",
    "\n",
    "for stream_chunk in advanced_streaming.stream_qa(qa_question):\n",
    "    if stream_chunk['type'] == 'reasoning_start':\n",
    "        print(f\"ü§î {stream_chunk['content']}\")\n",
    "    elif stream_chunk['type'] == 'reasoning':\n",
    "        print(f\"   {stream_chunk['content']}\", end=\" \")\n",
    "    elif stream_chunk['type'] == 'answer_start':\n",
    "        print(f\"\\n\\nüí° {stream_chunk['content']}\")\n",
    "    elif stream_chunk['type'] == 'answer':\n",
    "        print(f\"{stream_chunk['content']}\", end=\" \", flush=True)\n",
    "    elif stream_chunk['type'] == 'complete':\n",
    "        print(\"\\n\\n‚úÖ Response complete\")\n",
    "\n",
    "# Test 2: Streaming story generation\n",
    "print(\"\\n\\n2. Streaming Story Generation:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "story_prompt = \"A robot learning to paint\"\n",
    "print(f\"Story prompt: {story_prompt}\\n\")\n",
    "\n",
    "for stream_chunk in advanced_streaming.stream_story(story_prompt):\n",
    "    if stream_chunk['type'] == 'story_start':\n",
    "        print(f\"üìñ {stream_chunk['content']}\\n\")\n",
    "    elif stream_chunk['type'] == 'story':\n",
    "        print(f\"{stream_chunk['content']}\", end=\" \", flush=True)\n",
    "    elif stream_chunk['type'] == 'story_complete':\n",
    "        print(f\"\\n\\n{stream_chunk['content']}\")\n",
    "\n",
    "# Test 3: Streaming explanation\n",
    "print(\"\\n\\n3. Streaming Technical Explanation:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "explanation_topic = \"blockchain technology\"\n",
    "print(f\"Topic: {explanation_topic}\\n\")\n",
    "\n",
    "for stream_chunk in advanced_streaming.stream_explanation(explanation_topic, \"beginner\"):\n",
    "    if stream_chunk['type'] == 'explanation_start':\n",
    "        print(f\"üéØ {stream_chunk['content']}\\n\")\n",
    "    elif stream_chunk['type'] == 'explanation':\n",
    "        print(f\"{stream_chunk['content']}\", end=\" \", flush=True)\n",
    "    elif stream_chunk['type'] == 'explanation_complete':\n",
    "        print(\"\\n\\n‚úÖ Explanation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b207a",
   "metadata": {},
   "source": [
    "## Async Streaming Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ed17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncStreamingModule(dspy.Module):\n",
    "    \"\"\"Asynchronous streaming implementation for better performance.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predictor = dspy.ChainOfThought(\"prompt -> response\")\n",
    "        self.chunk_size = 4\n",
    "        self.delay = 0.05\n",
    "    \n",
    "    async def async_stream_response(self, prompt: str) -> AsyncGenerator[Dict[str, Any], None]:\n",
    "        \"\"\"Generate asynchronous streaming response.\"\"\"\n",
    "        \n",
    "        # Start response generation\n",
    "        yield {\n",
    "            'type': 'start',\n",
    "            'content': 'Processing your request...',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Simulate async processing\n",
    "        await asyncio.sleep(0.1)\n",
    "        \n",
    "        try:\n",
    "            # Generate response (in practice, this would be async)\n",
    "            result = self.predictor(prompt=prompt)\n",
    "            \n",
    "            # Stream reasoning if available\n",
    "            if hasattr(result, 'reasoning') and result.reasoning:\n",
    "                yield {\n",
    "                    'type': 'reasoning_start',\n",
    "                    'content': 'Thinking...',\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                async for chunk in self._async_chunk_text(result.reasoning):\n",
    "                    yield {\n",
    "                        'type': 'reasoning',\n",
    "                        'content': chunk,\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    }\n",
    "            \n",
    "            # Stream response\n",
    "            yield {\n",
    "                'type': 'response_start',\n",
    "                'content': 'Response:',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            async for chunk in self._async_chunk_text(result.response):\n",
    "                yield {\n",
    "                    'type': 'response',\n",
    "                    'content': chunk,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            \n",
    "            yield {\n",
    "                'type': 'complete',\n",
    "                'content': 'Done!',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            yield {\n",
    "                'type': 'error',\n",
    "                'content': f'Error: {e}',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    async def _async_chunk_text(self, text: str) -> AsyncGenerator[str, None]:\n",
    "        \"\"\"Asynchronously chunk text for streaming.\"\"\"\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), self.chunk_size):\n",
    "            chunk = ' '.join(words[i:i + self.chunk_size])\n",
    "            yield chunk\n",
    "            await asyncio.sleep(self.delay)\n",
    "    \n",
    "    async def batch_stream_responses(self, prompts: List[str]) -> AsyncGenerator[Dict[str, Any], None]:\n",
    "        \"\"\"Stream responses for multiple prompts concurrently.\"\"\"\n",
    "        \n",
    "        yield {\n",
    "            'type': 'batch_start',\n",
    "            'content': f'Processing {len(prompts)} prompts...',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Process prompts concurrently\n",
    "        tasks = []\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            task = asyncio.create_task(self._process_single_prompt(i, prompt))\n",
    "            tasks.append(task)\n",
    "        \n",
    "        # Stream results as they complete\n",
    "        for task in asyncio.as_completed(tasks):\n",
    "            result = await task\n",
    "            yield result\n",
    "        \n",
    "        yield {\n",
    "            'type': 'batch_complete',\n",
    "            'content': 'All prompts processed',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    async def _process_single_prompt(self, index: int, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single prompt asynchronously.\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = self.predictor(prompt=prompt)\n",
    "            \n",
    "            return {\n",
    "                'type': 'batch_result',\n",
    "                'index': index,\n",
    "                'prompt': prompt[:50] + '...' if len(prompt) > 50 else prompt,\n",
    "                'response': result.response[:100] + '...' if len(result.response) > 100 else result.response,\n",
    "                'duration': time.time() - start_time,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'type': 'batch_error',\n",
    "                'index': index,\n",
    "                'prompt': prompt[:50] + '...' if len(prompt) > 50 else prompt,\n",
    "                'error': str(e),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "# Initialize async streaming module\n",
    "async_streaming = AsyncStreamingModule()\n",
    "print_result(\"Async streaming module initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10f755",
   "metadata": {},
   "source": [
    "## Testing Async Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b51d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Testing Async Streaming\", \"Demonstrating asynchronous response generation\")\n",
    "\n",
    "async def test_async_streaming():\n",
    "    \"\"\"Test asynchronous streaming functionality.\"\"\"\n",
    "    \n",
    "    # Test 1: Single async stream\n",
    "    print(\"\\n1. Single Async Stream:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    async_prompt = \"Explain the benefits of asynchronous programming\"\n",
    "    print(f\"Prompt: {async_prompt}\\n\")\n",
    "    \n",
    "    async for chunk in async_streaming.async_stream_response(async_prompt):\n",
    "        if chunk['type'] == 'start':\n",
    "            print(f\"üöÄ {chunk['content']}\")\n",
    "        elif chunk['type'] == 'reasoning_start':\n",
    "            print(f\"\\nü§î {chunk['content']}\")\n",
    "        elif chunk['type'] == 'reasoning':\n",
    "            print(f\"   {chunk['content']}\", end=\" \")\n",
    "        elif chunk['type'] == 'response_start':\n",
    "            print(f\"\\n\\nüí¨ {chunk['content']}\")\n",
    "        elif chunk['type'] == 'response':\n",
    "            print(f\"{chunk['content']}\", end=\" \", flush=True)\n",
    "        elif chunk['type'] == 'complete':\n",
    "            print(f\"\\n\\n‚úÖ {chunk['content']}\")\n",
    "        elif chunk['type'] == 'error':\n",
    "            print(f\"\\n‚ùå {chunk['content']}\")\n",
    "    \n",
    "    # Test 2: Batch async processing\n",
    "    print(\"\\n\\n2. Batch Async Processing:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    batch_prompts = [\n",
    "        \"What is machine learning?\",\n",
    "        \"Explain quantum computing\",\n",
    "        \"Describe blockchain technology\",\n",
    "        \"What is artificial intelligence?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Processing {len(batch_prompts)} prompts concurrently...\\n\")\n",
    "    \n",
    "    async for result in async_streaming.batch_stream_responses(batch_prompts):\n",
    "        if result['type'] == 'batch_start':\n",
    "            print(f\"üîÑ {result['content']}\")\n",
    "        elif result['type'] == 'batch_result':\n",
    "            print(f\"\\n‚úÖ Result {result['index'] + 1}:\")\n",
    "            print(f\"   Prompt: {result['prompt']}\")\n",
    "            print(f\"   Response: {result['response']}\")\n",
    "            print(f\"   Duration: {result['duration']:.2f}s\")\n",
    "        elif result['type'] == 'batch_error':\n",
    "            print(f\"\\n‚ùå Error {result['index'] + 1}:\")\n",
    "            print(f\"   Prompt: {result['prompt']}\")\n",
    "            print(f\"   Error: {result['error']}\")\n",
    "        elif result['type'] == 'batch_complete':\n",
    "            print(f\"\\nüéâ {result['content']}\")\n",
    "\n",
    "# Run async tests\n",
    "try:\n",
    "    # Check if we're in a Jupyter environment\n",
    "    import IPython\n",
    "    # Use await in Jupyter\n",
    "    await test_async_streaming()\n",
    "except (ImportError, SyntaxError):\n",
    "    # Run with asyncio in regular Python\n",
    "    asyncio.run(test_async_streaming())\n",
    "except Exception as e:\n",
    "    print_error(f\"Error in async streaming test: {e}\")\n",
    "    print(\"Note: Async features may require specific environment setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82e3b1",
   "metadata": {},
   "source": [
    "## Streaming Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e95d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingChatInterface:\n",
    "    \"\"\"Interactive streaming chat interface using DSPy.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "        self.chat_module = dspy.ChainOfThought(\"conversation_history, user_message -> assistant_response\")\n",
    "        self.chunk_size = 3\n",
    "        self.delay = 0.06\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"Add message to conversation history.\"\"\"\n",
    "        self.conversation_history.append({\n",
    "            'role': role,\n",
    "            'content': content,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def get_context_string(self) -> str:\n",
    "        \"\"\"Get formatted conversation context.\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return \"This is the start of the conversation.\"\n",
    "        \n",
    "        context_parts = []\n",
    "        for msg in self.conversation_history[-6:]:  # Last 6 messages\n",
    "            context_parts.append(f\"{msg['role']}: {msg['content']}\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def stream_chat_response(self, user_message: str) -> Generator[Dict[str, Any], None, None]:\n",
    "        \"\"\"Generate streaming chat response.\"\"\"\n",
    "        \n",
    "        # Add user message to history\n",
    "        self.add_message('user', user_message)\n",
    "        \n",
    "        yield {\n",
    "            'type': 'thinking',\n",
    "            'content': 'Thinking...',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Generate response using conversation context\n",
    "            context = self.get_context_string()\n",
    "            result = self.chat_module(\n",
    "                conversation_history=context,\n",
    "                user_message=user_message\n",
    "            )\n",
    "            \n",
    "            # Stream the response\n",
    "            response_text = result.assistant_response\n",
    "            \n",
    "            yield {\n",
    "                'type': 'response_start',\n",
    "                'content': '',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Stream response in chunks\n",
    "            words = response_text.split()\n",
    "            for i in range(0, len(words), self.chunk_size):\n",
    "                chunk = ' '.join(words[i:i + self.chunk_size])\n",
    "                yield {\n",
    "                    'type': 'response_chunk',\n",
    "                    'content': chunk,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                time.sleep(self.delay)\n",
    "            \n",
    "            # Add assistant response to history\n",
    "            self.add_message('assistant', response_text)\n",
    "            \n",
    "            yield {\n",
    "                'type': 'response_complete',\n",
    "                'content': response_text,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Sorry, I encountered an error: {e}\"\n",
    "            self.add_message('assistant', error_msg)\n",
    "            \n",
    "            yield {\n",
    "                'type': 'error',\n",
    "                'content': error_msg,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def get_conversation_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of the conversation.\"\"\"\n",
    "        \n",
    "        user_messages = [msg for msg in self.conversation_history if msg['role'] == 'user']\n",
    "        assistant_messages = [msg for msg in self.conversation_history if msg['role'] == 'assistant']\n",
    "        \n",
    "        return {\n",
    "            'total_messages': len(self.conversation_history),\n",
    "            'user_messages': len(user_messages),\n",
    "            'assistant_messages': len(assistant_messages),\n",
    "            'conversation_start': self.conversation_history[0]['timestamp'] if self.conversation_history else None,\n",
    "            'last_activity': self.conversation_history[-1]['timestamp'] if self.conversation_history else None\n",
    "        }\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Initialize streaming chat interface\n",
    "chat_interface = StreamingChatInterface()\n",
    "print_result(\"Streaming chat interface initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b9c48a",
   "metadata": {},
   "source": [
    "## Testing Streaming Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86153b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Testing Streaming Chat\", \"Demonstrating conversational streaming interface\")\n",
    "\n",
    "# Simulate a conversation\n",
    "chat_messages = [\n",
    "    \"Hello! Can you explain what DSPy is?\",\n",
    "    \"That's interesting! How does it differ from LangChain?\",\n",
    "    \"Can you give me a practical example of using DSPy?\",\n",
    "    \"Thank you! This has been very helpful.\"\n",
    "]\n",
    "\n",
    "print(\"ü§ñ Starting streaming chat demonstration...\\n\")\n",
    "\n",
    "for i, message in enumerate(chat_messages, 1):\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Turn {i}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    print(f\"üë§ User: {message}\")\n",
    "    print(\"ü§ñ Assistant: \", end=\"\")\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    for chunk in chat_interface.stream_chat_response(message):\n",
    "        if chunk['type'] == 'thinking':\n",
    "            print(\"üí≠ \", end=\"\", flush=True)\n",
    "            time.sleep(0.5)\n",
    "            print(\"\\rü§ñ Assistant: \", end=\"\")\n",
    "        elif chunk['type'] == 'response_chunk':\n",
    "            print(chunk['content'], end=\" \", flush=True)\n",
    "            full_response += chunk['content'] + \" \"\n",
    "        elif chunk['type'] == 'response_complete':\n",
    "            print(\"\\n\")\n",
    "        elif chunk['type'] == 'error':\n",
    "            print(f\"‚ùå {chunk['content']}\\n\")\n",
    "    \n",
    "    time.sleep(0.5)  # Pause between messages\n",
    "\n",
    "# Show conversation summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONVERSATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = chat_interface.get_conversation_summary()\n",
    "print_result(f\"Total messages: {summary['total_messages']}\")\n",
    "print_result(f\"User messages: {summary['user_messages']}\")\n",
    "print_result(f\"Assistant messages: {summary['assistant_messages']}\")\n",
    "print_result(f\"Conversation duration: {summary['conversation_start']} to {summary['last_activity']}\")\n",
    "\n",
    "print(\"\\nüéâ Streaming chat demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cbf4f4",
   "metadata": {},
   "source": [
    "## Performance Optimization for Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedStreamingModule:\n",
    "    \"\"\"Performance-optimized streaming implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.predictor = dspy.Predict(\"prompt -> response\")\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.response_times = []\n",
    "        self.chunk_counts = []\n",
    "        self.total_requests = 0\n",
    "        \n",
    "        # Optimization settings\n",
    "        self.adaptive_chunking = True\n",
    "        self.min_chunk_size = 2\n",
    "        self.max_chunk_size = 8\n",
    "        self.base_delay = 0.05\n",
    "    \n",
    "    def optimized_stream(self, prompt: str) -> Generator[Dict[str, Any], None, None]:\n",
    "        \"\"\"Generate optimized streaming response.\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.total_requests += 1\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            result = self.predictor(prompt=prompt)\n",
    "            response_text = result.response\n",
    "            \n",
    "            # Adaptive chunking based on content\n",
    "            chunks = self._adaptive_chunk(response_text)\n",
    "            \n",
    "            # Stream with adaptive delay\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Adaptive delay based on chunk position and size\n",
    "                delay = self._calculate_adaptive_delay(i, len(chunks), len(chunk.split()))\n",
    "                \n",
    "                yield {\n",
    "                    'type': 'chunk',\n",
    "                    'content': chunk,\n",
    "                    'chunk_index': i,\n",
    "                    'total_chunks': len(chunks),\n",
    "                    'delay': delay,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                time.sleep(delay)\n",
    "            \n",
    "            # Track performance\n",
    "            response_time = time.time() - start_time\n",
    "            self.response_times.append(response_time)\n",
    "            self.chunk_counts.append(len(chunks))\n",
    "            \n",
    "            yield {\n",
    "                'type': 'complete',\n",
    "                'content': 'Stream complete',\n",
    "                'performance': {\n",
    "                    'response_time': response_time,\n",
    "                    'chunk_count': len(chunks),\n",
    "                    'avg_chunk_size': len(response_text.split()) / len(chunks)\n",
    "                },\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            yield {\n",
    "                'type': 'error',\n",
    "                'content': f'Streaming error: {e}',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def _adaptive_chunk(self, text: str) -> List[str]:\n",
    "        \"\"\"Create adaptive chunks based on content structure.\"\"\"\n",
    "        \n",
    "        if not self.adaptive_chunking:\n",
    "            # Simple fixed-size chunking\n",
    "            words = text.split()\n",
    "            chunk_size = (self.min_chunk_size + self.max_chunk_size) // 2\n",
    "            return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "        \n",
    "        # Adaptive chunking based on sentence boundaries and content\n",
    "        sentences = text.split('. ')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip() + '. ' if not sentence.endswith('.') else sentence + ' '\n",
    "            \n",
    "            # Check if adding this sentence would exceed max chunk size\n",
    "            potential_chunk = current_chunk + sentence\n",
    "            word_count = len(potential_chunk.split())\n",
    "            \n",
    "            if word_count <= self.max_chunk_size or not current_chunk:\n",
    "                current_chunk = potential_chunk\n",
    "            else:\n",
    "                # Finalize current chunk\n",
    "                if current_chunk.strip():\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "        \n",
    "        # Add remaining text\n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _calculate_adaptive_delay(self, chunk_index: int, total_chunks: int, chunk_word_count: int) -> float:\n",
    "        \"\"\"Calculate adaptive delay based on chunk characteristics.\"\"\"\n",
    "        \n",
    "        # Base delay\n",
    "        delay = self.base_delay\n",
    "        \n",
    "        # Adjust for chunk size (larger chunks get slightly longer delay)\n",
    "        delay += (chunk_word_count - self.min_chunk_size) * 0.01\n",
    "        \n",
    "        # Faster streaming at the beginning\n",
    "        if chunk_index < 3:\n",
    "            delay *= 0.8\n",
    "        \n",
    "        # Slightly slower at the end for dramatic effect\n",
    "        if chunk_index >= total_chunks - 2:\n",
    "            delay *= 1.2\n",
    "        \n",
    "        return max(0.02, min(0.15, delay))  # Clamp between 20ms and 150ms\n",
    "    \n",
    "    def get_performance_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get streaming performance statistics.\"\"\"\n",
    "        \n",
    "        if not self.response_times:\n",
    "            return {'status': 'no_data'}\n",
    "        \n",
    "        return {\n",
    "            'total_requests': self.total_requests,\n",
    "            'avg_response_time': sum(self.response_times) / len(self.response_times),\n",
    "            'min_response_time': min(self.response_times),\n",
    "            'max_response_time': max(self.response_times),\n",
    "            'avg_chunk_count': sum(self.chunk_counts) / len(self.chunk_counts),\n",
    "            'total_chunks_processed': sum(self.chunk_counts)\n",
    "        }\n",
    "    \n",
    "    def configure_optimization(self, adaptive_chunking: bool = True, min_chunk_size: int = 2, \n",
    "                             max_chunk_size: int = 8, base_delay: float = 0.05):\n",
    "        \"\"\"Configure optimization parameters.\"\"\"\n",
    "        self.adaptive_chunking = adaptive_chunking\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.base_delay = base_delay\n",
    "\n",
    "# Initialize optimized streaming module\n",
    "optimized_streaming = OptimizedStreamingModule()\n",
    "print_result(\"Performance-optimized streaming module initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738fe0e",
   "metadata": {},
   "source": [
    "## Testing Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c69ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Testing Performance Optimization\", \"Comparing optimized vs standard streaming\")\n",
    "\n",
    "# Test different optimization settings\n",
    "test_prompts = [\n",
    "    \"Explain machine learning algorithms in detail\",\n",
    "    \"Describe the history of computer science\",\n",
    "    \"What are the applications of artificial intelligence?\",\n",
    "]\n",
    "\n",
    "print(\"\\n1. Testing with Adaptive Chunking:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "optimized_streaming.configure_optimization(adaptive_chunking=True)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nTest {i}: {prompt}\")\n",
    "    print(\"Response: \", end=\"\")\n",
    "    \n",
    "    chunk_details = []\n",
    "    \n",
    "    for chunk_data in optimized_streaming.optimized_stream(prompt):\n",
    "        if chunk_data['type'] == 'chunk':\n",
    "            print(chunk_data['content'], end=\" \", flush=True)\n",
    "            chunk_details.append({\n",
    "                'size': len(chunk_data['content'].split()),\n",
    "                'delay': chunk_data['delay']\n",
    "            })\n",
    "        elif chunk_data['type'] == 'complete':\n",
    "            performance = chunk_data['performance']\n",
    "            print(f\"\\n\\nüìä Performance: {performance['response_time']:.2f}s, {performance['chunk_count']} chunks, avg {performance['avg_chunk_size']:.1f} words/chunk\")\n",
    "        elif chunk_data['type'] == 'error':\n",
    "            print(f\"\\n‚ùå {chunk_data['content']}\")\n",
    "\n",
    "# Compare with fixed chunking\n",
    "print(\"\\n\\n2. Testing with Fixed Chunking:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "optimized_streaming.configure_optimization(adaptive_chunking=False)\n",
    "\n",
    "test_prompt = \"Explain the future of artificial intelligence and its potential impact on society\"\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(\"Response: \", end=\"\")\n",
    "\n",
    "for chunk_data in optimized_streaming.optimized_stream(test_prompt):\n",
    "    if chunk_data['type'] == 'chunk':\n",
    "        print(chunk_data['content'], end=\" \", flush=True)\n",
    "    elif chunk_data['type'] == 'complete':\n",
    "        performance = chunk_data['performance']\n",
    "        print(f\"\\n\\nüìä Performance: {performance['response_time']:.2f}s, {performance['chunk_count']} chunks\")\n",
    "\n",
    "# Show overall performance stats\n",
    "print(\"\\n\\n3. Overall Performance Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "stats = optimized_streaming.get_performance_stats()\n",
    "if 'total_requests' in stats:\n",
    "    print_result(f\"Total Requests: {stats['total_requests']}\")\n",
    "    print_result(f\"Average Response Time: {stats['avg_response_time']:.3f}s\")\n",
    "    print_result(f\"Response Time Range: {stats['min_response_time']:.3f}s - {stats['max_response_time']:.3f}s\")\n",
    "    print_result(f\"Average Chunk Count: {stats['avg_chunk_count']:.1f}\")\n",
    "    print_result(f\"Total Chunks Processed: {stats['total_chunks_processed']}\")\n",
    "else:\n",
    "    print_result(\"No performance data available\")\n",
    "\n",
    "print(\"\\nüéØ Performance optimization demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185a3190",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated comprehensive streaming capabilities with DSPy:\n",
    "\n",
    "### Key Features Implemented:\n",
    "\n",
    "1. **Basic Streaming**: Simple word-by-word text generation\n",
    "2. **Advanced Structured Streaming**: Metadata-rich streaming with different content types\n",
    "3. **Async Streaming**: Non-blocking, concurrent response generation\n",
    "4. **Interactive Chat**: Conversational streaming with memory\n",
    "5. **Performance Optimization**: Adaptive chunking and delay management\n",
    "\n",
    "### DSPy Integration:\n",
    "\n",
    "- **Signatures**: Specialized streaming-optimized task definitions\n",
    "- **Modules**: Composable streaming components\n",
    "- **Chain of Thought**: Streaming reasoning and response generation\n",
    "- **Memory Management**: Persistent conversation state\n",
    "\n",
    "### Optimization Techniques:\n",
    "\n",
    "- **Adaptive Chunking**: Content-aware chunk sizing\n",
    "- **Dynamic Delays**: Context-sensitive streaming speed\n",
    "- **Performance Monitoring**: Real-time metrics and optimization\n",
    "- **Async Processing**: Concurrent request handling\n",
    "\n",
    "### Real-world Applications:\n",
    "\n",
    "- **Live Chat Systems**: Customer support and interactive assistants\n",
    "- **Content Generation**: Real-time writing and creative applications\n",
    "- **Educational Tools**: Interactive tutoring and explanation systems\n",
    "- **Gaming**: Dynamic narrative and dialogue systems\n",
    "- **Accessibility**: Real-time transcription and assistance\n",
    "\n",
    "### Best Practices for Production:\n",
    "\n",
    "1. **Error Handling**: Graceful degradation when streaming fails\n",
    "2. **Performance Monitoring**: Track response times and user engagement\n",
    "3. **Adaptive Configuration**: Adjust streaming parameters based on usage patterns\n",
    "4. **Resource Management**: Efficient memory and network usage\n",
    "5. **User Experience**: Balance speed with readability and comprehension\n",
    "\n",
    "This streaming implementation showcases how DSPy enables building responsive, interactive AI applications that provide engaging real-time user experiences while maintaining the power and flexibility of the DSPy framework."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
