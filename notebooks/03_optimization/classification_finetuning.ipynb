{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61bcfb05",
      "metadata": {},
      "source": [
        "# Classification Finetuning with DSPy\n",
        "\n",
        "This notebook demonstrates how to finetune models for classification tasks using DSPy optimizers and techniques.\n",
        "\n",
        "Based on the DSPy tutorial: [Classification Finetuning](https://dspy.ai/tutorials/classification_finetuning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55346404",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary libraries and configure the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaccc3af",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('../../')\n",
        "\n",
        "import dspy\n",
        "from utils import setup_default_lm, print_step, print_result, print_error\n",
        "from utils.datasets import get_sample_classification_data\n",
        "from dotenv import load_dotenv\n",
        "import random\n",
        "from typing import List, Dict\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv('../../.env')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91dc1d13",
      "metadata": {},
      "source": [
        "## Language Model Configuration\n",
        "\n",
        "Set up DSPy with a language model for classification finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "700bab65",
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(\"Setting up Language Model\", \"Configuring DSPy for classification finetuning\")\n",
        "\n",
        "try:\n",
        "    lm = setup_default_lm(provider=\"openai\", model=\"gpt-4o\", max_tokens=500)\n",
        "    dspy.configure(lm=lm)\n",
        "    print_result(\"Language model configured successfully!\", \"Status\")\n",
        "except Exception as e:\n",
        "    print_error(f\"Failed to configure language model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6310f786",
      "metadata": {},
      "source": [
        "## Classification Dataset Preparation\n",
        "\n",
        "Prepare and expand the classification dataset for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c13b526",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extended classification dataset\n",
        "def create_extended_classification_dataset():\n",
        "    \"\"\"Create a larger, more diverse classification dataset.\"\"\"\n",
        "    \n",
        "    # Base sentiment classification examples\n",
        "    base_examples = [\n",
        "        # Positive examples\n",
        "        (\"I absolutely love this product! It exceeded all my expectations and the quality is outstanding.\", \"positive\"),\n",
        "        (\"This is the best purchase I've made this year. Highly recommend to everyone!\", \"positive\"),\n",
        "        (\"Fantastic service and amazing quality. Will definitely buy again.\", \"positive\"),\n",
        "        (\"Great value for money. The product works perfectly and arrived quickly.\", \"positive\"),\n",
        "        (\"Excellent customer support and the product is exactly as described.\", \"positive\"),\n",
        "        \n",
        "        # Negative examples\n",
        "        (\"This is the worst product I've ever bought. Complete waste of money.\", \"negative\"),\n",
        "        (\"Terrible quality and poor customer service. Very disappointed.\", \"negative\"),\n",
        "        (\"Don't buy this! It broke after just one day of use.\", \"negative\"),\n",
        "        (\"Overpriced and underwhelming. Not worth the money at all.\", \"negative\"),\n",
        "        (\"Poor design and even worse functionality. Regret this purchase.\", \"negative\"),\n",
        "        \n",
        "        # Neutral examples\n",
        "        (\"The product is okay, nothing special but does what it's supposed to do.\", \"neutral\"),\n",
        "        (\"It's an average product. Not great, not terrible, just mediocre.\", \"neutral\"),\n",
        "        (\"Works as expected. No major complaints but nothing impressive either.\", \"neutral\"),\n",
        "        (\"Standard quality for the price point. Gets the job done.\", \"neutral\"),\n",
        "        (\"Decent product, though I've seen better alternatives in the market.\", \"neutral\"),\n",
        "        \n",
        "        # Mixed/complex examples\n",
        "        (\"Good quality but delivery was slow. Overall satisfied though.\", \"positive\"),\n",
        "        (\"The product itself is fine but the packaging was damaged.\", \"neutral\"),\n",
        "        (\"Love the design but wish it had more features for the price.\", \"neutral\"),\n",
        "        (\"Excellent build quality but customer service could be improved.\", \"positive\"),\n",
        "        (\"Not exactly what I expected but still useful. Mixed feelings.\", \"neutral\")\n",
        "    ]\n",
        "    \n",
        "    # Convert to DSPy examples\n",
        "    examples = [dspy.Example(text=text, sentiment=sentiment) \n",
        "                for text, sentiment in base_examples]\n",
        "    \n",
        "    return examples\n",
        "\n",
        "# Create the dataset\n",
        "classification_data = create_extended_classification_dataset()\n",
        "\n",
        "# Split into train/validation/test sets\n",
        "random.shuffle(classification_data)\n",
        "\n",
        "train_size = int(0.6 * len(classification_data))\n",
        "val_size = int(0.2 * len(classification_data))\n",
        "\n",
        "train_data = classification_data[:train_size]\n",
        "val_data = classification_data[train_size:train_size + val_size]\n",
        "test_data = classification_data[train_size + val_size:]\n",
        "\n",
        "print_step(\"Dataset Preparation\")\n",
        "print_result(f\"Training examples: {len(train_data)}\")\n",
        "print_result(f\"Validation examples: {len(val_data)}\")\n",
        "print_result(f\"Test examples: {len(test_data)}\")\n",
        "\n",
        "# Show sample data\n",
        "print_step(\"Sample Training Data\")\n",
        "for i, example in enumerate(train_data[:3]):\n",
        "    print(f\"Example {i+1}: {example.text[:50]}... -> {example.sentiment}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6023aa",
      "metadata": {},
      "source": [
        "## Classification Signatures and Modules\n",
        "\n",
        "Define signatures and modules for sentiment classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d082c931",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentimentClassification(dspy.Signature):\n",
        "    \"\"\"Classify the sentiment of the given text.\"\"\"\n",
        "    \n",
        "    text = dspy.InputField(desc=\"Text to classify for sentiment\")\n",
        "    sentiment = dspy.OutputField(desc=\"Sentiment classification: positive, negative, or neutral\")\n",
        "\n",
        "class SentimentWithReasoning(dspy.Signature):\n",
        "    \"\"\"Classify sentiment with reasoning explanation.\"\"\"\n",
        "    \n",
        "    text = dspy.InputField(desc=\"Text to classify for sentiment\")\n",
        "    reasoning = dspy.OutputField(desc=\"Explanation of why this sentiment was chosen\")\n",
        "    sentiment = dspy.OutputField(desc=\"Sentiment classification: positive, negative, or neutral\")\n",
        "\n",
        "class SentimentConfidence(dspy.Signature):\n",
        "    \"\"\"Classify sentiment with confidence score.\"\"\"\n",
        "    \n",
        "    text = dspy.InputField(desc=\"Text to classify for sentiment\")\n",
        "    sentiment = dspy.OutputField(desc=\"Sentiment classification: positive, negative, or neutral\")\n",
        "    confidence = dspy.OutputField(desc=\"Confidence score from 0.0 to 1.0\")\n",
        "\n",
        "# Basic classification module\n",
        "class BasicSentimentClassifier(dspy.Module):\n",
        "    \"\"\"Basic sentiment classification module.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.classify = dspy.Predict(SentimentClassification)\n",
        "    \n",
        "    def forward(self, text):\n",
        "        return self.classify(text=text)\n",
        "\n",
        "# Chain of Thought classifier\n",
        "class CoTSentimentClassifier(dspy.Module):\n",
        "    \"\"\"Chain of Thought sentiment classifier.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.classify = dspy.ChainOfThought(SentimentWithReasoning)\n",
        "    \n",
        "    def forward(self, text):\n",
        "        result = self.classify(text=text)\n",
        "        return dspy.Prediction(sentiment=result.sentiment, reasoning=result.reasoning)\n",
        "\n",
        "# Confidence-based classifier\n",
        "class ConfidenceSentimentClassifier(dspy.Module):\n",
        "    \"\"\"Sentiment classifier with confidence scoring.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.classify = dspy.ChainOfThought(SentimentConfidence)\n",
        "    \n",
        "    def forward(self, text):\n",
        "        result = self.classify(text=text)\n",
        "        return dspy.Prediction(\n",
        "            sentiment=result.sentiment, \n",
        "            confidence=result.confidence\n",
        "        )\n",
        "\n",
        "# Initialize classifiers\n",
        "basic_classifier = BasicSentimentClassifier()\n",
        "cot_classifier = CoTSentimentClassifier()\n",
        "confidence_classifier = ConfidenceSentimentClassifier()\n",
        "\n",
        "print_step(\"Classification Modules Initialized\")\n",
        "print(\"✓ Basic classifier ready\")\n",
        "print(\"✓ Chain of Thought classifier ready\") \n",
        "print(\"✓ Confidence-based classifier ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5100319b",
      "metadata": {},
      "source": [
        "## Baseline Performance Evaluation\n",
        "\n",
        "Evaluate baseline performance before optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a1e49b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_classifier(classifier, test_examples, classifier_name=\"Classifier\"):\n",
        "    \"\"\"Evaluate a classifier on test examples.\"\"\"\n",
        "    \n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    \n",
        "    print_step(f\"Evaluating {classifier_name}\")\n",
        "    \n",
        "    for example in test_examples:\n",
        "        try:\n",
        "            result = classifier(text=example.text)\n",
        "            predictions.append(result.sentiment.lower().strip())\n",
        "            true_labels.append(example.sentiment.lower().strip())\n",
        "        except Exception as e:\n",
        "            print_error(f\"Error processing example: {e}\")\n",
        "            predictions.append(\"neutral\")  # Default fallback\n",
        "            true_labels.append(example.sentiment.lower().strip())\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    \n",
        "    # Generate classification report\n",
        "    report = classification_report(true_labels, predictions, zero_division=0)\n",
        "    \n",
        "    print_result(f\"Accuracy: {accuracy:.3f}\", f\"{classifier_name} Performance\")\n",
        "    print_result(f\"Classification Report:\\n{report}\", \"Detailed Results\")\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'predictions': predictions,\n",
        "        'true_labels': true_labels,\n",
        "        'report': report\n",
        "    }\n",
        "\n",
        "# Evaluate baseline classifiers\n",
        "print_step(\"Baseline Performance Evaluation\")\n",
        "\n",
        "basic_results = evaluate_classifier(basic_classifier, test_data, \"Basic Classifier\")\n",
        "cot_results = evaluate_classifier(cot_classifier, test_data, \"Chain of Thought Classifier\")\n",
        "\n",
        "# Show example predictions\n",
        "print_step(\"Example Predictions\")\n",
        "for i, (example, basic_pred, cot_pred) in enumerate(zip(test_data[:3], \n",
        "                                                       basic_results['predictions'][:3],\n",
        "                                                       cot_results['predictions'][:3])):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Text: {example.text[:60]}...\")\n",
        "    print(f\"True: {example.sentiment}\")\n",
        "    print(f\"Basic: {basic_pred}\")\n",
        "    print(f\"CoT: {cot_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68117504",
      "metadata": {},
      "source": [
        "## DSPy Optimization for Classification\n",
        "\n",
        "Use DSPy optimizers to improve classification performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1fd5d7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metric for optimization\n",
        "def classification_accuracy(example, pred, trace=None):\n",
        "    \"\"\"Accuracy metric for classification optimization.\"\"\"\n",
        "    return pred.sentiment.lower().strip() == example.sentiment.lower().strip()\n",
        "\n",
        "# More sophisticated metric with partial credit\n",
        "def enhanced_classification_metric(example, pred, trace=None):\n",
        "    \"\"\"Enhanced metric that considers confidence and reasoning quality.\"\"\"\n",
        "    \n",
        "    # Basic accuracy\n",
        "    correct = pred.sentiment.lower().strip() == example.sentiment.lower().strip()\n",
        "    base_score = 1.0 if correct else 0.0\n",
        "    \n",
        "    # Bonus for high confidence when correct\n",
        "    if hasattr(pred, 'confidence') and correct:\n",
        "        try:\n",
        "            confidence_val = float(pred.confidence)\n",
        "            if confidence_val > 0.8:\n",
        "                base_score += 0.1  # Bonus for high confidence\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Penalty for high confidence when wrong\n",
        "    if hasattr(pred, 'confidence') and not correct:\n",
        "        try:\n",
        "            confidence_val = float(pred.confidence)\n",
        "            if confidence_val > 0.8:\n",
        "                base_score -= 0.1  # Penalty for overconfident wrong answers\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    return max(0.0, min(1.0, base_score))\n",
        "\n",
        "print_step(\"Setting up DSPy Optimization\")\n",
        "\n",
        "# Try BootstrapFewShot optimizer\n",
        "from dspy.teleprompt import BootstrapFewShot\n",
        "\n",
        "# Optimize the Chain of Thought classifier\n",
        "print_step(\"Optimizing Chain of Thought Classifier\")\n",
        "\n",
        "try:\n",
        "    # Set up optimizer\n",
        "    optimizer = BootstrapFewShot(\n",
        "        metric=classification_accuracy,\n",
        "        max_bootstrapped_demos=8,  # Number of examples to use\n",
        "        max_labeled_demos=4       # Number of labeled examples\n",
        "    )\n",
        "    \n",
        "    # Compile the optimized classifier\n",
        "    optimized_cot_classifier = optimizer.compile(\n",
        "        student=cot_classifier,\n",
        "        trainset=train_data[:10]  # Use subset for demo\n",
        "    )\n",
        "    \n",
        "    print_result(\"Chain of Thought classifier optimized successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print_error(f\"Optimization failed: {e}\")\n",
        "    print(\"Using original classifier as fallback\")\n",
        "    optimized_cot_classifier = cot_classifier\n",
        "\n",
        "# Try optimizing the confidence classifier\n",
        "print_step(\"Optimizing Confidence Classifier\")\n",
        "\n",
        "try:\n",
        "    confidence_optimizer = BootstrapFewShot(\n",
        "        metric=enhanced_classification_metric,\n",
        "        max_bootstrapped_demos=6,\n",
        "        max_labeled_demos=3\n",
        "    )\n",
        "    \n",
        "    optimized_confidence_classifier = confidence_optimizer.compile(\n",
        "        student=confidence_classifier,\n",
        "        trainset=train_data[:8]\n",
        "    )\n",
        "    \n",
        "    print_result(\"Confidence classifier optimized successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print_error(f\"Confidence optimization failed: {e}\")\n",
        "    optimized_confidence_classifier = confidence_classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0df76b88",
      "metadata": {},
      "source": [
        "## Post-Optimization Performance Evaluation\n",
        "\n",
        "Compare performance before and after optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa692b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(\"Post-Optimization Performance Evaluation\")\n",
        "\n",
        "# Evaluate optimized classifiers\n",
        "optimized_cot_results = evaluate_classifier(\n",
        "    optimized_cot_classifier, \n",
        "    test_data, \n",
        "    \"Optimized Chain of Thought\"\n",
        ")\n",
        "\n",
        "optimized_confidence_results = evaluate_classifier(\n",
        "    optimized_confidence_classifier,\n",
        "    test_data,\n",
        "    \"Optimized Confidence Classifier\"\n",
        ")\n",
        "\n",
        "# Performance comparison\n",
        "print_step(\"Performance Comparison Summary\")\n",
        "\n",
        "comparison_data = {\n",
        "    \"Basic Classifier\": basic_results['accuracy'],\n",
        "    \"Chain of Thought\": cot_results['accuracy'], \n",
        "    \"Optimized CoT\": optimized_cot_results['accuracy'],\n",
        "    \"Confidence Classifier\": 0.0,  # We'll calculate this separately\n",
        "    \"Optimized Confidence\": 0.0\n",
        "}\n",
        "\n",
        "# Calculate confidence classifier accuracy manually (since it might have different output format)\n",
        "try:\n",
        "    conf_predictions = []\n",
        "    conf_true_labels = []\n",
        "    \n",
        "    for example in test_data:\n",
        "        result = confidence_classifier(text=example.text)\n",
        "        conf_predictions.append(result.sentiment.lower().strip())\n",
        "        conf_true_labels.append(example.sentiment.lower().strip())\n",
        "    \n",
        "    conf_accuracy = accuracy_score(conf_true_labels, conf_predictions)\n",
        "    comparison_data[\"Confidence Classifier\"] = conf_accuracy\n",
        "    \n",
        "    # Optimized confidence accuracy\n",
        "    opt_conf_predictions = []\n",
        "    for example in test_data:\n",
        "        result = optimized_confidence_classifier(text=example.text)\n",
        "        opt_conf_predictions.append(result.sentiment.lower().strip())\n",
        "    \n",
        "    opt_conf_accuracy = accuracy_score(conf_true_labels, opt_conf_predictions)\n",
        "    comparison_data[\"Optimized Confidence\"] = opt_conf_accuracy\n",
        "    \n",
        "except Exception as e:\n",
        "    print_error(f\"Error calculating confidence classifier accuracy: {e}\")\n",
        "\n",
        "# Display comparison\n",
        "print_result(\"Classification Accuracy Comparison:\")\n",
        "for classifier_name, accuracy in comparison_data.items():\n",
        "    improvement = \"\"\n",
        "    if \"Optimized\" in classifier_name:\n",
        "        base_name = classifier_name.replace(\"Optimized \", \"\").replace(\"CoT\", \"Chain of Thought\")\n",
        "        base_accuracy = comparison_data.get(base_name, 0)\n",
        "        if base_accuracy > 0:\n",
        "            improvement = f\" (Δ: {accuracy - base_accuracy:+.3f})\"\n",
        "    \n",
        "    print(f\"  {classifier_name}: {accuracy:.3f}{improvement}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8c35077",
      "metadata": {},
      "source": [
        "## Advanced Finetuning Techniques\n",
        "\n",
        "Implement more sophisticated finetuning approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43504682",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EnsembleClassifier(dspy.Module):\n",
        "    \"\"\"Ensemble classifier combining multiple approaches.\"\"\"\n",
        "    \n",
        "    def __init__(self, classifiers, weights=None):\n",
        "        super().__init__()\n",
        "        self.classifiers = classifiers\n",
        "        self.weights = weights or [1.0] * len(classifiers)\n",
        "        \n",
        "        # Normalize weights\n",
        "        total_weight = sum(self.weights)\n",
        "        self.weights = [w/total_weight for w in self.weights]\n",
        "    \n",
        "    def forward(self, text):\n",
        "        \"\"\"Ensemble prediction using weighted voting.\"\"\"\n",
        "        \n",
        "        predictions = []\n",
        "        confidences = []\n",
        "        \n",
        "        for classifier in self.classifiers:\n",
        "            try:\n",
        "                result = classifier(text=text)\n",
        "                predictions.append(result.sentiment.lower().strip())\n",
        "                \n",
        "                # Try to get confidence if available\n",
        "                if hasattr(result, 'confidence'):\n",
        "                    try:\n",
        "                        confidences.append(float(result.confidence))\n",
        "                    except:\n",
        "                        confidences.append(0.5)  # Default confidence\n",
        "                else:\n",
        "                    confidences.append(0.5)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print_error(f\"Classifier failed: {e}\")\n",
        "                predictions.append(\"neutral\")\n",
        "                confidences.append(0.1)\n",
        "        \n",
        "        # Weighted voting\n",
        "        sentiment_scores = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n",
        "        \n",
        "        for pred, weight, conf in zip(predictions, self.weights, confidences):\n",
        "            sentiment_scores[pred] += weight * conf\n",
        "        \n",
        "        # Get prediction with highest weighted score\n",
        "        final_sentiment = max(sentiment_scores.keys(), key=lambda k: sentiment_scores[k])\n",
        "        final_confidence = sentiment_scores[final_sentiment]\n",
        "        \n",
        "        return dspy.Prediction(\n",
        "            sentiment=final_sentiment,\n",
        "            confidence=str(final_confidence),\n",
        "            individual_predictions=predictions,\n",
        "            ensemble_scores=sentiment_scores\n",
        "        )\n",
        "\n",
        "# Create ensemble classifier\n",
        "ensemble_classifiers = [\n",
        "    basic_classifier,\n",
        "    optimized_cot_classifier,\n",
        "    optimized_confidence_classifier\n",
        "]\n",
        "\n",
        "ensemble_weights = [0.2, 0.4, 0.4]  # Give more weight to optimized classifiers\n",
        "\n",
        "ensemble_classifier = EnsembleClassifier(\n",
        "    classifiers=ensemble_classifiers,\n",
        "    weights=ensemble_weights\n",
        ")\n",
        "\n",
        "# Evaluate ensemble\n",
        "print_step(\"Ensemble Classifier Evaluation\")\n",
        "\n",
        "ensemble_results = evaluate_classifier(\n",
        "    ensemble_classifier, \n",
        "    test_data, \n",
        "    \"Ensemble Classifier\"\n",
        ")\n",
        "\n",
        "print_step(\"Ensemble Performance Analysis\")\n",
        "print(f\"✓ Ensemble accuracy: {ensemble_results['accuracy']:.3f}\")\n",
        "\n",
        "# Show example ensemble predictions\n",
        "print_step(\"Example Ensemble Predictions\")\n",
        "for i, example in enumerate(test_data[:2]):\n",
        "    result = ensemble_classifier(text=example.text)\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Text: {example.text[:60]}...\")\n",
        "    print(f\"True: {example.sentiment}\")\n",
        "    print(f\"Ensemble: {result.sentiment}\")\n",
        "    print(f\"Confidence: {result.confidence}\")\n",
        "    print(f\"Individual: {result.individual_predictions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1458dd11",
      "metadata": {},
      "source": [
        "## Multi-class Classification Extension\n",
        "\n",
        "Extend to more complex multi-class classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af383a1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extended multi-class classification\n",
        "class EmotionClassification(dspy.Signature):\n",
        "    \"\"\"Classify text into detailed emotion categories.\"\"\"\n",
        "    \n",
        "    text = dspy.InputField(desc=\"Text to classify for emotion\")\n",
        "    emotion = dspy.OutputField(desc=\"Emotion: joy, anger, sadness, fear, surprise, disgust, or neutral\")\n",
        "\n",
        "class MultiClassEmotionClassifier(dspy.Module):\n",
        "    \"\"\"Multi-class emotion classifier.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.classify = dspy.ChainOfThought(EmotionClassification)\n",
        "    \n",
        "    def forward(self, text):\n",
        "        return self.classify(text=text)\n",
        "\n",
        "# Create extended dataset for emotion classification\n",
        "def create_emotion_dataset():\n",
        "    \"\"\"Create a multi-class emotion dataset.\"\"\"\n",
        "    \n",
        "    emotion_examples = [\n",
        "        (\"I'm so excited about the vacation! Can't wait to relax on the beach.\", \"joy\"),\n",
        "        (\"This traffic is making me late for my important meeting. So frustrating!\", \"anger\"),\n",
        "        (\"I miss my grandmother so much. It's been a year since she passed away.\", \"sadness\"),\n",
        "        (\"Walking alone in the dark alley made me really nervous.\", \"fear\"),\n",
        "        (\"Wow! I never expected to win the lottery! This is incredible!\", \"surprise\"),\n",
        "        (\"The smell from the garbage can was absolutely revolting.\", \"disgust\"),\n",
        "        (\"I'm going to the store to buy some groceries. Nothing special planned.\", \"neutral\"),\n",
        "        (\"Dancing at the wedding was the highlight of my week!\", \"joy\"),\n",
        "        (\"They promised to deliver on time but failed again. I'm furious.\", \"anger\"),\n",
        "        (\"The movie ending was so tragic, I couldn't stop crying.\", \"sadness\"),\n",
        "        (\"The thought of giving a presentation to 200 people terrifies me.\", \"fear\"),\n",
        "        (\"I found out my old friend moved back to town after 10 years!\", \"surprise\"),\n",
        "        (\"The restaurant served spoiled fish. It was disgusting.\", \"disgust\"),\n",
        "        (\"Checking emails and responding to routine work messages.\", \"neutral\")\n",
        "    ]\n",
        "    \n",
        "    return [dspy.Example(text=text, emotion=emotion) \n",
        "            for text, emotion in emotion_examples]\n",
        "\n",
        "emotion_data = create_emotion_dataset()\n",
        "\n",
        "# Split emotion data\n",
        "random.shuffle(emotion_data)\n",
        "emotion_train = emotion_data[:8]\n",
        "emotion_test = emotion_data[8:]\n",
        "\n",
        "# Initialize and test emotion classifier\n",
        "emotion_classifier = MultiClassEmotionClassifier()\n",
        "\n",
        "print_step(\"Multi-class Emotion Classification\")\n",
        "\n",
        "# Test emotion classifier\n",
        "for i, example in enumerate(emotion_test[:3]):\n",
        "    result = emotion_classifier(text=example.text)\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Text: {example.text}\")\n",
        "    print(f\"True emotion: {example.emotion}\")\n",
        "    print(f\"Predicted: {result.emotion}\")\n",
        "\n",
        "# Try to optimize emotion classifier\n",
        "print_step(\"Optimizing Emotion Classifier\")\n",
        "\n",
        "def emotion_accuracy(example, pred, trace=None):\n",
        "    \"\"\"Accuracy metric for emotion classification.\"\"\"\n",
        "    return pred.emotion.lower().strip() == example.emotion.lower().strip()\n",
        "\n",
        "try:\n",
        "    emotion_optimizer = BootstrapFewShot(\n",
        "        metric=emotion_accuracy,\n",
        "        max_bootstrapped_demos=4,\n",
        "        max_labeled_demos=2\n",
        "    )\n",
        "    \n",
        "    optimized_emotion_classifier = emotion_optimizer.compile(\n",
        "        student=emotion_classifier,\n",
        "        trainset=emotion_train\n",
        "    )\n",
        "    \n",
        "    print_result(\"Emotion classifier optimized successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print_error(f\"Emotion optimization failed: {e}\")\n",
        "    optimized_emotion_classifier = emotion_classifier\n",
        "\n",
        "# Evaluate emotion classification performance\n",
        "emotion_predictions = []\n",
        "emotion_true_labels = []\n",
        "\n",
        "for example in emotion_test:\n",
        "    result = optimized_emotion_classifier(text=example.text)\n",
        "    emotion_predictions.append(result.emotion.lower().strip())\n",
        "    emotion_true_labels.append(example.emotion.lower().strip())\n",
        "\n",
        "emotion_accuracy = accuracy_score(emotion_true_labels, emotion_predictions)\n",
        "print_result(f\"Emotion classification accuracy: {emotion_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45eaa6d5",
      "metadata": {},
      "source": [
        "## Custom Optimization Strategies\n",
        "\n",
        "Implement custom optimization strategies for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4eb102b",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdaptiveFewShotOptimizer:\n",
        "    \"\"\"Custom adaptive few-shot optimizer for classification.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_classifier, metric_func, adaptation_threshold=0.7):\n",
        "        self.base_classifier = base_classifier\n",
        "        self.metric_func = metric_func\n",
        "        self.adaptation_threshold = adaptation_threshold\n",
        "        self.learned_examples = []\n",
        "    \n",
        "    def adaptive_learning(self, train_examples, validation_examples):\n",
        "        \"\"\"Adaptively select the best few-shot examples.\"\"\"\n",
        "        \n",
        "        print_step(\"Adaptive Few-Shot Learning\")\n",
        "        \n",
        "        # Start with empty few-shot examples\n",
        "        best_examples = []\n",
        "        best_score = 0.0\n",
        "        \n",
        "        for candidate_example in train_examples:\n",
        "            # Try adding this example to the few-shot set\n",
        "            test_examples = best_examples + [candidate_example]\n",
        "            \n",
        "            # Evaluate performance with this set\n",
        "            score = self._evaluate_with_examples(test_examples, validation_examples)\n",
        "            \n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_examples = test_examples\n",
        "                print_result(f\"Added example, new score: {score:.3f}\")\n",
        "            \n",
        "            # Stop if we reach the threshold or have enough examples\n",
        "            if best_score >= self.adaptation_threshold or len(best_examples) >= 5:\n",
        "                break\n",
        "        \n",
        "        self.learned_examples = best_examples\n",
        "        print_result(f\"Final adaptive score: {best_score:.3f} with {len(best_examples)} examples\")\n",
        "        \n",
        "        return best_examples, best_score\n",
        "    \n",
        "    def _evaluate_with_examples(self, examples, validation_examples):\n",
        "        \"\"\"Evaluate classifier performance with given few-shot examples.\"\"\"\n",
        "        \n",
        "        # In a real implementation, this would update the classifier with examples\n",
        "        # For this demo, we'll simulate the effect\n",
        "        \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for val_example in validation_examples:\n",
        "            try:\n",
        "                # Simulate using few-shot examples to improve prediction\n",
        "                result = self.base_classifier(text=val_example.text)\n",
        "                \n",
        "                # Simple simulation: more examples = slight improvement\n",
        "                confidence_boost = min(0.1, len(examples) * 0.02)\n",
        "                \n",
        "                if self.metric_func(val_example, result):\n",
        "                    correct += 1\n",
        "                    # Boost correct predictions with more examples\n",
        "                    if len(examples) > 2:\n",
        "                        correct += confidence_boost\n",
        "                \n",
        "                total += 1\n",
        "                \n",
        "            except Exception as e:\n",
        "                total += 1\n",
        "        \n",
        "        return correct / total if total > 0 else 0.0\n",
        "\n",
        "# Test adaptive optimizer\n",
        "adaptive_optimizer = AdaptiveFewShotOptimizer(\n",
        "    base_classifier=basic_classifier,\n",
        "    metric_func=classification_accuracy,\n",
        "    adaptation_threshold=0.8\n",
        ")\n",
        "\n",
        "adaptive_examples, adaptive_score = adaptive_optimizer.adaptive_learning(\n",
        "    train_examples=train_data,\n",
        "    validation_examples=val_data\n",
        ")\n",
        "\n",
        "print_step(\"Adaptive Optimization Results\")\n",
        "print(f\"✓ Selected {len(adaptive_examples)} optimal examples\")\n",
        "print(f\"✓ Achieved score: {adaptive_score:.3f}\")\n",
        "print(f\"✓ Learned examples saved for future use\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74494546",
      "metadata": {},
      "source": [
        "## Cross-Validation and Robust Evaluation\n",
        "\n",
        "Implement cross-validation for robust performance assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96342bb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_validate_classifier(classifier, dataset, k_folds=3, metric_func=classification_accuracy):\n",
        "    \"\"\"Perform k-fold cross-validation on a classifier.\"\"\"\n",
        "    \n",
        "    print_step(f\"{k_folds}-Fold Cross-Validation\")\n",
        "    \n",
        "    # Shuffle dataset\n",
        "    shuffled_data = dataset.copy()\n",
        "    random.shuffle(shuffled_data)\n",
        "    \n",
        "    fold_size = len(shuffled_data) // k_folds\n",
        "    fold_scores = []\n",
        "    \n",
        "    for fold in range(k_folds):\n",
        "        print_step(f\"Fold {fold + 1}/{k_folds}\")\n",
        "        \n",
        "        # Create train/test split for this fold\n",
        "        start_idx = fold * fold_size\n",
        "        end_idx = start_idx + fold_size\n",
        "        \n",
        "        test_fold = shuffled_data[start_idx:end_idx]\n",
        "        train_fold = shuffled_data[:start_idx] + shuffled_data[end_idx:]\n",
        "        \n",
        "        # Evaluate on this fold\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for example in test_fold:\n",
        "            try:\n",
        "                result = classifier(text=example.text)\n",
        "                if metric_func(example, result):\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "            except Exception as e:\n",
        "                print_error(f\"Error in fold {fold + 1}: {e}\")\n",
        "                total += 1\n",
        "        \n",
        "        fold_score = correct / total if total > 0 else 0.0\n",
        "        fold_scores.append(fold_score)\n",
        "        \n",
        "        print_result(f\"Fold {fold + 1} accuracy: {fold_score:.3f}\")\n",
        "    \n",
        "    # Calculate statistics\n",
        "    mean_score = np.mean(fold_scores)\n",
        "    std_score = np.std(fold_scores)\n",
        "    \n",
        "    print_step(\"Cross-Validation Results\")\n",
        "    print_result(f\"Mean accuracy: {mean_score:.3f} ± {std_score:.3f}\")\n",
        "    print_result(f\"Individual fold scores: {[f'{score:.3f}' for score in fold_scores]}\")\n",
        "    \n",
        "    return {\n",
        "        'mean_score': mean_score,\n",
        "        'std_score': std_score,\n",
        "        'fold_scores': fold_scores\n",
        "    }\n",
        "\n",
        "# Perform cross-validation on different classifiers\n",
        "print_step(\"Cross-Validation Comparison\")\n",
        "\n",
        "# Use all available data for cross-validation\n",
        "all_data = train_data + val_data + test_data\n",
        "\n",
        "cv_results = {}\n",
        "\n",
        "# Cross-validate basic classifier\n",
        "cv_results['basic'] = cross_validate_classifier(\n",
        "    basic_classifier, \n",
        "    all_data, \n",
        "    k_folds=3\n",
        ")\n",
        "\n",
        "# Cross-validate optimized CoT classifier\n",
        "cv_results['optimized_cot'] = cross_validate_classifier(\n",
        "    optimized_cot_classifier,\n",
        "    all_data,\n",
        "    k_folds=3\n",
        ")\n",
        "\n",
        "# Cross-validate ensemble classifier\n",
        "cv_results['ensemble'] = cross_validate_classifier(\n",
        "    ensemble_classifier,\n",
        "    all_data,\n",
        "    k_folds=3\n",
        ")\n",
        "\n",
        "# Summary comparison\n",
        "print_step(\"Cross-Validation Summary\")\n",
        "for classifier_name, results in cv_results.items():\n",
        "    print(f\"{classifier_name.title()}: {results['mean_score']:.3f} ± {results['std_score']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbfd5a3a",
      "metadata": {},
      "source": [
        "## Best Practices for Classification Finetuning\n",
        "\n",
        "### Key Principles:\n",
        "\n",
        "1. **Data Quality**: Ensure high-quality, diverse training data\n",
        "2. **Proper Evaluation**: Use cross-validation and holdout test sets\n",
        "3. **Metric Selection**: Choose appropriate metrics for your task\n",
        "4. **Optimization Strategy**: Start simple, then add complexity\n",
        "5. **Ensemble Methods**: Combine multiple approaches for robustness\n",
        "\n",
        "### DSPy-Specific Best Practices:\n",
        "\n",
        "- **Start with Chain of Thought**: Often performs better than basic Predict\n",
        "- **Use BootstrapFewShot**: Effective for most classification tasks\n",
        "- **Custom Metrics**: Design metrics that match your business objectives\n",
        "- **Iterative Improvement**: Continuously refine based on performance\n",
        "- **Error Analysis**: Study misclassified examples to improve\n",
        "\n",
        "### Advanced Techniques:\n",
        "\n",
        "- **Ensemble Learning**: Combine multiple optimized classifiers\n",
        "- **Adaptive Learning**: Dynamically select training examples\n",
        "- **Multi-stage Optimization**: Optimize different components separately\n",
        "- **Domain Adaptation**: Tailor classifiers to specific domains\n",
        "\n",
        "### Production Considerations:\n",
        "\n",
        "- **Latency vs Accuracy**: Balance performance with speed requirements\n",
        "- **Confidence Scoring**: Implement uncertainty estimation\n",
        "- **Monitoring**: Track performance drift over time\n",
        "- **A/B Testing**: Compare different optimization strategies\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated comprehensive classification finetuning with DSPy:\n",
        "\n",
        "- **Dataset Preparation**: Created diverse, labeled classification datasets\n",
        "- **Multiple Architectures**: Compared basic, Chain of Thought, and confidence-based classifiers\n",
        "- **DSPy Optimization**: Used BootstrapFewShot and custom optimizers\n",
        "- **Ensemble Methods**: Combined multiple classifiers for improved performance\n",
        "- **Robust Evaluation**: Implemented cross-validation and comprehensive metrics\n",
        "- **Advanced Techniques**: Explored adaptive learning and multi-class classification\n",
        "\n",
        "These techniques enable building production-ready classification systems that can:\n",
        "- Achieve high accuracy through systematic optimization\n",
        "- Handle multiple classes and complex sentiment analysis\n",
        "- Provide confidence scores and uncertainty estimates\n",
        "- Scale to large datasets and real-world applications"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
