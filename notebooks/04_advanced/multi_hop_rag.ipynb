{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ebb693",
   "metadata": {},
   "source": [
    "# Advanced Multi-Hop RAG with DSPy\n",
    "\n",
    "This notebook demonstrates advanced multi-hop Retrieval-Augmented Generation (RAG) techniques using DSPy. We'll cover:\n",
    "\n",
    "1. Multi-hop reasoning patterns\n",
    "2. Dynamic query decomposition\n",
    "3. Iterative retrieval strategies\n",
    "4. Answer synthesis from multiple sources\n",
    "5. Performance optimization\n",
    "\n",
    "Multi-hop RAG is essential for complex questions that require information from multiple sources or reasoning across multiple steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fcee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "# Configure DSPy with OpenAI\n",
    "lm = dspy.OpenAI(model=\"gpt-3.5-turbo\")\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16265ecd",
   "metadata": {},
   "source": [
    "## Data Structures for Multi-Hop RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Represents a document in our knowledge base.\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    \n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Represents a retrieval result with relevance scoring.\"\"\"\n",
    "    document: Document\n",
    "    score: float\n",
    "    query: str\n",
    "    hop_number: int\n",
    "\n",
    "@dataclass \n",
    "class ReasoningStep:\n",
    "    \"\"\"Represents a step in multi-hop reasoning.\"\"\"\n",
    "    query: str\n",
    "    retrieved_docs: List[RetrievalResult]\n",
    "    intermediate_answer: str\n",
    "    confidence: float\n",
    "    next_query: Optional[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6cb97",
   "metadata": {},
   "source": [
    "## Query Decomposition Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDecomposition(dspy.Signature):\n",
    "    \"\"\"Decompose a complex question into simpler sub-questions for multi-hop reasoning.\"\"\"\n",
    "    \n",
    "    complex_question = dspy.InputField(desc=\"The original complex question\")\n",
    "    sub_questions = dspy.OutputField(desc=\"List of simpler sub-questions, one per line\")\n",
    "    reasoning_strategy = dspy.OutputField(desc=\"Brief explanation of the decomposition strategy\")\n",
    "\n",
    "class QueryDecomposer(dspy.Module):\n",
    "    \"\"\"Module for decomposing complex queries into sub-questions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decompose = dspy.ChainOfThought(QueryDecomposition)\n",
    "    \n",
    "    def forward(self, question: str) -> List[str]:\n",
    "        result = self.decompose(complex_question=question)\n",
    "        \n",
    "        # Parse sub-questions from the output\n",
    "        sub_questions = [\n",
    "            q.strip() for q in result.sub_questions.split('\\n') \n",
    "            if q.strip() and not q.strip().startswith('-') and '?' in q\n",
    "        ]\n",
    "        \n",
    "        return sub_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521b0c3",
   "metadata": {},
   "source": [
    "## Enhanced Retrieval Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2db7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedRetriever(dspy.Module):\n",
    "    \"\"\"Enhanced retriever with multi-hop capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, documents: List[Document], k: int = 3):\n",
    "        super().__init__()\n",
    "        self.documents = documents\n",
    "        self.k = k\n",
    "        \n",
    "        # Simple TF-IDF based retrieval (in practice, use vector embeddings)\n",
    "        self._build_index()\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"Build a simple keyword-based index.\"\"\"\n",
    "        self.doc_keywords = {}\n",
    "        for doc in self.documents:\n",
    "            # Simple keyword extraction\n",
    "            text = f\"{doc.title} {doc.content}\".lower()\n",
    "            keywords = set(word.strip('.,!?;:\"()[]{}') for word in text.split())\n",
    "            self.doc_keywords[doc.id] = keywords\n",
    "    \n",
    "    def retrieve(self, query: str, hop_number: int = 1, \n",
    "                previous_results: List[RetrievalResult] = None) -> List[RetrievalResult]:\n",
    "        \"\"\"Retrieve documents for a given query.\"\"\"\n",
    "        query_keywords = set(query.lower().split())\n",
    "        \n",
    "        # Calculate relevance scores\n",
    "        scored_docs = []\n",
    "        for doc in self.documents:\n",
    "            doc_keywords = self.doc_keywords[doc.id]\n",
    "            \n",
    "            # Basic keyword overlap scoring\n",
    "            overlap = len(query_keywords & doc_keywords)\n",
    "            score = overlap / len(query_keywords) if query_keywords else 0\n",
    "            \n",
    "            # Boost score if document relates to previous results\n",
    "            if previous_results and hop_number > 1:\n",
    "                for prev_result in previous_results:\n",
    "                    prev_keywords = self.doc_keywords[prev_result.document.id]\n",
    "                    connection_score = len(doc_keywords & prev_keywords) / len(doc_keywords)\n",
    "                    score += connection_score * 0.3\n",
    "            \n",
    "            if score > 0:\n",
    "                scored_docs.append(RetrievalResult(\n",
    "                    document=doc,\n",
    "                    score=score,\n",
    "                    query=query,\n",
    "                    hop_number=hop_number\n",
    "                ))\n",
    "        \n",
    "        # Sort by score and return top-k\n",
    "        scored_docs.sort(key=lambda x: x.score, reverse=True)\n",
    "        return scored_docs[:self.k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d81d4",
   "metadata": {},
   "source": [
    "## Multi-Hop Reasoning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntermediateReasoning(dspy.Signature):\n",
    "    \"\"\"Generate intermediate reasoning based on retrieved documents.\"\"\"\n",
    "    \n",
    "    question = dspy.InputField(desc=\"The sub-question being answered\")\n",
    "    context = dspy.InputField(desc=\"Retrieved documents context\")\n",
    "    previous_reasoning = dspy.InputField(desc=\"Previous reasoning steps\")\n",
    "    \n",
    "    intermediate_answer = dspy.OutputField(desc=\"Intermediate answer to the sub-question\")\n",
    "    confidence = dspy.OutputField(desc=\"Confidence score (0-1) in this answer\")\n",
    "    next_question = dspy.OutputField(desc=\"Next sub-question to explore, or 'COMPLETE' if done\")\n",
    "\n",
    "class MultiHopReasoner(dspy.Module):\n",
    "    \"\"\"Multi-hop reasoning module that iteratively retrieves and reasons.\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever: EnhancedRetriever, max_hops: int = 3):\n",
    "        super().__init__()\n",
    "        self.retriever = retriever\n",
    "        self.max_hops = max_hops\n",
    "        self.reasoning = dspy.ChainOfThought(IntermediateReasoning)\n",
    "    \n",
    "    def forward(self, question: str) -> List[ReasoningStep]:\n",
    "        \"\"\"Perform multi-hop reasoning for a question.\"\"\"\n",
    "        reasoning_steps = []\n",
    "        current_question = question\n",
    "        previous_results = []\n",
    "        \n",
    "        for hop in range(1, self.max_hops + 1):\n",
    "            # Retrieve documents for current question\n",
    "            retrieved_docs = self.retriever.retrieve(\n",
    "                current_question, hop, previous_results\n",
    "            )\n",
    "            \n",
    "            if not retrieved_docs:\n",
    "                break\n",
    "            \n",
    "            # Format context from retrieved documents\n",
    "            context = \"\\n\\n\".join([\n",
    "                f\"Document {i+1}: {doc.document.title}\\n{doc.document.content}\"\n",
    "                for i, doc in enumerate(retrieved_docs)\n",
    "            ])\n",
    "            \n",
    "            # Format previous reasoning\n",
    "            previous_reasoning = \"\\n\".join([\n",
    "                f\"Step {i+1}: {step.query} -> {step.intermediate_answer}\"\n",
    "                for i, step in enumerate(reasoning_steps)\n",
    "            ])\n",
    "            \n",
    "            # Generate intermediate reasoning\n",
    "            result = self.reasoning(\n",
    "                question=current_question,\n",
    "                context=context,\n",
    "                previous_reasoning=previous_reasoning or \"No previous steps\"\n",
    "            )\n",
    "            \n",
    "            # Parse confidence score\n",
    "            try:\n",
    "                confidence = float(result.confidence)\n",
    "            except (ValueError, TypeError):\n",
    "                confidence = 0.5\n",
    "            \n",
    "            # Create reasoning step\n",
    "            step = ReasoningStep(\n",
    "                query=current_question,\n",
    "                retrieved_docs=retrieved_docs,\n",
    "                intermediate_answer=result.intermediate_answer,\n",
    "                confidence=confidence,\n",
    "                next_query=result.next_question if result.next_question != \"COMPLETE\" else None\n",
    "            )\n",
    "            \n",
    "            reasoning_steps.append(step)\n",
    "            previous_results.extend(retrieved_docs)\n",
    "            \n",
    "            # Check if reasoning is complete\n",
    "            if result.next_question == \"COMPLETE\" or not result.next_question:\n",
    "                break\n",
    "            \n",
    "            current_question = result.next_question\n",
    "        \n",
    "        return reasoning_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88ee9b",
   "metadata": {},
   "source": [
    "## Answer Synthesis Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87175057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerSynthesis(dspy.Signature):\n",
    "    \"\"\"Synthesize a final answer from multi-hop reasoning steps.\"\"\"\n",
    "    \n",
    "    original_question = dspy.InputField(desc=\"The original complex question\")\n",
    "    reasoning_steps = dspy.InputField(desc=\"All reasoning steps with intermediate answers\")\n",
    "    confidence_scores = dspy.InputField(desc=\"Confidence scores for each step\")\n",
    "    \n",
    "    final_answer = dspy.OutputField(desc=\"Comprehensive final answer\")\n",
    "    supporting_evidence = dspy.OutputField(desc=\"Key supporting evidence from the reasoning\")\n",
    "    overall_confidence = dspy.OutputField(desc=\"Overall confidence in the final answer (0-1)\")\n",
    "\n",
    "class AnswerSynthesizer(dspy.Module):\n",
    "    \"\"\"Module for synthesizing final answers from multi-hop reasoning.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.synthesize = dspy.ChainOfThought(AnswerSynthesis)\n",
    "    \n",
    "    def forward(self, original_question: str, reasoning_steps: List[ReasoningStep]) -> Dict[str, Any]:\n",
    "        \"\"\"Synthesize final answer from reasoning steps.\"\"\"\n",
    "        # Format reasoning steps\n",
    "        steps_text = \"\\n\\n\".join([\n",
    "            f\"Step {i+1}:\\n\"\n",
    "            f\"Question: {step.query}\\n\"\n",
    "            f\"Answer: {step.intermediate_answer}\\n\"\n",
    "            f\"Confidence: {step.confidence:.2f}\"\n",
    "            for i, step in enumerate(reasoning_steps)\n",
    "        ])\n",
    "        \n",
    "        # Format confidence scores\n",
    "        confidence_text = \", \".join([f\"{step.confidence:.2f}\" for step in reasoning_steps])\n",
    "        \n",
    "        # Synthesize answer\n",
    "        result = self.synthesize(\n",
    "            original_question=original_question,\n",
    "            reasoning_steps=steps_text,\n",
    "            confidence_scores=confidence_text\n",
    "        )\n",
    "        \n",
    "        # Parse overall confidence\n",
    "        try:\n",
    "            overall_confidence = float(result.overall_confidence)\n",
    "        except (ValueError, TypeError):\n",
    "            # Calculate average confidence if parsing fails\n",
    "            overall_confidence = sum(step.confidence for step in reasoning_steps) / len(reasoning_steps)\n",
    "        \n",
    "        return {\n",
    "            \"final_answer\": result.final_answer,\n",
    "            \"supporting_evidence\": result.supporting_evidence,\n",
    "            \"overall_confidence\": overall_confidence,\n",
    "            \"reasoning_steps\": reasoning_steps\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32918e36",
   "metadata": {},
   "source": [
    "## Complete Multi-Hop RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5115134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHopRAG(dspy.Module):\n",
    "    \"\"\"Complete multi-hop RAG system.\"\"\"\n",
    "    \n",
    "    def __init__(self, documents: List[Document], max_hops: int = 3):\n",
    "        super().__init__()\n",
    "        self.retriever = EnhancedRetriever(documents)\n",
    "        self.decomposer = QueryDecomposer()\n",
    "        self.reasoner = MultiHopReasoner(self.retriever, max_hops)\n",
    "        self.synthesizer = AnswerSynthesizer()\n",
    "    \n",
    "    def forward(self, question: str, use_decomposition: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Process a question through multi-hop RAG.\"\"\"\n",
    "        results = {\n",
    "            \"original_question\": question,\n",
    "            \"decomposed_questions\": [],\n",
    "            \"reasoning_chains\": [],\n",
    "            \"final_answer\": \"\",\n",
    "            \"supporting_evidence\": \"\",\n",
    "            \"overall_confidence\": 0.0\n",
    "        }\n",
    "        \n",
    "        if use_decomposition:\n",
    "            # Decompose complex question\n",
    "            sub_questions = self.decomposer(question)\n",
    "            results[\"decomposed_questions\"] = sub_questions\n",
    "            \n",
    "            # Process each sub-question\n",
    "            all_reasoning_steps = []\n",
    "            for sub_q in sub_questions:\n",
    "                reasoning_steps = self.reasoner(sub_q)\n",
    "                results[\"reasoning_chains\"].append({\n",
    "                    \"sub_question\": sub_q,\n",
    "                    \"steps\": reasoning_steps\n",
    "                })\n",
    "                all_reasoning_steps.extend(reasoning_steps)\n",
    "        else:\n",
    "            # Process question directly\n",
    "            all_reasoning_steps = self.reasoner(question)\n",
    "            results[\"reasoning_chains\"] = [{\n",
    "                \"sub_question\": question,\n",
    "                \"steps\": all_reasoning_steps\n",
    "            }]\n",
    "        \n",
    "        # Synthesize final answer\n",
    "        if all_reasoning_steps:\n",
    "            synthesis_result = self.synthesizer(question, all_reasoning_steps)\n",
    "            results.update(synthesis_result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa33900",
   "metadata": {},
   "source": [
    "## Example Usage with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb487971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        id=\"doc1\",\n",
    "        title=\"Machine Learning Fundamentals\",\n",
    "        content=\"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data. It includes supervised learning, unsupervised learning, and reinforcement learning. Common algorithms include linear regression, decision trees, and neural networks.\",\n",
    "        metadata={\"category\": \"ML\", \"year\": 2023}\n",
    "    ),\n",
    "    Document(\n",
    "        id=\"doc2\",\n",
    "        title=\"Neural Networks and Deep Learning\",\n",
    "        content=\"Neural networks are computing systems inspired by biological neural networks. Deep learning uses multi-layer neural networks to model complex patterns in data. Popular architectures include CNNs for image processing and RNNs for sequence data.\",\n",
    "        metadata={\"category\": \"DL\", \"year\": 2023}\n",
    "    ),\n",
    "    Document(\n",
    "        id=\"doc3\",\n",
    "        title=\"Natural Language Processing\",\n",
    "        content=\"Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and humans through language. It includes tasks like text classification, named entity recognition, and machine translation. Modern NLP uses transformer models like BERT and GPT.\",\n",
    "        metadata={\"category\": \"NLP\", \"year\": 2023}\n",
    "    ),\n",
    "    Document(\n",
    "        id=\"doc4\",\n",
    "        title=\"Transformer Architecture\",\n",
    "        content=\"The Transformer architecture revolutionized NLP with its attention mechanism. It consists of encoder and decoder layers with multi-head attention. This architecture enabled the development of large language models like GPT and BERT, which achieve state-of-the-art performance on many NLP tasks.\",\n",
    "        metadata={\"category\": \"Architecture\", \"year\": 2023}\n",
    "    ),\n",
    "    Document(\n",
    "        id=\"doc5\",\n",
    "        title=\"Large Language Models\",\n",
    "        content=\"Large Language Models (LLMs) are neural networks trained on vast amounts of text data. They can generate human-like text, answer questions, and perform various language tasks. Examples include GPT-3, GPT-4, and Claude. These models use transformer architecture and are trained using self-supervised learning.\",\n",
    "        metadata={\"category\": \"LLM\", \"year\": 2023}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize the multi-hop RAG system\n",
    "multi_hop_rag = MultiHopRAG(sample_documents, max_hops=3)\n",
    "\n",
    "print(\"Multi-hop RAG system initialized with\", len(sample_documents), \"documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36abb3a9",
   "metadata": {},
   "source": [
    "## Test Complex Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb925b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a complex question requiring multi-hop reasoning\n",
    "complex_question = \"How do transformer architectures enable large language models to perform natural language processing tasks?\"\n",
    "\n",
    "print(f\"Processing question: {complex_question}\\n\")\n",
    "\n",
    "# Process the question\n",
    "result = multi_hop_rag(complex_question, use_decomposition=True)\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTI-HOP RAG RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nOriginal Question: {result['original_question']}\")\n",
    "\n",
    "if result['decomposed_questions']:\n",
    "    print(f\"\\nDecomposed Sub-questions:\")\n",
    "    for i, sub_q in enumerate(result['decomposed_questions'], 1):\n",
    "        print(f\"  {i}. {sub_q}\")\n",
    "\n",
    "print(f\"\\nReasoning Chains:\")\n",
    "for i, chain in enumerate(result['reasoning_chains'], 1):\n",
    "    print(f\"\\n  Chain {i} - {chain['sub_question']}:\")\n",
    "    for j, step in enumerate(chain['steps'], 1):\n",
    "        print(f\"    Step {j}: {step.query}\")\n",
    "        print(f\"    Answer: {step.intermediate_answer}\")\n",
    "        print(f\"    Confidence: {step.confidence:.2f}\")\n",
    "        if step.next_query:\n",
    "            print(f\"    Next Query: {step.next_query}\")\n",
    "        print()\n",
    "\n",
    "print(f\"Final Answer: {result['final_answer']}\")\n",
    "print(f\"\\nSupporting Evidence: {result['supporting_evidence']}\")\n",
    "print(f\"\\nOverall Confidence: {result['overall_confidence']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b3f8b",
   "metadata": {},
   "source": [
    "## Performance Analysis and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "def analyze_performance(rag_system: MultiHopRAG, test_questions: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze performance of the multi-hop RAG system.\"\"\"\n",
    "    results = {\n",
    "        \"total_questions\": len(test_questions),\n",
    "        \"processing_times\": [],\n",
    "        \"average_hops\": [],\n",
    "        \"confidence_scores\": [],\n",
    "        \"decomposition_effectiveness\": []\n",
    "    }\n",
    "    \n",
    "    for question in test_questions:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process with decomposition\n",
    "        result_with_decomp = rag_system(question, use_decomposition=True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_hops = sum(len(chain['steps']) for chain in result_with_decomp['reasoning_chains'])\n",
    "        avg_hops = total_hops / len(result_with_decomp['reasoning_chains']) if result_with_decomp['reasoning_chains'] else 0\n",
    "        \n",
    "        # Store results\n",
    "        results[\"processing_times\"].append(processing_time)\n",
    "        results[\"average_hops\"].append(avg_hops)\n",
    "        results[\"confidence_scores\"].append(result_with_decomp['overall_confidence'])\n",
    "        results[\"decomposition_effectiveness\"].append(len(result_with_decomp['decomposed_questions']))\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    results[\"avg_processing_time\"] = sum(results[\"processing_times\"]) / len(results[\"processing_times\"])\n",
    "    results[\"avg_confidence\"] = sum(results[\"confidence_scores\"]) / len(results[\"confidence_scores\"])\n",
    "    results[\"avg_decomposition_size\"] = sum(results[\"decomposition_effectiveness\"]) / len(results[\"decomposition_effectiveness\"])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test questions for performance analysis\n",
    "test_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do neural networks work in deep learning?\",\n",
    "    \"What makes transformer architectures effective for NLP?\",\n",
    "    \"How are large language models trained and what can they do?\"\n",
    "]\n",
    "\n",
    "print(\"Running performance analysis...\")\n",
    "perf_results = analyze_performance(multi_hop_rag, test_questions)\n",
    "\n",
    "print(\"\\n=\" * 50)\n",
    "print(\"PERFORMANCE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Questions Processed: {perf_results['total_questions']}\")\n",
    "print(f\"Average Processing Time: {perf_results['avg_processing_time']:.2f} seconds\")\n",
    "print(f\"Average Confidence Score: {perf_results['avg_confidence']:.2f}\")\n",
    "print(f\"Average Decomposition Size: {perf_results['avg_decomposition_size']:.1f} sub-questions\")\n",
    "print(f\"Average Reasoning Hops: {sum(perf_results['average_hops'])/len(perf_results['average_hops']):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db991292",
   "metadata": {},
   "source": [
    "## Advanced Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f089b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedMultiHopRAG(MultiHopRAG):\n",
    "    \"\"\"Optimized version of multi-hop RAG with caching and parallel processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, documents: List[Document], max_hops: int = 3, enable_cache: bool = True):\n",
    "        super().__init__(documents, max_hops)\n",
    "        self.enable_cache = enable_cache\n",
    "        self.retrieval_cache = {} if enable_cache else None\n",
    "        self.reasoning_cache = {} if enable_cache else None\n",
    "    \n",
    "    def _get_cache_key(self, query: str, hop: int = 1) -> str:\n",
    "        \"\"\"Generate cache key for query and hop combination.\"\"\"\n",
    "        return f\"{query.lower().strip()}_{hop}\"\n",
    "    \n",
    "    def cached_retrieve(self, query: str, hop_number: int = 1, \n",
    "                       previous_results: List[RetrievalResult] = None) -> List[RetrievalResult]:\n",
    "        \"\"\"Cached version of retrieval.\"\"\"\n",
    "        if not self.enable_cache:\n",
    "            return self.retriever.retrieve(query, hop_number, previous_results)\n",
    "        \n",
    "        cache_key = self._get_cache_key(query, hop_number)\n",
    "        \n",
    "        if cache_key in self.retrieval_cache:\n",
    "            return self.retrieval_cache[cache_key]\n",
    "        \n",
    "        results = self.retriever.retrieve(query, hop_number, previous_results)\n",
    "        self.retrieval_cache[cache_key] = results\n",
    "        return results\n",
    "    \n",
    "    def forward(self, question: str, use_decomposition: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Optimized forward pass with caching.\"\"\"\n",
    "        # Check reasoning cache\n",
    "        if self.enable_cache and question in self.reasoning_cache:\n",
    "            return self.reasoning_cache[question]\n",
    "        \n",
    "        # Process normally\n",
    "        result = super().forward(question, use_decomposition)\n",
    "        \n",
    "        # Cache result\n",
    "        if self.enable_cache:\n",
    "            self.reasoning_cache[question] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get caching statistics.\"\"\"\n",
    "        if not self.enable_cache:\n",
    "            return {\"cache_enabled\": False}\n",
    "        \n",
    "        return {\n",
    "            \"cache_enabled\": True,\n",
    "            \"retrieval_cache_size\": len(self.retrieval_cache),\n",
    "            \"reasoning_cache_size\": len(self.reasoning_cache)\n",
    "        }\n",
    "\n",
    "# Create optimized system\n",
    "optimized_rag = OptimizedMultiHopRAG(sample_documents, max_hops=3, enable_cache=True)\n",
    "\n",
    "print(\"Optimized Multi-hop RAG system created\")\n",
    "print(\"Cache stats:\", optimized_rag.get_cache_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29330a",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This notebook demonstrated advanced multi-hop RAG techniques with DSPy:\n",
    "\n",
    "### Key Components:\n",
    "1. **Query Decomposition**: Breaking complex questions into simpler sub-questions\n",
    "2. **Enhanced Retrieval**: Multi-hop aware retrieval with context from previous steps\n",
    "3. **Iterative Reasoning**: Step-by-step reasoning with intermediate answers\n",
    "4. **Answer Synthesis**: Combining multiple reasoning chains into coherent final answers\n",
    "\n",
    "### Optimization Techniques:\n",
    "- **Caching**: Store retrieval and reasoning results to avoid redundant computation\n",
    "- **Confidence Scoring**: Track confidence at each step for better answer quality\n",
    "- **Dynamic Stopping**: Stop reasoning when confidence is high or no new information is found\n",
    "\n",
    "### Best Practices:\n",
    "1. **Design for Modularity**: Keep components separate and reusable\n",
    "2. **Implement Caching**: Cache expensive operations like retrieval and LLM calls\n",
    "3. **Monitor Performance**: Track processing time, confidence scores, and reasoning quality\n",
    "4. **Handle Edge Cases**: Gracefully handle empty results and parsing errors\n",
    "5. **Optimize Retrieval**: Use vector embeddings and semantic similarity for better retrieval\n",
    "\n",
    "### Next Steps:\n",
    "- Implement vector-based retrieval with embeddings\n",
    "- Add parallel processing for sub-questions\n",
    "- Integrate evaluation metrics for reasoning quality\n",
    "- Experiment with different decomposition strategies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
