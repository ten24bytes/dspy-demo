{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "172ea1ab",
      "metadata": {},
      "source": [
        "# Experimental RL Optimization for DSPy\n",
        "\n",
        "This notebook demonstrates how to use Reinforcement Learning (RL) techniques to optimize DSPy programs and improve their performance through trial-and-error learning.\n",
        "\n",
        "Based on the DSPy tutorial: [Experimental RL Optimization for DSPy](https://dspy.ai/tutorials/rl_ai_program/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8c00a5",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary libraries and configure the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f912ce2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('../../')\n",
        "\n",
        "import dspy\n",
        "from utils import setup_default_lm, print_step, print_result, print_error\n",
        "from utils.datasets import get_sample_qa_data, get_sample_classification_data\n",
        "from dotenv import load_dotenv\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import json\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv('../../.env')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eecffe5",
      "metadata": {},
      "source": [
        "## Language Model Configuration\n",
        "\n",
        "Set up DSPy with a language model for RL optimization experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d0593e",
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(\"Setting up Language Model\", \"Configuring DSPy for RL optimization\")\n",
        "\n",
        "try:\n",
        "    lm = setup_default_lm(provider=\"openai\", model=\"gpt-4o\", max_tokens=1000)\n",
        "    dspy.configure(lm=lm)\n",
        "    print_result(\"Language model configured successfully!\", \"Status\")\n",
        "except Exception as e:\n",
        "    print_error(f\"Failed to configure language model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "068d6873",
      "metadata": {},
      "source": [
        "## RL Environment for DSPy Programs\n",
        "\n",
        "Create a reinforcement learning environment for optimizing DSPy programs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "799d94dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DSPyRLEnvironment:\n",
        "    \"\"\"RL Environment for optimizing DSPy programs.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_program, train_examples, eval_examples, metric_func):\n",
        "        self.base_program = base_program\n",
        "        self.train_examples = train_examples\n",
        "        self.eval_examples = eval_examples\n",
        "        self.metric_func = metric_func\n",
        "        \n",
        "        # State representation\n",
        "        self.current_state = self._get_initial_state()\n",
        "        self.episode_rewards = []\n",
        "        self.step_count = 0\n",
        "        self.max_steps = 50\n",
        "        \n",
        "        # Action space - different optimization strategies\n",
        "        self.action_space = {\n",
        "            0: \"adjust_temperature\",\n",
        "            1: \"modify_prompt_structure\", \n",
        "            2: \"change_example_selection\",\n",
        "            3: \"adjust_chain_of_thought\",\n",
        "            4: \"modify_output_format\",\n",
        "            5: \"ensemble_prediction\"\n",
        "        }\n",
        "        \n",
        "        # History tracking\n",
        "        self.performance_history = []\n",
        "        self.action_history = []\n",
        "        \n",
        "    def _get_initial_state(self):\n",
        "        \"\"\"Get initial state representation.\"\"\"\n",
        "        # Evaluate current program performance\n",
        "        performance = self._evaluate_program(self.base_program)\n",
        "        \n",
        "        return {\n",
        "            \"performance_score\": performance,\n",
        "            \"steps_taken\": 0,\n",
        "            \"recent_actions\": [],\n",
        "            \"program_complexity\": self._estimate_complexity(),\n",
        "            \"convergence_trend\": 0.0\n",
        "        }\n",
        "    \n",
        "    def _evaluate_program(self, program):\n",
        "        \"\"\"Evaluate program performance on evaluation examples.\"\"\"\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for example in self.eval_examples[:10]:  # Use subset for speed\n",
        "            try:\n",
        "                prediction = program(**example.inputs())\n",
        "                if self.metric_func(example, prediction):\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "            except:\n",
        "                total += 1\n",
        "        \n",
        "        return correct / total if total > 0 else 0.0\n",
        "    \n",
        "    def _estimate_complexity(self):\n",
        "        \"\"\"Estimate program complexity (simplified).\"\"\"\n",
        "        # In a real implementation, this would analyze the program structure\n",
        "        return random.uniform(0.3, 0.7)\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment for new episode.\"\"\"\n",
        "        self.current_state = self._get_initial_state()\n",
        "        self.step_count = 0\n",
        "        self.episode_rewards = []\n",
        "        return self.current_state\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Take an action and return new state, reward, done.\"\"\"\n",
        "        \n",
        "        self.step_count += 1\n",
        "        \n",
        "        # Apply action to modify program\n",
        "        modified_program, action_success = self._apply_action(action)\n",
        "        \n",
        "        # Evaluate modified program\n",
        "        new_performance = self._evaluate_program(modified_program)\n",
        "        \n",
        "        # Calculate reward\n",
        "        reward = self._calculate_reward(new_performance, action, action_success)\n",
        "        \n",
        "        # Update state\n",
        "        self.current_state = self._update_state(new_performance, action)\n",
        "        \n",
        "        # Check if episode is done\n",
        "        done = (self.step_count >= self.max_steps or \n",
        "                new_performance > 0.95 or  # Excellent performance achieved\n",
        "                len(self.episode_rewards) > 10 and all(r < 0 for r in self.episode_rewards[-5:]))  # Consistent poor performance\n",
        "        \n",
        "        # Record history\n",
        "        self.performance_history.append(new_performance)\n",
        "        self.action_history.append(action)\n",
        "        self.episode_rewards.append(reward)\n",
        "        \n",
        "        return self.current_state, reward, done, {\"performance\": new_performance, \"action_success\": action_success}\n",
        "    \n",
        "    def _apply_action(self, action):\n",
        "        \"\"\"Apply an action to modify the DSPy program.\"\"\"\n",
        "        \n",
        "        action_name = self.action_space[action]\n",
        "        \n",
        "        try:\n",
        "            if action_name == \"adjust_temperature\":\n",
        "                # Simulate adjusting model temperature\n",
        "                modified_program = self._modify_temperature()\n",
        "                return modified_program, True\n",
        "                \n",
        "            elif action_name == \"modify_prompt_structure\":\n",
        "                # Simulate prompt engineering\n",
        "                modified_program = self._modify_prompts()\n",
        "                return modified_program, True\n",
        "                \n",
        "            elif action_name == \"change_example_selection\":\n",
        "                # Simulate changing few-shot examples\n",
        "                modified_program = self._change_examples()\n",
        "                return modified_program, True\n",
        "                \n",
        "            elif action_name == \"adjust_chain_of_thought\":\n",
        "                # Simulate CoT modifications\n",
        "                modified_program = self._adjust_cot()\n",
        "                return modified_program, True\n",
        "                \n",
        "            elif action_name == \"modify_output_format\":\n",
        "                # Simulate output format changes\n",
        "                modified_program = self._modify_output_format()\n",
        "                return modified_program, True\n",
        "                \n",
        "            elif action_name == \"ensemble_prediction\":\n",
        "                # Simulate ensemble methods\n",
        "                modified_program = self._create_ensemble()\n",
        "                return modified_program, True\n",
        "                \n",
        "            else:\n",
        "                return self.base_program, False\n",
        "                \n",
        "        except Exception as e:\n",
        "            print_error(f\"Action failed: {e}\")\n",
        "            return self.base_program, False\n",
        "    \n",
        "    def _modify_temperature(self):\n",
        "        \"\"\"Simulate temperature adjustment.\"\"\"\n",
        "        # In practice, this would modify the actual LM configuration\n",
        "        class ModifiedProgram:\n",
        "            def __init__(self, base):\n",
        "                self.base = base\n",
        "                self.temperature_adjustment = random.uniform(-0.2, 0.2)\n",
        "            \n",
        "            def __call__(self, **kwargs):\n",
        "                # Simulate effect of temperature change\n",
        "                result = self.base(**kwargs)\n",
        "                # Add some randomness to simulate temperature effect\n",
        "                if hasattr(result, 'confidence'):\n",
        "                    result.confidence = max(0.1, min(1.0, result.confidence + self.temperature_adjustment))\n",
        "                return result\n",
        "        \n",
        "        return ModifiedProgram(self.base_program)\n",
        "    \n",
        "    def _modify_prompts(self):\n",
        "        \"\"\"Simulate prompt modifications.\"\"\"\n",
        "        class ModifiedProgram:\n",
        "            def __init__(self, base):\n",
        "                self.base = base\n",
        "                self.prompt_modification = random.choice([\"more_detailed\", \"concise\", \"structured\"])\n",
        "            \n",
        "            def __call__(self, **kwargs):\n",
        "                # Simulate prompt engineering effect\n",
        "                result = self.base(**kwargs)\n",
        "                # Randomly modify performance based on prompt changes\n",
        "                performance_modifier = random.uniform(0.9, 1.1)\n",
        "                return result\n",
        "        \n",
        "        return ModifiedProgram(self.base_program)\n",
        "    \n",
        "    def _change_examples(self):\n",
        "        \"\"\"Simulate changing few-shot examples.\"\"\"\n",
        "        class ModifiedProgram:\n",
        "            def __init__(self, base):\n",
        "                self.base = base\n",
        "                self.example_selection = random.choice([\"diverse\", \"similar\", \"hard_cases\"])\n",
        "            \n",
        "            def __call__(self, **kwargs):\n",
        "                return self.base(**kwargs)\n",
        "        \n",
        "        return ModifiedProgram(self.base_program)\n",
        "    \n",
        "    def _adjust_cot(self):\n",
        "        \"\"\"Simulate Chain of Thought adjustments.\"\"\"\n",
        "        class ModifiedProgram:\n",
        "            def __init__(self, base):\n",
        "                self.base = base\n",
        "                self.cot_style = random.choice([\"step_by_step\", \"reasoning_first\", \"conclusion_first\"])\n",
        "            \n",
        "            def __call__(self, **kwargs):\n",
        "                return self.base(**kwargs)\n",
        "        \n",
        "        return ModifiedProgram(self.base_program)\n",
        "    \n",
        "    def _modify_output_format(self):\n",
        "        \"\"\"Simulate output format modifications.\"\"\"\n",
        "        class ModifiedProgram:\n",
        "            def __init__(self, base):\n",
        "                self.base = base\n",
        "                self.output_format = random.choice([\"structured\", \"free_form\", \"json\", \"bullet_points\"])\n",
        "            \n",
        "            def __call__(self, **kwargs):\n",
        "                return self.base(**kwargs)\n",
        "        \n",
        "        return ModifiedProgram(self.base_program)\n",
        "    \n",
        "    def _create_ensemble(self):\n",
        "        \"\"\"Simulate ensemble creation.\"\"\"\n",
        "        class ModifiedProgram:\n",
        "            def __init__(self, base):\n",
        "                self.base = base\n",
        "                self.ensemble_size = random.randint(2, 4)\n",
        "            \n",
        "            def __call__(self, **kwargs):\n",
        "                # Simulate ensemble voting\n",
        "                return self.base(**kwargs)\n",
        "        \n",
        "        return ModifiedProgram(self.base_program)\n",
        "    \n",
        "    def _calculate_reward(self, new_performance, action, action_success):\n",
        "        \"\"\"Calculate reward for the action taken.\"\"\"\n",
        "        \n",
        "        if not action_success:\n",
        "            return -0.5  # Penalty for failed actions\n",
        "        \n",
        "        # Performance improvement reward\n",
        "        if self.performance_history:\n",
        "            performance_delta = new_performance - self.performance_history[-1]\n",
        "            reward = performance_delta * 10  # Scale the reward\n",
        "        else:\n",
        "            reward = new_performance * 5  # Initial performance reward\n",
        "        \n",
        "        # Bonus for high absolute performance\n",
        "        if new_performance > 0.8:\n",
        "            reward += 2.0\n",
        "        elif new_performance > 0.6:\n",
        "            reward += 1.0\n",
        "        \n",
        "        # Penalty for low performance\n",
        "        if new_performance < 0.3:\n",
        "            reward -= 1.0\n",
        "        \n",
        "        # Efficiency bonus (fewer steps to good performance)\n",
        "        if new_performance > 0.7 and self.step_count < 10:\n",
        "            reward += 1.0\n",
        "        \n",
        "        return reward\n",
        "    \n",
        "    def _update_state(self, new_performance, action):\n",
        "        \"\"\"Update state based on new performance and action.\"\"\"\n",
        "        \n",
        "        # Calculate convergence trend\n",
        "        if len(self.performance_history) >= 3:\n",
        "            recent_perf = self.performance_history[-3:]\n",
        "            trend = (recent_perf[-1] - recent_perf[0]) / 3\n",
        "        else:\n",
        "            trend = 0.0\n",
        "        \n",
        "        return {\n",
        "            \"performance_score\": new_performance,\n",
        "            \"steps_taken\": self.step_count,\n",
        "            \"recent_actions\": self.action_history[-5:],  # Last 5 actions\n",
        "            \"program_complexity\": self.current_state[\"program_complexity\"],\n",
        "            \"convergence_trend\": trend\n",
        "        }\n",
        "\n",
        "# Test the RL environment\n",
        "print_step(\"Testing RL Environment\")\n",
        "\n",
        "# Create a simple test program\n",
        "class SimpleQA(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.generate_answer = dspy.Predict(\"question -> answer\")\n",
        "    \n",
        "    def forward(self, question):\n",
        "        return self.generate_answer(question=question)\n",
        "\n",
        "# Simple metric\n",
        "def qa_metric(example, prediction, trace=None):\n",
        "    return example.answer.lower() in prediction.answer.lower()\n",
        "\n",
        "# Get test data\n",
        "qa_data = get_sample_qa_data()\n",
        "train_data = qa_data[:3]\n",
        "eval_data = qa_data[3:]\n",
        "\n",
        "# Initialize environment\n",
        "simple_qa = SimpleQA()\n",
        "rl_env = DSPyRLEnvironment(simple_qa, train_data, eval_data, qa_metric)\n",
        "\n",
        "# Test environment\n",
        "initial_state = rl_env.reset()\n",
        "print_result(f\"Initial state: {initial_state}\")\n",
        "\n",
        "# Take a few test steps\n",
        "for i in range(3):\n",
        "    action = random.randint(0, 5)\n",
        "    state, reward, done, info = rl_env.step(action)\n",
        "    print(f\"Step {i+1}: Action {action}, Reward {reward:.3f}, Performance {info['performance']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "676c99d5",
      "metadata": {},
      "source": [
        "## Q-Learning Agent for DSPy Optimization\n",
        "\n",
        "Implement a Q-Learning agent to optimize DSPy programs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1f3de88",
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "    \"\"\"Q-Learning agent for optimizing DSPy programs.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        # Q-table (simplified state representation)\n",
        "        self.q_table = defaultdict(lambda: np.zeros(action_size))\n",
        "        \n",
        "        # Training statistics\n",
        "        self.training_rewards = []\n",
        "        self.training_performance = []\n",
        "        self.episode_count = 0\n",
        "    \n",
        "    def _state_to_key(self, state):\n",
        "        \"\"\"Convert state dict to a key for Q-table.\"\"\"\n",
        "        # Discretize continuous values for Q-table\n",
        "        perf_bucket = int(state[\"performance_score\"] * 10)  # 0-10\n",
        "        steps_bucket = min(state[\"steps_taken\"] // 5, 10)   # 0-10\n",
        "        trend_bucket = int((state[\"convergence_trend\"] + 1) * 5)  # 0-10\n",
        "        \n",
        "        return (perf_bucket, steps_bucket, trend_bucket)\n",
        "    \n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n",
        "        state_key = self._state_to_key(state)\n",
        "        \n",
        "        if random.random() < self.epsilon:\n",
        "            # Exploration\n",
        "            return random.randint(0, self.action_size - 1)\n",
        "        else:\n",
        "            # Exploitation\n",
        "            return np.argmax(self.q_table[state_key])\n",
        "    \n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Update Q-values using Q-learning update rule.\"\"\"\n",
        "        \n",
        "        state_key = self._state_to_key(state)\n",
        "        next_state_key = self._state_to_key(next_state)\n",
        "        \n",
        "        # Q-learning update\n",
        "        current_q = self.q_table[state_key][action]\n",
        "        \n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            target_q = reward + self.discount_factor * np.max(self.q_table[next_state_key])\n",
        "        \n",
        "        # Update Q-value\n",
        "        self.q_table[state_key][action] = current_q + self.learning_rate * (target_q - current_q)\n",
        "        \n",
        "        # Decay epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "    \n",
        "    def train(self, environment, num_episodes=100):\n",
        "        \"\"\"Train the agent in the given environment.\"\"\"\n",
        "        \n",
        "        print_step(\"Q-Learning Training\", f\"Training for {num_episodes} episodes\")\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            state = environment.reset()\n",
        "            total_reward = 0\n",
        "            step_count = 0\n",
        "            \n",
        "            while True:\n",
        "                # Choose action\n",
        "                action = self.choose_action(state)\n",
        "                \n",
        "                # Take action\n",
        "                next_state, reward, done, info = environment.step(action)\n",
        "                \n",
        "                # Learn from experience\n",
        "                self.learn(state, action, reward, next_state, done)\n",
        "                \n",
        "                # Update tracking\n",
        "                total_reward += reward\n",
        "                step_count += 1\n",
        "                state = next_state\n",
        "                \n",
        "                if done:\n",
        "                    break\n",
        "            \n",
        "            # Record episode statistics\n",
        "            self.training_rewards.append(total_reward)\n",
        "            final_performance = info.get(\"performance\", 0.0)\n",
        "            self.training_performance.append(final_performance)\n",
        "            self.episode_count += 1\n",
        "            \n",
        "            # Print progress\n",
        "            if episode % 20 == 0:\n",
        "                avg_reward = np.mean(self.training_rewards[-20:]) if len(self.training_rewards) >= 20 else total_reward\n",
        "                avg_performance = np.mean(self.training_performance[-20:]) if len(self.training_performance) >= 20 else final_performance\n",
        "                print(f\"Episode {episode}: Avg Reward: {avg_reward:.3f}, Avg Performance: {avg_performance:.3f}, Epsilon: {self.epsilon:.3f}\")\n",
        "        \n",
        "        print_result(f\"Training completed! Final epsilon: {self.epsilon:.3f}\")\n",
        "    \n",
        "    def get_training_stats(self):\n",
        "        \"\"\"Get training statistics.\"\"\"\n",
        "        return {\n",
        "            \"total_episodes\": self.episode_count,\n",
        "            \"final_epsilon\": self.epsilon,\n",
        "            \"average_reward\": np.mean(self.training_rewards) if self.training_rewards else 0,\n",
        "            \"best_performance\": max(self.training_performance) if self.training_performance else 0,\n",
        "            \"final_performance\": self.training_performance[-1] if self.training_performance else 0,\n",
        "            \"q_table_size\": len(self.q_table)\n",
        "        }\n",
        "\n",
        "# Initialize and train Q-learning agent\n",
        "print_step(\"Initializing Q-Learning Agent\")\n",
        "\n",
        "agent = QLearningAgent(\n",
        "    state_size=4,  # Simplified state representation\n",
        "    action_size=6,  # Number of available actions\n",
        "    learning_rate=0.1,\n",
        "    discount_factor=0.95,\n",
        "    epsilon=1.0,\n",
        "    epsilon_decay=0.995,\n",
        "    epsilon_min=0.01\n",
        ")\n",
        "\n",
        "# Train the agent\n",
        "agent.train(rl_env, num_episodes=50)\n",
        "\n",
        "# Get training statistics\n",
        "stats = agent.get_training_stats()\n",
        "print_step(\"Training Statistics\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "631d0562",
      "metadata": {},
      "source": [
        "## Advanced RL Optimization with Policy Gradient\n",
        "\n",
        "Implement a more sophisticated Policy Gradient method for DSPy optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3074535e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyGradientAgent:\n",
        "    \"\"\"Policy Gradient agent for DSPy optimization.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, action_size, learning_rate=0.01):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # Policy parameters (simplified linear policy)\n",
        "        self.policy_weights = np.random.randn(state_size, action_size) * 0.1\n",
        "        \n",
        "        # Episode memory\n",
        "        self.episode_states = []\n",
        "        self.episode_actions = []\n",
        "        self.episode_rewards = []\n",
        "        \n",
        "        # Training statistics\n",
        "        self.episode_returns = []\n",
        "        self.policy_losses = []\n",
        "    \n",
        "    def _state_to_vector(self, state):\n",
        "        \"\"\"Convert state dict to feature vector.\"\"\"\n",
        "        return np.array([\n",
        "            state[\"performance_score\"],\n",
        "            state[\"steps_taken\"] / 50.0,  # Normalize\n",
        "            state[\"convergence_trend\"],\n",
        "            len(state[\"recent_actions\"]) / 5.0  # Normalize\n",
        "        ])\n",
        "    \n",
        "    def _softmax(self, x):\n",
        "        \"\"\"Softmax activation function.\"\"\"\n",
        "        exp_x = np.exp(x - np.max(x))\n",
        "        return exp_x / np.sum(exp_x)\n",
        "    \n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Choose action based on policy probabilities.\"\"\"\n",
        "        state_vector = self._state_to_vector(state)\n",
        "        \n",
        "        # Compute action probabilities\n",
        "        logits = np.dot(state_vector, self.policy_weights)\n",
        "        action_probs = self._softmax(logits)\n",
        "        \n",
        "        # Sample action from probability distribution\n",
        "        action = np.random.choice(self.action_size, p=action_probs)\n",
        "        \n",
        "        return action, action_probs[action]\n",
        "    \n",
        "    def store_transition(self, state, action, reward):\n",
        "        \"\"\"Store transition for later learning.\"\"\"\n",
        "        state_vector = self._state_to_vector(state)\n",
        "        \n",
        "        self.episode_states.append(state_vector)\n",
        "        self.episode_actions.append(action)\n",
        "        self.episode_rewards.append(reward)\n",
        "    \n",
        "    def learn_from_episode(self):\n",
        "        \"\"\"Learn from completed episode using policy gradient.\"\"\"\n",
        "        \n",
        "        if not self.episode_rewards:\n",
        "            return\n",
        "        \n",
        "        # Calculate discounted returns\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for reward in reversed(self.episode_rewards):\n",
        "            G = reward + 0.99 * G  # Discount factor = 0.99\n",
        "            returns.insert(0, G)\n",
        "        \n",
        "        # Normalize returns\n",
        "        returns = np.array(returns)\n",
        "        if len(returns) > 1:\n",
        "            returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-8)\n",
        "        \n",
        "        # Policy gradient update\n",
        "        for i in range(len(self.episode_states)):\n",
        "            state = self.episode_states[i]\n",
        "            action = self.episode_actions[i]\n",
        "            G = returns[i]\n",
        "            \n",
        "            # Compute gradient\n",
        "            logits = np.dot(state, self.policy_weights)\n",
        "            action_probs = self._softmax(logits)\n",
        "            \n",
        "            # Policy gradient\n",
        "            grad = np.zeros_like(self.policy_weights)\n",
        "            for a in range(self.action_size):\n",
        "                if a == action:\n",
        "                    grad[:, a] = state * (1 - action_probs[a]) * G\n",
        "                else:\n",
        "                    grad[:, a] = -state * action_probs[a] * G\n",
        "            \n",
        "            # Update policy weights\n",
        "            self.policy_weights += self.learning_rate * grad\n",
        "        \n",
        "        # Record episode return\n",
        "        episode_return = sum(self.episode_rewards)\n",
        "        self.episode_returns.append(episode_return)\n",
        "        \n",
        "        # Clear episode memory\n",
        "        self.episode_states = []\n",
        "        self.episode_actions = []\n",
        "        self.episode_rewards = []\n",
        "    \n",
        "    def train(self, environment, num_episodes=100):\n",
        "        \"\"\"Train the policy gradient agent.\"\"\"\n",
        "        \n",
        "        print_step(\"Policy Gradient Training\", f\"Training for {num_episodes} episodes\")\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            state = environment.reset()\n",
        "            episode_reward = 0\n",
        "            \n",
        "            while True:\n",
        "                # Choose action\n",
        "                action, action_prob = self.choose_action(state)\n",
        "                \n",
        "                # Take action\n",
        "                next_state, reward, done, info = environment.step(action)\n",
        "                \n",
        "                # Store transition\n",
        "                self.store_transition(state, action, reward)\n",
        "                \n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "                \n",
        "                if done:\n",
        "                    break\n",
        "            \n",
        "            # Learn from episode\n",
        "            self.learn_from_episode()\n",
        "            \n",
        "            # Print progress\n",
        "            if episode % 20 == 0:\n",
        "                avg_return = np.mean(self.episode_returns[-20:]) if len(self.episode_returns) >= 20 else episode_reward\n",
        "                avg_performance = np.mean([info.get(\"performance\", 0) for _ in range(min(20, len(self.episode_returns)))])\n",
        "                print(f\"Episode {episode}: Avg Return: {avg_return:.3f}, Recent Performance: {info.get('performance', 0):.3f}\")\n",
        "        \n",
        "        print_result(\"Policy Gradient training completed!\")\n",
        "    \n",
        "    def get_policy_summary(self):\n",
        "        \"\"\"Get summary of learned policy.\"\"\"\n",
        "        return {\n",
        "            \"policy_weights_shape\": self.policy_weights.shape,\n",
        "            \"total_episodes\": len(self.episode_returns),\n",
        "            \"average_return\": np.mean(self.episode_returns) if self.episode_returns else 0,\n",
        "            \"best_return\": max(self.episode_returns) if self.episode_returns else 0,\n",
        "            \"policy_weights_norm\": np.linalg.norm(self.policy_weights)\n",
        "        }\n",
        "\n",
        "# Test Policy Gradient agent\n",
        "print_step(\"Testing Policy Gradient Agent\")\n",
        "\n",
        "pg_agent = PolicyGradientAgent(\n",
        "    state_size=4,\n",
        "    action_size=6,\n",
        "    learning_rate=0.01\n",
        ")\n",
        "\n",
        "# Create fresh environment for PG training\n",
        "rl_env_pg = DSPyRLEnvironment(simple_qa, train_data, eval_data, qa_metric)\n",
        "\n",
        "# Train policy gradient agent\n",
        "pg_agent.train(rl_env_pg, num_episodes=50)\n",
        "\n",
        "# Get policy summary\n",
        "pg_stats = pg_agent.get_policy_summary()\n",
        "print_step(\"Policy Gradient Statistics\")\n",
        "for key, value in pg_stats.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5893ff",
      "metadata": {},
      "source": [
        "## Multi-Agent RL for Complex DSPy Optimization\n",
        "\n",
        "Implement multiple RL agents working together to optimize different aspects of DSPy programs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8527b21d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiAgentRLOptimizer:\n",
        "    \"\"\"Multi-agent RL system for comprehensive DSPy optimization.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_program, train_examples, eval_examples, metric_func):\n",
        "        self.base_program = base_program\n",
        "        self.train_examples = train_examples\n",
        "        self.eval_examples = eval_examples\n",
        "        self.metric_func = metric_func\n",
        "        \n",
        "        # Specialized agents for different optimization aspects\n",
        "        self.agents = {\n",
        "            \"prompt_optimizer\": QLearningAgent(state_size=4, action_size=3, learning_rate=0.1),\n",
        "            \"example_selector\": QLearningAgent(state_size=4, action_size=3, learning_rate=0.1),\n",
        "            \"structure_optimizer\": PolicyGradientAgent(state_size=4, action_size=3, learning_rate=0.01)\n",
        "        }\n",
        "        \n",
        "        # Specialized action spaces for each agent\n",
        "        self.agent_actions = {\n",
        "            \"prompt_optimizer\": {\n",
        "                0: \"detailed_prompts\",\n",
        "                1: \"concise_prompts\", \n",
        "                2: \"structured_prompts\"\n",
        "            },\n",
        "            \"example_selector\": {\n",
        "                0: \"diverse_examples\",\n",
        "                1: \"similar_examples\",\n",
        "                2: \"hard_examples\"\n",
        "            },\n",
        "            \"structure_optimizer\": {\n",
        "                0: \"chain_of_thought\",\n",
        "                1: \"direct_prediction\",\n",
        "                2: \"ensemble_approach\"\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Coordination mechanism\n",
        "        self.coordination_history = []\n",
        "        self.collaborative_rewards = []\n",
        "    \n",
        "    def optimize_collaboratively(self, num_episodes=30):\n",
        "        \"\"\"Optimize using collaborative multi-agent approach.\"\"\"\n",
        "        \n",
        "        print_step(\"Multi-Agent Collaborative Optimization\", f\"Running {num_episodes} episodes\")\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            print_step(f\"Episode {episode + 1}\")\n",
        "            \n",
        "            # Each agent contributes to the optimization\n",
        "            agent_contributions = {}\n",
        "            total_episode_reward = 0\n",
        "            \n",
        "            for agent_name, agent in self.agents.items():\n",
        "                print(f\"  {agent_name} optimization...\")\n",
        "                \n",
        "                # Create specialized environment for this agent\n",
        "                specialized_env = self._create_specialized_environment(agent_name)\n",
        "                \n",
        "                # Run optimization for this agent\n",
        "                state = specialized_env.reset()\n",
        "                agent_reward = 0\n",
        "                \n",
        "                for step in range(5):  # Limited steps per agent per episode\n",
        "                    if isinstance(agent, QLearningAgent):\n",
        "                        action = agent.choose_action(state)\n",
        "                    else:  # PolicyGradientAgent\n",
        "                        action, _ = agent.choose_action(state)\n",
        "                    \n",
        "                    next_state, reward, done, info = specialized_env.step(action)\n",
        "                    \n",
        "                    # Learn from experience\n",
        "                    if isinstance(agent, QLearningAgent):\n",
        "                        agent.learn(state, action, reward, next_state, done)\n",
        "                    else:\n",
        "                        agent.store_transition(state, action, reward)\n",
        "                    \n",
        "                    agent_reward += reward\n",
        "                    state = next_state\n",
        "                    \n",
        "                    if done:\n",
        "                        break\n",
        "                \n",
        "                # For Policy Gradient agents, learn from episode\n",
        "                if isinstance(agent, PolicyGradientAgent):\n",
        "                    agent.learn_from_episode()\n",
        "                \n",
        "                agent_contributions[agent_name] = {\n",
        "                    \"reward\": agent_reward,\n",
        "                    \"performance\": info.get(\"performance\", 0),\n",
        "                    \"actions_taken\": step + 1\n",
        "                }\n",
        "                \n",
        "                total_episode_reward += agent_reward\n",
        "            \n",
        "            # Coordinate agents and share rewards\n",
        "            coordination_bonus = self._calculate_coordination_bonus(agent_contributions)\n",
        "            total_episode_reward += coordination_bonus\n",
        "            \n",
        "            self.collaborative_rewards.append(total_episode_reward)\n",
        "            self.coordination_history.append(agent_contributions)\n",
        "            \n",
        "            # Print episode summary\n",
        "            if episode % 10 == 0:\n",
        "                avg_reward = np.mean(self.collaborative_rewards[-10:]) if len(self.collaborative_rewards) >= 10 else total_episode_reward\n",
        "                print(f\"  Episode {episode}: Avg Collaborative Reward: {avg_reward:.3f}\")\n",
        "                \n",
        "                for agent_name, contrib in agent_contributions.items():\n",
        "                    print(f\"    {agent_name}: Reward {contrib['reward']:.3f}, Performance {contrib['performance']:.3f}\")\n",
        "        \n",
        "        print_result(\"Multi-agent optimization completed!\")\n",
        "    \n",
        "    def _create_specialized_environment(self, agent_name):\n",
        "        \"\"\"Create specialized environment for specific agent.\"\"\"\n",
        "        \n",
        "        # Simplified specialized environments\n",
        "        if agent_name == \"prompt_optimizer\":\n",
        "            # Focus on prompt-related actions\n",
        "            specialized_env = DSPyRLEnvironment(self.base_program, self.train_examples, self.eval_examples, self.metric_func)\n",
        "            specialized_env.action_space = {0: \"detailed_prompts\", 1: \"concise_prompts\", 2: \"structured_prompts\"}\n",
        "        \n",
        "        elif agent_name == \"example_selector\":\n",
        "            # Focus on example selection\n",
        "            specialized_env = DSPyRLEnvironment(self.base_program, self.train_examples, self.eval_examples, self.metric_func)\n",
        "            specialized_env.action_space = {0: \"diverse_examples\", 1: \"similar_examples\", 2: \"hard_examples\"}\n",
        "        \n",
        "        else:  # structure_optimizer\n",
        "            # Focus on program structure\n",
        "            specialized_env = DSPyRLEnvironment(self.base_program, self.train_examples, self.eval_examples, self.metric_func)\n",
        "            specialized_env.action_space = {0: \"chain_of_thought\", 1: \"direct_prediction\", 2: \"ensemble_approach\"}\n",
        "        \n",
        "        return specialized_env\n",
        "    \n",
        "    def _calculate_coordination_bonus(self, agent_contributions):\n",
        "        \"\"\"Calculate bonus reward for good coordination between agents.\"\"\"\n",
        "        \n",
        "        # Reward agents for complementary improvements\n",
        "        performances = [contrib[\"performance\"] for contrib in agent_contributions.values()]\n",
        "        avg_performance = np.mean(performances)\n",
        "        \n",
        "        # Bonus for high average performance\n",
        "        coordination_bonus = 0.0\n",
        "        if avg_performance > 0.7:\n",
        "            coordination_bonus += 2.0\n",
        "        elif avg_performance > 0.5:\n",
        "            coordination_bonus += 1.0\n",
        "        \n",
        "        # Bonus for balanced contributions (avoid one agent dominating)\n",
        "        rewards = [contrib[\"reward\"] for contrib in agent_contributions.values()]\n",
        "        if len(rewards) > 1:\n",
        "            reward_std = np.std(rewards)\n",
        "            if reward_std < 2.0:  # Balanced contributions\n",
        "                coordination_bonus += 1.0\n",
        "        \n",
        "        return coordination_bonus\n",
        "    \n",
        "    def get_optimization_summary(self):\n",
        "        \"\"\"Get summary of multi-agent optimization.\"\"\"\n",
        "        \n",
        "        summary = {\n",
        "            \"total_episodes\": len(self.collaborative_rewards),\n",
        "            \"average_collaborative_reward\": np.mean(self.collaborative_rewards) if self.collaborative_rewards else 0,\n",
        "            \"best_episode_reward\": max(self.collaborative_rewards) if self.collaborative_rewards else 0,\n",
        "            \"agent_statistics\": {}\n",
        "        }\n",
        "        \n",
        "        # Get statistics for each agent\n",
        "        for agent_name, agent in self.agents.items():\n",
        "            if isinstance(agent, QLearningAgent):\n",
        "                agent_stats = agent.get_training_stats()\n",
        "            else:  # PolicyGradientAgent\n",
        "                agent_stats = agent.get_policy_summary()\n",
        "            \n",
        "            summary[\"agent_statistics\"][agent_name] = agent_stats\n",
        "        \n",
        "        # Coordination effectiveness\n",
        "        if self.coordination_history:\n",
        "            avg_coordination = np.mean([\n",
        "                np.mean([contrib[\"performance\"] for contrib in episode.values()]) \n",
        "                for episode in self.coordination_history\n",
        "            ])\n",
        "            summary[\"average_coordination_performance\"] = avg_coordination\n",
        "        \n",
        "        return summary\n",
        "\n",
        "# Test multi-agent RL optimization\n",
        "print_step(\"Testing Multi-Agent RL Optimization\")\n",
        "\n",
        "multi_agent_optimizer = MultiAgentRLOptimizer(\n",
        "    base_program=simple_qa,\n",
        "    train_examples=train_data,\n",
        "    eval_examples=eval_data,\n",
        "    metric_func=qa_metric\n",
        ")\n",
        "\n",
        "# Run collaborative optimization\n",
        "multi_agent_optimizer.optimize_collaboratively(num_episodes=20)\n",
        "\n",
        "# Get optimization summary\n",
        "optimization_summary = multi_agent_optimizer.get_optimization_summary()\n",
        "\n",
        "print_step(\"Multi-Agent Optimization Summary\")\n",
        "print(f\"  Total Episodes: {optimization_summary['total_episodes']}\")\n",
        "print(f\"  Average Collaborative Reward: {optimization_summary['average_collaborative_reward']:.3f}\")\n",
        "print(f\"  Best Episode Reward: {optimization_summary['best_episode_reward']:.3f}\")\n",
        "\n",
        "if \"average_coordination_performance\" in optimization_summary:\n",
        "    print(f\"  Average Coordination Performance: {optimization_summary['average_coordination_performance']:.3f}\")\n",
        "\n",
        "print_step(\"Individual Agent Performance\")\n",
        "for agent_name, stats in optimization_summary[\"agent_statistics\"].items():\n",
        "    print(f\"  {agent_name}:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"    {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e296a79d",
      "metadata": {},
      "source": [
        "## RL-Based Hyperparameter Optimization\n",
        "\n",
        "Use RL to optimize DSPy hyperparameters automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9277116b",
      "metadata": {},
      "outputs": [],
      "source": [
        "class RLHyperparameterOptimizer:\n",
        "    \"\"\"RL-based hyperparameter optimization for DSPy programs.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_program_class, train_examples, eval_examples, metric_func):\n",
        "        self.base_program_class = base_program_class\n",
        "        self.train_examples = train_examples\n",
        "        self.eval_examples = eval_examples\n",
        "        self.metric_func = metric_func\n",
        "        \n",
        "        # Hyperparameter space\n",
        "        self.hyperparameter_space = {\n",
        "            \"temperature\": [0.0, 0.3, 0.7, 1.0],\n",
        "            \"max_tokens\": [100, 300, 500, 1000],\n",
        "            \"num_examples\": [1, 3, 5, 8],\n",
        "            \"optimization_method\": [\"basic\", \"bootstrap\", \"ensemble\"]\n",
        "        }\n",
        "        \n",
        "        # RL agent for hyperparameter selection\n",
        "        self.hp_agent = QLearningAgent(\n",
        "            state_size=4,  # Current performance, episode number, recent trend, complexity\n",
        "            action_size=len(self._enumerate_hyperparameter_combinations()),\n",
        "            learning_rate=0.15,\n",
        "            epsilon=0.8,\n",
        "            epsilon_decay=0.99\n",
        "        )\n",
        "        \n",
        "        # Optimization history\n",
        "        self.optimization_history = []\n",
        "        self.best_hyperparameters = None\n",
        "        self.best_performance = 0.0\n",
        "    \n",
        "    def _enumerate_hyperparameter_combinations(self):\n",
        "        \"\"\"Enumerate all possible hyperparameter combinations.\"\"\"\n",
        "        \n",
        "        combinations = []\n",
        "        \n",
        "        for temp in self.hyperparameter_space[\"temperature\"]:\n",
        "            for tokens in self.hyperparameter_space[\"max_tokens\"]:\n",
        "                for examples in self.hyperparameter_space[\"num_examples\"]:\n",
        "                    for method in self.hyperparameter_space[\"optimization_method\"]:\n",
        "                        combinations.append({\n",
        "                            \"temperature\": temp,\n",
        "                            \"max_tokens\": tokens,\n",
        "                            \"num_examples\": examples,\n",
        "                            \"optimization_method\": method\n",
        "                        })\n",
        "        \n",
        "        return combinations\n",
        "    \n",
        "    def _evaluate_hyperparameters(self, hyperparameters):\n",
        "        \"\"\"Evaluate a set of hyperparameters.\"\"\"\n",
        "        \n",
        "        try:\n",
        "            # Create program with hyperparameters\n",
        "            program = self.base_program_class()\n",
        "            \n",
        "            # Simulate applying hyperparameters\n",
        "            # In practice, you would configure the actual DSPy program\n",
        "            performance_modifier = 1.0\n",
        "            \n",
        "            # Temperature effect\n",
        "            if hyperparameters[\"temperature\"] < 0.3:\n",
        "                performance_modifier *= 0.95  # Conservative\n",
        "            elif hyperparameters[\"temperature\"] > 0.7:\n",
        "                performance_modifier *= 0.9   # Too random\n",
        "            \n",
        "            # Token limit effect\n",
        "            if hyperparameters[\"max_tokens\"] < 200:\n",
        "                performance_modifier *= 0.8   # Too restrictive\n",
        "            elif hyperparameters[\"max_tokens\"] > 800:\n",
        "                performance_modifier *= 0.95  # Diminishing returns\n",
        "            \n",
        "            # Example count effect\n",
        "            if hyperparameters[\"num_examples\"] >= 5:\n",
        "                performance_modifier *= 1.1   # Good few-shot learning\n",
        "            elif hyperparameters[\"num_examples\"] <= 1:\n",
        "                performance_modifier *= 0.85  # Limited context\n",
        "            \n",
        "            # Optimization method effect\n",
        "            if hyperparameters[\"optimization_method\"] == \"ensemble\":\n",
        "                performance_modifier *= 1.15\n",
        "            elif hyperparameters[\"optimization_method\"] == \"bootstrap\":\n",
        "                performance_modifier *= 1.05\n",
        "            \n",
        "            # Evaluate on test data\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            \n",
        "            for example in self.eval_examples[:5]:  # Use subset for speed\n",
        "                try:\n",
        "                    # Simulate prediction with hyperparameters\n",
        "                    # In practice, this would use the configured program\n",
        "                    base_prediction = program(**example.inputs())\n",
        "                    \n",
        "                    # Apply performance modifier\n",
        "                    if random.random() < performance_modifier:\n",
        "                        prediction_correct = self.metric_func(example, base_prediction)\n",
        "                    else:\n",
        "                        prediction_correct = False\n",
        "                    \n",
        "                    if prediction_correct:\n",
        "                        correct += 1\n",
        "                    total += 1\n",
        "                    \n",
        "                except:\n",
        "                    total += 1\n",
        "            \n",
        "            performance = correct / total if total > 0 else 0.0\n",
        "            \n",
        "            return performance * performance_modifier\n",
        "            \n",
        "        except Exception as e:\n",
        "            print_error(f\"Hyperparameter evaluation failed: {e}\")\n",
        "            return 0.0\n",
        "    \n",
        "    def optimize_hyperparameters(self, num_episodes=50):\n",
        "        \"\"\"Optimize hyperparameters using RL.\"\"\"\n",
        "        \n",
        "        print_step(\"RL Hyperparameter Optimization\", f\"Optimizing for {num_episodes} episodes\")\n",
        "        \n",
        "        hyperparameter_combinations = self._enumerate_hyperparameter_combinations()\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            # Current state (simplified)\n",
        "            current_performance = self.best_performance\n",
        "            episode_progress = episode / num_episodes\n",
        "            recent_trend = self._calculate_recent_trend()\n",
        "            complexity_estimate = 0.5  # Simplified\n",
        "            \n",
        "            state = {\n",
        "                \"performance_score\": current_performance,\n",
        "                \"steps_taken\": episode,\n",
        "                \"convergence_trend\": recent_trend,\n",
        "                \"program_complexity\": complexity_estimate\n",
        "            }\n",
        "            \n",
        "            # Choose hyperparameter combination\n",
        "            action = self.hp_agent.choose_action(state)\n",
        "            \n",
        "            if action < len(hyperparameter_combinations):\n",
        "                hyperparameters = hyperparameter_combinations[action]\n",
        "            else:\n",
        "                # Fallback to random combination\n",
        "                hyperparameters = random.choice(hyperparameter_combinations)\n",
        "            \n",
        "            # Evaluate hyperparameters\n",
        "            performance = self._evaluate_hyperparameters(hyperparameters)\n",
        "            \n",
        "            # Calculate reward\n",
        "            reward = self._calculate_hyperparameter_reward(performance, hyperparameters)\n",
        "            \n",
        "            # Update best performance\n",
        "            if performance > self.best_performance:\n",
        "                self.best_performance = performance\n",
        "                self.best_hyperparameters = hyperparameters.copy()\n",
        "            \n",
        "            # Record history\n",
        "            self.optimization_history.append({\n",
        "                \"episode\": episode,\n",
        "                \"hyperparameters\": hyperparameters,\n",
        "                \"performance\": performance,\n",
        "                \"reward\": reward,\n",
        "                \"is_best\": performance == self.best_performance\n",
        "            })\n",
        "            \n",
        "            # Learn from experience (simplified next state)\n",
        "            next_state = {\n",
        "                \"performance_score\": max(current_performance, performance),\n",
        "                \"steps_taken\": episode + 1,\n",
        "                \"convergence_trend\": self._calculate_recent_trend(),\n",
        "                \"program_complexity\": complexity_estimate\n",
        "            }\n",
        "            \n",
        "            done = episode == num_episodes - 1\n",
        "            self.hp_agent.learn(state, action, reward, next_state, done)\n",
        "            \n",
        "            # Print progress\n",
        "            if episode % 10 == 0:\n",
        "                print(f\"Episode {episode}: Performance {performance:.3f}, Best {self.best_performance:.3f}\")\n",
        "                print(f\"  Hyperparameters: {hyperparameters}\")\n",
        "        \n",
        "        print_result(f\"Hyperparameter optimization completed!\")\n",
        "        print_result(f\"Best performance: {self.best_performance:.3f}\")\n",
        "        print_result(f\"Best hyperparameters: {self.best_hyperparameters}\")\n",
        "    \n",
        "    def _calculate_recent_trend(self):\n",
        "        \"\"\"Calculate recent performance trend.\"\"\"\n",
        "        if len(self.optimization_history) < 3:\n",
        "            return 0.0\n",
        "        \n",
        "        recent_performances = [h[\"performance\"] for h in self.optimization_history[-3:]]\n",
        "        return (recent_performances[-1] - recent_performances[0]) / 3\n",
        "    \n",
        "    def _calculate_hyperparameter_reward(self, performance, hyperparameters):\n",
        "        \"\"\"Calculate reward for hyperparameter choice.\"\"\"\n",
        "        \n",
        "        # Base reward from performance\n",
        "        reward = performance * 10\n",
        "        \n",
        "        # Bonus for improvement\n",
        "        if performance > self.best_performance:\n",
        "            improvement = performance - self.best_performance\n",
        "            reward += improvement * 20  # Bonus for improvement\n",
        "        \n",
        "        # Efficiency bonus for good configurations\n",
        "        if (hyperparameters[\"temperature\"] == 0.3 and \n",
        "            hyperparameters[\"num_examples\"] >= 3 and\n",
        "            hyperparameters[\"optimization_method\"] in [\"bootstrap\", \"ensemble\"]):\n",
        "            reward += 2.0\n",
        "        \n",
        "        # Penalty for extreme configurations\n",
        "        if hyperparameters[\"temperature\"] > 0.8 or hyperparameters[\"max_tokens\"] > 900:\n",
        "            reward -= 1.0\n",
        "        \n",
        "        return reward\n",
        "    \n",
        "    def get_optimization_report(self):\n",
        "        \"\"\"Get comprehensive optimization report.\"\"\"\n",
        "        \n",
        "        if not self.optimization_history:\n",
        "            return {\"error\": \"No optimization history available\"}\n",
        "        \n",
        "        performances = [h[\"performance\"] for h in self.optimization_history]\n",
        "        \n",
        "        report = {\n",
        "            \"total_episodes\": len(self.optimization_history),\n",
        "            \"best_performance\": self.best_performance,\n",
        "            \"best_hyperparameters\": self.best_hyperparameters,\n",
        "            \"average_performance\": np.mean(performances),\n",
        "            \"performance_std\": np.std(performances),\n",
        "            \"improvement_episodes\": len([h for h in self.optimization_history if h[\"is_best\"]]),\n",
        "            \"final_epsilon\": self.hp_agent.epsilon,\n",
        "            \"hyperparameter_analysis\": self._analyze_hyperparameter_importance()\n",
        "        }\n",
        "        \n",
        "        return report\n",
        "    \n",
        "    def _analyze_hyperparameter_importance(self):\n",
        "        \"\"\"Analyze which hyperparameters had the most impact.\"\"\"\n",
        "        \n",
        "        analysis = {}\n",
        "        \n",
        "        for param_name in self.hyperparameter_space.keys():\n",
        "            param_performance = defaultdict(list)\n",
        "            \n",
        "            for history_item in self.optimization_history:\n",
        "                param_value = history_item[\"hyperparameters\"][param_name]\n",
        "                param_performance[param_value].append(history_item[\"performance\"])\n",
        "            \n",
        "            # Calculate average performance for each parameter value\n",
        "            param_averages = {\n",
        "                value: np.mean(performances) \n",
        "                for value, performances in param_performance.items()\n",
        "            }\n",
        "            \n",
        "            analysis[param_name] = param_averages\n",
        "        \n",
        "        return analysis\n",
        "\n",
        "# Test RL hyperparameter optimization\n",
        "print_step(\"Testing RL Hyperparameter Optimization\")\n",
        "\n",
        "hp_optimizer = RLHyperparameterOptimizer(\n",
        "    base_program_class=SimpleQA,\n",
        "    train_examples=train_data,\n",
        "    eval_examples=eval_data,\n",
        "    metric_func=qa_metric\n",
        ")\n",
        "\n",
        "# Run hyperparameter optimization\n",
        "hp_optimizer.optimize_hyperparameters(num_episodes=30)\n",
        "\n",
        "# Get optimization report\n",
        "hp_report = hp_optimizer.get_optimization_report()\n",
        "\n",
        "print_step(\"Hyperparameter Optimization Report\")\n",
        "print(f\"  Best Performance: {hp_report['best_performance']:.3f}\")\n",
        "print(f\"  Best Hyperparameters: {hp_report['best_hyperparameters']}\")\n",
        "print(f\"  Average Performance: {hp_report['average_performance']:.3f}\")\n",
        "print(f\"  Improvement Episodes: {hp_report['improvement_episodes']}\")\n",
        "\n",
        "print_step(\"Hyperparameter Importance Analysis\")\n",
        "for param_name, param_analysis in hp_report[\"hyperparameter_analysis\"].items():\n",
        "    print(f\"  {param_name}:\")\n",
        "    sorted_values = sorted(param_analysis.items(), key=lambda x: x[1], reverse=True)\n",
        "    for value, avg_performance in sorted_values[:2]:  # Top 2 values\n",
        "        print(f\"    {value}: {avg_performance:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1caa4b16",
      "metadata": {},
      "source": [
        "## Best Practices for RL Optimization in DSPy\n",
        "\n",
        "### Key Principles:\n",
        "\n",
        "1. **Environment Design**: Create meaningful state representations and reward functions\n",
        "2. **Action Space**: Define actions that correspond to actual DSPy optimizations\n",
        "3. **Exploration vs Exploitation**: Balance trying new approaches with using known good ones\n",
        "4. **Multi-Agent Coordination**: Use specialized agents for different optimization aspects\n",
        "5. **Hyperparameter Search**: Apply RL to automatically tune DSPy configurations\n",
        "\n",
        "### RL Algorithm Selection:\n",
        "\n",
        "- **Q-Learning**: Simple and effective for discrete action spaces\n",
        "- **Policy Gradient**: Better for continuous or complex action spaces\n",
        "- **Actor-Critic**: Combines benefits of value-based and policy-based methods\n",
        "- **Multi-Agent RL**: Coordinate multiple optimization objectives\n",
        "\n",
        "### Implementation Considerations:\n",
        "\n",
        "- **State Representation**: Include performance metrics, optimization history, and program characteristics\n",
        "- **Reward Engineering**: Design rewards that align with actual DSPy performance goals\n",
        "- **Sample Efficiency**: Use sample-efficient algorithms to minimize LM API calls\n",
        "- **Stability**: Implement safeguards against unstable optimization trajectories\n",
        "\n",
        "### Production Deployment:\n",
        "\n",
        "- **Online Learning**: Continue optimization during deployment\n",
        "- **Safety Constraints**: Ensure RL doesn't degrade critical performance metrics\n",
        "- **Monitoring**: Track RL agent behavior and intervention triggers\n",
        "- **Rollback Mechanisms**: Ability to revert to previous good configurations\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated experimental RL optimization techniques for DSPy:\n",
        "\n",
        "- **RL Environment**: Created environments for optimizing DSPy programs\n",
        "- **Q-Learning**: Implemented value-based RL for discrete optimization choices\n",
        "- **Policy Gradient**: Used policy-based methods for more complex optimization\n",
        "- **Multi-Agent RL**: Coordinated multiple agents for comprehensive optimization\n",
        "- **Hyperparameter Optimization**: Automated tuning of DSPy configurations\n",
        "\n",
        "These experimental techniques show promise for:\n",
        "- Automated optimization of complex DSPy programs\n",
        "- Dynamic adaptation to changing requirements\n",
        "- Multi-objective optimization (accuracy, speed, cost)\n",
        "- Continuous improvement during deployment\n",
        "\n",
        "While still experimental, RL-based optimization could become a powerful tool for automatically improving DSPy programs, especially for complex applications where manual optimization is challenging or time-consuming."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
