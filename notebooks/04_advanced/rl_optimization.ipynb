{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "172ea1ab",
   "metadata": {},
   "source": [
    "# Experimental RL Optimization for DSPy\n",
    "\n",
    "This notebook demonstrates how to use Reinforcement Learning (RL) techniques to optimize DSPy programs and improve their performance through trial-and-error learning.\n",
    "\n",
    "Based on the DSPy tutorial: [Experimental RL Optimization for DSPy](https://dspy.ai/tutorials/rl_ai_program/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c00a5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f912ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import dspy\n",
    "from utils import setup_default_lm, print_step, print_result, print_error\n",
    "from utils.datasets import get_sample_qa_data, get_sample_classification_data\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import json\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecffe5",
   "metadata": {},
   "source": [
    "## Language Model Configuration\n",
    "\n",
    "Set up DSPy with a language model for RL optimization experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step(\"Setting up Language Model\", \"Configuring DSPy for RL optimization\")\n",
    "\n",
    "try:\n",
    "    lm = setup_default_lm(provider=\"openai\", model=\"gpt-3.5-turbo\", max_tokens=1000)\n",
    "    dspy.configure(lm=lm)\n",
    "    print_result(\"Language model configured successfully!\", \"Status\")\n",
    "except Exception as e:\n",
    "    print_error(f\"Failed to configure language model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d6873",
   "metadata": {},
   "source": [
    "## RL Environment for DSPy Programs\n",
    "\n",
    "Create a reinforcement learning environment for optimizing DSPy programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSPyRLEnvironment:\n",
    "    \"\"\"RL Environment for optimizing DSPy programs.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_program, train_examples, eval_examples, metric_func):\n",
    "        self.base_program = base_program\n",
    "        self.train_examples = train_examples\n",
    "        self.eval_examples = eval_examples\n",
    "        self.metric_func = metric_func\n",
    "        \n",
    "        # State representation\n",
    "        self.current_state = self._get_initial_state()\n",
    "        self.episode_rewards = []\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 50\n",
    "        \n",
    "        # Action space - different optimization strategies\n",
    "        self.action_space = {\n",
    "            0: \"adjust_temperature\",\n",
    "            1: \"modify_prompt_structure\", \n",
    "            2: \"change_example_selection\",\n",
    "            3: \"adjust_chain_of_thought\",\n",
    "            4: \"modify_output_format\",\n",
    "            5: \"ensemble_prediction\"\n",
    "        }\n",
    "        \n",
    "        # History tracking\n",
    "        self.performance_history = []\n",
    "        self.action_history = []\n",
    "        \n",
    "    def _get_initial_state(self):\n",
    "        \"\"\"Get initial state representation.\"\"\"\n",
    "        # Evaluate current program performance\n",
    "        performance = self._evaluate_program(self.base_program)\n",
    "        \n",
    "        return {\n",
    "            \"performance_score\": performance,\n",
    "            \"steps_taken\": 0,\n",
    "            \"recent_actions\": [],\n",
    "            \"program_complexity\": self._estimate_complexity(),\n",
    "            \"convergence_trend\": 0.0\n",
    "        }\n",
    "    \n",
    "    def _evaluate_program(self, program):\n",
    "        \"\"\"Evaluate program performance on evaluation examples.\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for example in self.eval_examples[:10]:  # Use subset for speed\n",
    "            try:\n",
    "                prediction = program(**example.inputs())\n",
    "                if self.metric_func(example, prediction):\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "            except:\n",
    "                total += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0\n",
    "    \n",
    "    def _estimate_complexity(self):\n",
    "        \"\"\"Estimate program complexity (simplified).\"\"\"\n",
    "        # In a real implementation, this would analyze the program structure\n",
    "        return random.uniform(0.3, 0.7)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment for new episode.\"\"\"\n",
    "        self.current_state = self._get_initial_state()\n",
    "        self.step_count = 0\n",
    "        self.episode_rewards = []\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return new state, reward, done.\"\"\"\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Apply action to modify program\n",
    "        modified_program, action_success = self._apply_action(action)\n",
    "        \n",
    "        # Evaluate modified program\n",
    "        new_performance = self._evaluate_program(modified_program)\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward(new_performance, action, action_success)\n",
    "        \n",
    "        # Update state\n",
    "        self.current_state = self._update_state(new_performance, action)\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = (self.step_count >= self.max_steps or \n",
    "                new_performance > 0.95 or  # Excellent performance achieved\n",
    "                len(self.episode_rewards) > 10 and all(r < 0 for r in self.episode_rewards[-5:]))  # Consistent poor performance\n",
    "        \n",
    "        # Record history\n",
    "        self.performance_history.append(new_performance)\n",
    "        self.action_history.append(action)\n",
    "        self.episode_rewards.append(reward)\n",
    "        \n",
    "        return self.current_state, reward, done, {\"performance\": new_performance, \"action_success\": action_success}\n",
    "    \n",
    "    def _apply_action(self, action):\n",
    "        \"\"\"Apply an action to modify the DSPy program.\"\"\"\n",
    "        \n",
    "        action_name = self.action_space[action]\n",
    "        \n",
    "        try:\n",
    "            if action_name == \"adjust_temperature\":\n",
    "                # Simulate adjusting model temperature\n",
    "                modified_program = self._modify_temperature()\n",
    "                return modified_program, True\n",
    "                \n",
    "            elif action_name == \"modify_prompt_structure\":\n",
    "                # Simulate prompt engineering\n",
    "                modified_program = self._modify_prompts()\n",
    "                return modified_program, True\n",
    "                \n",
    "            elif action_name == \"change_example_selection\":\n",
    "                # Simulate changing few-shot examples\n",
    "                modified_program = self._change_examples()\n",
    "                return modified_program, True\n",
    "                \n",
    "            elif action_name == \"adjust_chain_of_thought\":\n",
    "                # Simulate CoT modifications\n",
    "                modified_program = self._adjust_cot()\n",
    "                return modified_program, True\n",
    "                \n",
    "            elif action_name == \"modify_output_format\":\n",
    "                # Simulate output format changes\n",
    "                modified_program = self._modify_output_format()\n",
    "                return modified_program, True\n",
    "                \n",
    "            elif action_name == \"ensemble_prediction\":\n",
    "                # Simulate ensemble methods\n",
    "                modified_program = self._create_ensemble()\n",
    "                return modified_program, True\n",
    "                \n",
    "            else:\n",
    "                return self.base_program, False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print_error(f\"Action failed: {e}\")\n",
    "            return self.base_program, False\n",
    "    \n",
    "    def _modify_temperature(self):\n",
    "        \"\"\"Simulate temperature adjustment.\"\"\"\n",
    "        # In practice, this would modify the actual LM configuration\n",
    "        class ModifiedProgram:\n",
    "            def __init__(self, base):\n",
    "                self.base = base\n",
    "                self.temperature_adjustment = random.uniform(-0.2, 0.2)\n",
    "            \n",
    "            def __call__(self, **kwargs):\n",
    "                # Simulate effect of temperature change\n",
    "                result = self.base(**kwargs)\n",
    "                # Add some randomness to simulate temperature effect\n",
    "                if hasattr(result, 'confidence'):\n",
    "                    result.confidence = max(0.1, min(1.0, result.confidence + self.temperature_adjustment))\n",
    "                return result\n",
    "        \n",
    "        return ModifiedProgram(self.base_program)\n",
    "    \n",
    "    def _modify_prompts(self):\n",
    "        \"\"\"Simulate prompt modifications.\"\"\"\n",
    "        class ModifiedProgram:\n",
    "            def __init__(self, base):\n",
    "                self.base = base\n",
    "                self.prompt_modification = random.choice([\"more_detailed\", \"concise\", \"structured\"])\n",
    "            \n",
    "            def __call__(self, **kwargs):\n",
    "                # Simulate prompt engineering effect\n",
    "                result = self.base(**kwargs)\n",
    "                # Randomly modify performance based on prompt changes\n",
    "                performance_modifier = random.uniform(0.9, 1.1)\n",
    "                return result\n",
    "        \n",
    "        return ModifiedProgram(self.base_program)\n",
    "    \n",
    "    def _change_examples(self):\n",
    "        \"\"\"Simulate changing few-shot examples.\"\"\"\n",
    "        class ModifiedProgram:\n",
    "            def __init__(self, base):\n",
    "                self.base = base\n",
    "                self.example_selection = random.choice([\"diverse\", \"similar\", \"hard_cases\"])\n",
    "            \n",
    "            def __call__(self, **kwargs):\n",
    "                return self.base(**kwargs)\n",
    "        \n",
    "        return ModifiedProgram(self.base_program)\n",
    "    \n",
    "    def _adjust_cot(self):\n",
    "        \"\"\"Simulate Chain of Thought adjustments.\"\"\"\n",
    "        class ModifiedProgram:\n",
    "            def __init__(self, base):\n",
    "                self.base = base\n",
    "                self.cot_style = random.choice([\"step_by_step\", \"reasoning_first\", \"conclusion_first\"])\n",
    "            \n",
    "            def __call__(self, **kwargs):\n",
    "                return self.base(**kwargs)\n",
    "        \n",
    "        return ModifiedProgram(self.base_program)\n",
    "    \n",
    "    def _modify_output_format(self):\n",
    "        \"\"\"Simulate output format modifications.\"\"\"\n",
    "        class ModifiedProgram:\n",
    "            def __init__(self, base):\n",
    "                self.base = base\n",
    "                self.output_format = random.choice([\"structured\", \"free_form\", \"json\", \"bullet_points\"])\n",
    "            \n",
    "            def __call__(self, **kwargs):\n",
    "                return self.base(**kwargs)\n",
    "        \n",
    "        return ModifiedProgram(self.base_program)\n",
    "    \n",
    "    def _create_ensemble(self):\n",
    "        \"\"\"Simulate ensemble creation.\"\"\"\n",
    "        class ModifiedProgram:\n",
    "            def __init__(self, base):\n",
    "                self.base = base\n",
    "                self.ensemble_size = random.randint(2, 4)\n",
    "            \n",
    "            def __call__(self, **kwargs):\n",
    "                # Simulate ensemble voting\n",
    "                return self.base(**kwargs)\n",
    "        \n",
    "        return ModifiedProgram(self.base_program)\n",
    "    \n",
    "    def _calculate_reward(self, new_performance, action, action_success):\n",
    "        \"\"\"Calculate reward for the action taken.\"\"\"\n",
    "        \n",
    "        if not action_success:\n",
    "            return -0.5  # Penalty for failed actions\n",
    "        \n",
    "        # Performance improvement reward\n",
    "        if self.performance_history:\n",
    "            performance_delta = new_performance - self.performance_history[-1]\n",
    "            reward = performance_delta * 10  # Scale the reward\n",
    "        else:\n",
    "            reward = new_performance * 5  # Initial performance reward\n",
    "        \n",
    "        # Bonus for high absolute performance\n",
    "        if new_performance > 0.8:\n",
    "            reward += 2.0\n",
    "        elif new_performance > 0.6:\n",
    "            reward += 1.0\n",
    "        \n",
    "        # Penalty for low performance\n",
    "        if new_performance < 0.3:\n",
    "            reward -= 1.0\n",
    "        \n",
    "        # Efficiency bonus (fewer steps to good performance)\n",
    "        if new_performance > 0.7 and self.step_count < 10:\n",
    "            reward += 1.0\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _update_state(self, new_performance, action):\n",
    "        \"\"\"Update state based on new performance and action.\"\"\"\n",
    "        \n",
    "        # Calculate convergence trend\n",
    "        if len(self.performance_history) >= 3:\n",
    "            recent_perf = self.performance_history[-3:]\n",
    "            trend = (recent_perf[-1] - recent_perf[0]) / 3\n",
    "        else:\n",
    "            trend = 0.0\n",
    "        \n",
    "        return {\n",
    "            \"performance_score\": new_performance,\n",
    "            \"steps_taken\": self.step_count,\n",
    "            \"recent_actions\": self.action_history[-5:],  # Last 5 actions\n",
    "            \"program_complexity\": self.current_state[\"program_complexity\"],\n",
    "            \"convergence_trend\": trend\n",
    "        }\n",
    "\n",
    "# Test the RL environment\n",
    "print_step(\"Testing RL Environment\")\n",
    "\n",
    "# Create a simple test program\n",
    "class SimpleQA(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(\"question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        return self.generate_answer(question=question)\n",
    "\n",
    "# Simple metric\n",
    "def qa_metric(example, prediction, trace=None):\n",
    "    return example.answer.lower() in prediction.answer.lower()\n",
    "\n",
    "# Get test data\n",
    "qa_data = get_sample_qa_data()\n",
    "train_data = qa_data[:3]\n",
    "eval_data = qa_data[3:]\n",
    "\n",
    "# Initialize environment\n",
    "simple_qa = SimpleQA()\n",
    "rl_env = DSPyRLEnvironment(simple_qa, train_data, eval_data, qa_metric)\n",
    "\n",
    "# Test environment\n",
    "initial_state = rl_env.reset()\n",
    "print_result(f\"Initial state: {initial_state}\")\n",
    "\n",
    "# Take a few test steps\n",
    "for i in range(3):\n",
    "    action = random.randint(0, 5)\n",
    "    state, reward, done, info = rl_env.step(action)\n",
    "    print(f\"Step {i+1}: Action {action}, Reward {reward:.3f}, Performance {info['performance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c99d5",
   "metadata": {},
   "source": [
    "## Q-Learning Agent for DSPy Optimization\n",
    "\n",
    "Implement a Q-Learning agent to optimize DSPy programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent for optimizing DSPy programs.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Q-table (simplified state representation)\n",
    "        self.q_table = defaultdict(lambda: np.zeros(action_size))\n",
    "        \n",
    "        # Training statistics\n",
    "        self.training_rewards = []\n",
    "        self.training_performance = []\n",
    "        self.episode_count = 0\n",
    "    \n",
    "    def _state_to_key(self, state):\n",
    "        \"\"\"Convert state dict to a key for Q-table.\"\"\"\n",
    "        # Discretize continuous values for Q-table\n",
    "        perf_bucket = int(state[\"performance_score\"] * 10)  # 0-10\n",
    "        steps_bucket = min(state[\"steps_taken\"] // 5, 10)   # 0-10\n",
    "        trend_bucket = int((state[\"convergence_trend\"] + 1) * 5)  # 0-10\n",
    "        \n",
    "        return (perf_bucket, steps_bucket, trend_bucket)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n",
    "        state_key = self._state_to_key(state)\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            # Exploration\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            # Exploitation\n",
    "            return np.argmax(self.q_table[state_key])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-values using Q-learning update rule.\"\"\"\n",
    "        \n",
    "        state_key = self._state_to_key(state)\n",
    "        next_state_key = self._state_to_key(next_state)\n",
    "        \n",
    "        # Q-learning update\n",
    "        current_q = self.q_table[state_key][action]\n",
    "        \n",
    "        if done:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            target_q = reward + self.discount_factor * np.max(self.q_table[next_state_key])\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.q_table[state_key][action] = current_q + self.learning_rate * (target_q - current_q)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def train(self, environment, num_episodes=100):\n",
    "        \"\"\"Train the agent in the given environment.\"\"\"\n",
    "        \n",
    "        print_step(\"Q-Learning Training\", f\"Training for {num_episodes} episodes\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = environment.reset()\n",
    "            total_reward = 0\n",
    "            step_count = 0\n",
    "            \n",
    "            while True:\n",
    "                # Choose action\n",
    "                action = self.choose_action(state)\n",
    "                \n",
    "                # Take action\n",
    "                next_state, reward, done, info = environment.step(action)\n",
    "                \n",
    "                # Learn from experience\n",
    "                self.learn(state, action, reward, next_state, done)\n",
    "                \n",
    "                # Update tracking\n",
    "                total_reward += reward\n",
    "                step_count += 1\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Record episode statistics\n",
    "            self.training_rewards.append(total_reward)\n",
    "            final_performance = info.get(\"performance\", 0.0)\n",
    "            self.training_performance.append(final_performance)\n",
    "            self.episode_count += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if episode % 20 == 0:\n",
    "                avg_reward = np.mean(self.training_rewards[-20:]) if len(self.training_rewards) >= 20 else total_reward\n",
    "                avg_performance = np.mean(self.training_performance[-20:]) if len(self.training_performance) >= 20 else final_performance\n",
    "                print(f\"Episode {episode}: Avg Reward: {avg_reward:.3f}, Avg Performance: {avg_performance:.3f}, Epsilon: {self.epsilon:.3f}\")\n",
    "        \n",
    "        print_result(f\"Training completed! Final epsilon: {self.epsilon:.3f}\")\n",
    "    \n",
    "    def get_training_stats(self):\n",
    "        \"\"\"Get training statistics.\"\"\"\n",
    "        return {\n",
    "            \"total_episodes\": self.episode_count,\n",
    "            \"final_epsilon\": self.epsilon,\n",
    "            \"average_reward\": np.mean(self.training_rewards) if self.training_rewards else 0,\n",
    "            \"best_performance\": max(self.training_performance) if self.training_performance else 0,\n",
    "            \"final_performance\": self.training_performance[-1] if self.training_performance else 0,\n",
    "            \"q_table_size\": len(self.q_table)\n",
    "        }\n",
    "\n",
    "# Initialize and train Q-learning agent\n",
    "print_step(\"Initializing Q-Learning Agent\")\n",
    "\n",
    "agent = QLearningAgent(\n",
    "    state_size=4,  # Simplified state representation\n",
    "    action_size=6,  # Number of available actions\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.95,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "agent.train(rl_env, num_episodes=50)\n",
    "\n",
    "# Get training statistics\n",
    "stats = agent.get_training_stats()\n",
    "print_step(\"Training Statistics\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d0562",
   "metadata": {},
   "source": [
    "## Advanced RL Optimization with Policy Gradient\n",
    "\n",
    "Implement a more sophisticated Policy Gradient method for DSPy optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3074535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    \"\"\"Policy Gradient agent for DSPy optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, learning_rate=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Policy parameters (simplified linear policy)\n",
    "        self.policy_weights = np.random.randn(state_size, action_size) * 0.1\n",
    "        \n",
    "        # Episode memory\n",
    "        self.episode_states = []\n",
    "        self.episode_actions = []\n",
    "        self.episode_rewards = []\n",
    "        \n",
    "        # Training statistics\n",
    "        self.episode_returns = []\n",
    "        self.policy_losses = []\n",
    "    \n",
    "    def _state_to_vector(self, state):\n",
    "        \"\"\"Convert state dict to feature vector.\"\"\"\n",
    "        return np.array([\n",
    "            state[\"performance_score\"],\n",
    "            state[\"steps_taken\"] / 50.0,  # Normalize\n",
    "            state[\"convergence_trend\"],\n",
    "            len(state[\"recent_actions\"]) / 5.0  # Normalize\n",
    "        ])\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        \"\"\"Softmax activation function.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose action based on policy probabilities.\"\"\"\n",
    "        state_vector = self._state_to_vector(state)\n",
    "        \n",
    "        # Compute action probabilities\n",
    "        logits = np.dot(state_vector, self.policy_weights)\n",
    "        action_probs = self._softmax(logits)\n",
    "        \n",
    "        # Sample action from probability distribution\n",
    "        action = np.random.choice(self.action_size, p=action_probs)\n",
    "        \n",
    "        return action, action_probs[action]\n",
    "    \n",
    "    def store_transition(self, state, action, reward):\n",
    "        \"\"\"Store transition for later learning.\"\"\"\n",
    "        state_vector = self._state_to_vector(state)\n",
    "        \n",
    "        self.episode_states.append(state_vector)\n",
    "        self.episode_actions.append(action)\n",
    "        self.episode_rewards.append(reward)\n",
    "    \n",
    "    def learn_from_episode(self):\n",
    "        \"\"\"Learn from completed episode using policy gradient.\"\"\"\n",
    "        \n",
    "        if not self.episode_rewards:\n",
    "            return\n",
    "        \n",
    "        # Calculate discounted returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(self.episode_rewards):\n",
    "            G = reward + 0.99 * G  # Discount factor = 0.99\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        # Normalize returns\n",
    "        returns = np.array(returns)\n",
    "        if len(returns) > 1:\n",
    "            returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-8)\n",
    "        \n",
    "        # Policy gradient update\n",
    "        for i in range(len(self.episode_states)):\n",
    "            state = self.episode_states[i]\n",
    "            action = self.episode_actions[i]\n",
    "            G = returns[i]\n",
    "            \n",
    "            # Compute gradient\n",
    "            logits = np.dot(state, self.policy_weights)\n",
    "            action_probs = self._softmax(logits)\n",
    "            \n",
    "            # Policy gradient\n",
    "            grad = np.zeros_like(self.policy_weights)\n",
    "            for a in range(self.action_size):\n",
    "                if a == action:\n",
    "                    grad[:, a] = state * (1 - action_probs[a]) * G\n",
    "                else:\n",
    "                    grad[:, a] = -state * action_probs[a] * G\n",
    "            \n",
    "            # Update policy weights\n",
    "            self.policy_weights += self.learning_rate * grad\n",
    "        \n",
    "        # Record episode return\n",
    "        episode_return = sum(self.episode_rewards)\n",
    "        self.episode_returns.append(episode_return)\n",
    "        \n",
    "        # Clear episode memory\n",
    "        self.episode_states = []\n",
    "        self.episode_actions = []\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def train(self, environment, num_episodes=100):\n",
    "        \"\"\"Train the policy gradient agent.\"\"\"\n",
    "        \n",
    "        print_step(\"Policy Gradient Training\", f\"Training for {num_episodes} episodes\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = environment.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while True:\n",
    "                # Choose action\n",
    "                action, action_prob = self.choose_action(state)\n",
    "                \n",
    "                # Take action\n",
    "                next_state, reward, done, info = environment.step(action)\n",
    "                \n",
    "                # Store transition\n",
    "                self.store_transition(state, action, reward)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Learn from episode\n",
    "            self.learn_from_episode()\n",
    "            \n",
    "            # Print progress\n",
    "            if episode % 20 == 0:\n",
    "                avg_return = np.mean(self.episode_returns[-20:]) if len(self.episode_returns) >= 20 else episode_reward\n",
    "                avg_performance = np.mean([info.get(\"performance\", 0) for _ in range(min(20, len(self.episode_returns)))])\n",
    "                print(f\"Episode {episode}: Avg Return: {avg_return:.3f}, Recent Performance: {info.get('performance', 0):.3f}\")\n",
    "        \n",
    "        print_result(\"Policy Gradient training completed!\")\n",
    "    \n",
    "    def get_policy_summary(self):\n",
    "        \"\"\"Get summary of learned policy.\"\"\"\n",
    "        return {\n",
    "            \"policy_weights_shape\": self.policy_weights.shape,\n",
    "            \"total_episodes\": len(self.episode_returns),\n",
    "            \"average_return\": np.mean(self.episode_returns) if self.episode_returns else 0,\n",
    "            \"best_return\": max(self.episode_returns) if self.episode_returns else 0,\n",
    "            \"policy_weights_norm\": np.linalg.norm(self.policy_weights)\n",
    "        }\n",
    "\n",
    "# Test Policy Gradient agent\n",
    "print_step(\"Testing Policy Gradient Agent\")\n",
    "\n",
    "pg_agent = PolicyGradientAgent(\n",
    "    state_size=4,\n",
    "    action_size=6,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# Create fresh environment for PG training\n",
    "rl_env_pg = DSPyRLEnvironment(simple_qa, train_data, eval_data, qa_metric)\n",
    "\n",
    "# Train policy gradient agent\n",
    "pg_agent.train(rl_env_pg, num_episodes=50)\n",
    "\n",
    "# Get policy summary\n",
    "pg_stats = pg_agent.get_policy_summary()\n",
    "print_step(\"Policy Gradient Statistics\")\n",
    "for key, value in pg_stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5893ff",
   "metadata": {},
   "source": [
    "## Multi-Agent RL for Complex DSPy Optimization\n",
    "\n",
    "Implement multiple RL agents working together to optimize different aspects of DSPy programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentRLOptimizer:\n",
    "    \"\"\"Multi-agent RL system for comprehensive DSPy optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_program, train_examples, eval_examples, metric_func):\n",
    "        self.base_program = base_program\n",
    "        self.train_examples = train_examples\n",
    "        self.eval_examples = eval_examples\n",
    "        self.metric_func = metric_func\n",
    "        \n",
    "        # Specialized agents for different optimization aspects\n",
    "        self.agents = {\n",
    "            \"prompt_optimizer\": QLearningAgent(state_size=4, action_size=3, learning_rate=0.1),\n",
    "            \"example_selector\": QLearningAgent(state_size=4, action_size=3, learning_rate=0.1),\n",
    "            \"structure_optimizer\": PolicyGradientAgent(state_size=4, action_size=3, learning_rate=0.01)\n",
    "        }\n",
    "        \n",
    "        # Specialized action spaces for each agent\n",
    "        self.agent_actions = {\n",
    "            \"prompt_optimizer\": {\n",
    "                0: \"detailed_prompts\",\n",
    "                1: \"concise_prompts\", \n",
    "                2: \"structured_prompts\"\n",
    "            },\n",
    "            \"example_selector\": {\n",
    "                0: \"diverse_examples\",\n",
    "                1: \"similar_examples\",\n",
    "                2: \"hard_examples\"\n",
    "            },\n",
    "            \"structure_optimizer\": {\n",
    "                0: \"chain_of_thought\",\n",
    "                1: \"direct_prediction\",\n",
    "                2: \"ensemble_approach\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Coordination mechanism\n",
    "        self.coordination_history = []\n",
    "        self.collaborative_rewards = []\n",
    "    \n",
    "    def optimize_collaboratively(self, num_episodes=30):\n",
    "        \"\"\"Optimize using collaborative multi-agent approach.\"\"\"\n",
    "        \n",
    "        print_step(\"Multi-Agent Collaborative Optimization\", f\"Running {num_episodes} episodes\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            print_step(f\"Episode {episode + 1}\")\n",
    "            \n",
    "            # Each agent contributes to the optimization\n",
    "            agent_contributions = {}\n",
    "            total_episode_reward = 0\n",
    "            \n",
    "            for agent_name, agent in self.agents.items():\n",
    "                print(f\"  {agent_name} optimization...\")\n",
    "                \n",
    "                # Create specialized environment for this agent\n",
    "                specialized_env = self._create_specialized_environment(agent_name)\n",
    "                \n",
    "                # Run optimization for this agent\n",
    "                state = specialized_env.reset()\n",
    "                agent_reward = 0\n",
    "                \n",
    "                for step in range(5):  # Limited steps per agent per episode\n",
    "                    if isinstance(agent, QLearningAgent):\n",
    "                        action = agent.choose_action(state)\n",
    "                    else:  # PolicyGradientAgent\n",
    "                        action, _ = agent.choose_action(state)\n",
    "                    \n",
    "                    next_state, reward, done, info = specialized_env.step(action)\n",
    "                    \n",
    "                    # Learn from experience\n",
    "                    if isinstance(agent, QLearningAgent):\n",
    "                        agent.learn(state, action, reward, next_state, done)\n",
    "                    else:\n",
    "                        agent.store_transition(state, action, reward)\n",
    "                    \n",
    "                    agent_reward += reward\n",
    "                    state = next_state\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "                \n",
    "                # For Policy Gradient agents, learn from episode\n",
    "                if isinstance(agent, PolicyGradientAgent):\n",
    "                    agent.learn_from_episode()\n",
    "                \n",
    "                agent_contributions[agent_name] = {\n",
    "                    \"reward\": agent_reward,\n",
    "                    \"performance\": info.get(\"performance\", 0),\n",
    "                    \"actions_taken\": step + 1\n",
    "                }\n",
    "                \n",
    "                total_episode_reward += agent_reward\n",
    "            \n",
    "            # Coordinate agents and share rewards\n",
    "            coordination_bonus = self._calculate_coordination_bonus(agent_contributions)\n",
    "            total_episode_reward += coordination_bonus\n",
    "            \n",
    "            self.collaborative_rewards.append(total_episode_reward)\n",
    "            self.coordination_history.append(agent_contributions)\n",
    "            \n",
    "            # Print episode summary\n",
    "            if episode % 10 == 0:\n",
    "                avg_reward = np.mean(self.collaborative_rewards[-10:]) if len(self.collaborative_rewards) >= 10 else total_episode_reward\n",
    "                print(f\"  Episode {episode}: Avg Collaborative Reward: {avg_reward:.3f}\")\n",
    "                \n",
    "                for agent_name, contrib in agent_contributions.items():\n",
    "                    print(f\"    {agent_name}: Reward {contrib['reward']:.3f}, Performance {contrib['performance']:.3f}\")\n",
    "        \n",
    "        print_result(\"Multi-agent optimization completed!\")\n",
    "    \n",
    "    def _create_specialized_environment(self, agent_name):\n",
    "        \"\"\"Create specialized environment for specific agent.\"\"\"\n",
    "        \n",
    "        # Simplified specialized environments\n",
    "        if agent_name == \"prompt_optimizer\":\n",
    "            # Focus on prompt-related actions\n",
    "            specialized_env = DSPyRLEnvironment(self.base_program, self.train_examples, self.eval_examples, self.metric_func)\n",
    "            specialized_env.action_space = {0: \"detailed_prompts\", 1: \"concise_prompts\", 2: \"structured_prompts\"}\n",
    "        \n",
    "        elif agent_name == \"example_selector\":\n",
    "            # Focus on example selection\n",
    "            specialized_env = DSPyRLEnvironment(self.base_program, self.train_examples, self.eval_examples, self.metric_func)\n",
    "            specialized_env.action_space = {0: \"diverse_examples\", 1: \"similar_examples\", 2: \"hard_examples\"}\n",
    "        \n",
    "        else:  # structure_optimizer\n",
    "            # Focus on program structure\n",
    "            specialized_env = DSPyRLEnvironment(self.base_program, self.train_examples, self.eval_examples, self.metric_func)\n",
    "            specialized_env.action_space = {0: \"chain_of_thought\", 1: \"direct_prediction\", 2: \"ensemble_approach\"}\n",
    "        \n",
    "        return specialized_env\n",
    "    \n",
    "    def _calculate_coordination_bonus(self, agent_contributions):\n",
    "        \"\"\"Calculate bonus reward for good coordination between agents.\"\"\"\n",
    "        \n",
    "        # Reward agents for complementary improvements\n",
    "        performances = [contrib[\"performance\"] for contrib in agent_contributions.values()]\n",
    "        avg_performance = np.mean(performances)\n",
    "        \n",
    "        # Bonus for high average performance\n",
    "        coordination_bonus = 0.0\n",
    "        if avg_performance > 0.7:\n",
    "            coordination_bonus += 2.0\n",
    "        elif avg_performance > 0.5:\n",
    "            coordination_bonus += 1.0\n",
    "        \n",
    "        # Bonus for balanced contributions (avoid one agent dominating)\n",
    "        rewards = [contrib[\"reward\"] for contrib in agent_contributions.values()]\n",
    "        if len(rewards) > 1:\n",
    "            reward_std = np.std(rewards)\n",
    "            if reward_std < 2.0:  # Balanced contributions\n",
    "                coordination_bonus += 1.0\n",
    "        \n",
    "        return coordination_bonus\n",
    "    \n",
    "    def get_optimization_summary(self):\n",
    "        \"\"\"Get summary of multi-agent optimization.\"\"\"\n",
    "        \n",
    "        summary = {\n",
    "            \"total_episodes\": len(self.collaborative_rewards),\n",
    "            \"average_collaborative_reward\": np.mean(self.collaborative_rewards) if self.collaborative_rewards else 0,\n",
    "            \"best_episode_reward\": max(self.collaborative_rewards) if self.collaborative_rewards else 0,\n",
    "            \"agent_statistics\": {}\n",
    "        }\n",
    "        \n",
    "        # Get statistics for each agent\n",
    "        for agent_name, agent in self.agents.items():\n",
    "            if isinstance(agent, QLearningAgent):\n",
    "                agent_stats = agent.get_training_stats()\n",
    "            else:  # PolicyGradientAgent\n",
    "                agent_stats = agent.get_policy_summary()\n",
    "            \n",
    "            summary[\"agent_statistics\"][agent_name] = agent_stats\n",
    "        \n",
    "        # Coordination effectiveness\n",
    "        if self.coordination_history:\n",
    "            avg_coordination = np.mean([\n",
    "                np.mean([contrib[\"performance\"] for contrib in episode.values()]) \n",
    "                for episode in self.coordination_history\n",
    "            ])\n",
    "            summary[\"average_coordination_performance\"] = avg_coordination\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Test multi-agent RL optimization\n",
    "print_step(\"Testing Multi-Agent RL Optimization\")\n",
    "\n",
    "multi_agent_optimizer = MultiAgentRLOptimizer(\n",
    "    base_program=simple_qa,\n",
    "    train_examples=train_data,\n",
    "    eval_examples=eval_data,\n",
    "    metric_func=qa_metric\n",
    ")\n",
    "\n",
    "# Run collaborative optimization\n",
    "multi_agent_optimizer.optimize_collaboratively(num_episodes=20)\n",
    "\n",
    "# Get optimization summary\n",
    "optimization_summary = multi_agent_optimizer.get_optimization_summary()\n",
    "\n",
    "print_step(\"Multi-Agent Optimization Summary\")\n",
    "print(f\"  Total Episodes: {optimization_summary['total_episodes']}\")\n",
    "print(f\"  Average Collaborative Reward: {optimization_summary['average_collaborative_reward']:.3f}\")\n",
    "print(f\"  Best Episode Reward: {optimization_summary['best_episode_reward']:.3f}\")\n",
    "\n",
    "if \"average_coordination_performance\" in optimization_summary:\n",
    "    print(f\"  Average Coordination Performance: {optimization_summary['average_coordination_performance']:.3f}\")\n",
    "\n",
    "print_step(\"Individual Agent Performance\")\n",
    "for agent_name, stats in optimization_summary[\"agent_statistics\"].items():\n",
    "    print(f\"  {agent_name}:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296a79d",
   "metadata": {},
   "source": [
    "## RL-Based Hyperparameter Optimization\n",
    "\n",
    "Use RL to optimize DSPy hyperparameters automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLHyperparameterOptimizer:\n",
    "    \"\"\"RL-based hyperparameter optimization for DSPy programs.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_program_class, train_examples, eval_examples, metric_func):\n",
    "        self.base_program_class = base_program_class\n",
    "        self.train_examples = train_examples\n",
    "        self.eval_examples = eval_examples\n",
    "        self.metric_func = metric_func\n",
    "        \n",
    "        # Hyperparameter space\n",
    "        self.hyperparameter_space = {\n",
    "            \"temperature\": [0.0, 0.3, 0.7, 1.0],\n",
    "            \"max_tokens\": [100, 300, 500, 1000],\n",
    "            \"num_examples\": [1, 3, 5, 8],\n",
    "            \"optimization_method\": [\"basic\", \"bootstrap\", \"ensemble\"]\n",
    "        }\n",
    "        \n",
    "        # RL agent for hyperparameter selection\n",
    "        self.hp_agent = QLearningAgent(\n",
    "            state_size=4,  # Current performance, episode number, recent trend, complexity\n",
    "            action_size=len(self._enumerate_hyperparameter_combinations()),\n",
    "            learning_rate=0.15,\n",
    "            epsilon=0.8,\n",
    "            epsilon_decay=0.99\n",
    "        )\n",
    "        \n",
    "        # Optimization history\n",
    "        self.optimization_history = []\n",
    "        self.best_hyperparameters = None\n",
    "        self.best_performance = 0.0\n",
    "    \n",
    "    def _enumerate_hyperparameter_combinations(self):\n",
    "        \"\"\"Enumerate all possible hyperparameter combinations.\"\"\"\n",
    "        \n",
    "        combinations = []\n",
    "        \n",
    "        for temp in self.hyperparameter_space[\"temperature\"]:\n",
    "            for tokens in self.hyperparameter_space[\"max_tokens\"]:\n",
    "                for examples in self.hyperparameter_space[\"num_examples\"]:\n",
    "                    for method in self.hyperparameter_space[\"optimization_method\"]:\n",
    "                        combinations.append({\n",
    "                            \"temperature\": temp,\n",
    "                            \"max_tokens\": tokens,\n",
    "                            \"num_examples\": examples,\n",
    "                            \"optimization_method\": method\n",
    "                        })\n",
    "        \n",
    "        return combinations\n",
    "    \n",
    "    def _evaluate_hyperparameters(self, hyperparameters):\n",
    "        \"\"\"Evaluate a set of hyperparameters.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Create program with hyperparameters\n",
    "            program = self.base_program_class()\n",
    "            \n",
    "            # Simulate applying hyperparameters\n",
    "            # In practice, you would configure the actual DSPy program\n",
    "            performance_modifier = 1.0\n",
    "            \n",
    "            # Temperature effect\n",
    "            if hyperparameters[\"temperature\"] < 0.3:\n",
    "                performance_modifier *= 0.95  # Conservative\n",
    "            elif hyperparameters[\"temperature\"] > 0.7:\n",
    "                performance_modifier *= 0.9   # Too random\n",
    "            \n",
    "            # Token limit effect\n",
    "            if hyperparameters[\"max_tokens\"] < 200:\n",
    "                performance_modifier *= 0.8   # Too restrictive\n",
    "            elif hyperparameters[\"max_tokens\"] > 800:\n",
    "                performance_modifier *= 0.95  # Diminishing returns\n",
    "            \n",
    "            # Example count effect\n",
    "            if hyperparameters[\"num_examples\"] >= 5:\n",
    "                performance_modifier *= 1.1   # Good few-shot learning\n",
    "            elif hyperparameters[\"num_examples\"] <= 1:\n",
    "                performance_modifier *= 0.85  # Limited context\n",
    "            \n",
    "            # Optimization method effect\n",
    "            if hyperparameters[\"optimization_method\"] == \"ensemble\":\n",
    "                performance_modifier *= 1.15\n",
    "            elif hyperparameters[\"optimization_method\"] == \"bootstrap\":\n",
    "                performance_modifier *= 1.05\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for example in self.eval_examples[:5]:  # Use subset for speed\n",
    "                try:\n",
    "                    # Simulate prediction with hyperparameters\n",
    "                    # In practice, this would use the configured program\n",
    "                    base_prediction = program(**example.inputs())\n",
    "                    \n",
    "                    # Apply performance modifier\n",
    "                    if random.random() < performance_modifier:\n",
    "                        prediction_correct = self.metric_func(example, base_prediction)\n",
    "                    else:\n",
    "                        prediction_correct = False\n",
    "                    \n",
    "                    if prediction_correct:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "                    \n",
    "                except:\n",
    "                    total += 1\n",
    "            \n",
    "            performance = correct / total if total > 0 else 0.0\n",
    "            \n",
    "            return performance * performance_modifier\n",
    "            \n",
    "        except Exception as e:\n",
    "            print_error(f\"Hyperparameter evaluation failed: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def optimize_hyperparameters(self, num_episodes=50):\n",
    "        \"\"\"Optimize hyperparameters using RL.\"\"\"\n",
    "        \n",
    "        print_step(\"RL Hyperparameter Optimization\", f\"Optimizing for {num_episodes} episodes\")\n",
    "        \n",
    "        hyperparameter_combinations = self._enumerate_hyperparameter_combinations()\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # Current state (simplified)\n",
    "            current_performance = self.best_performance\n",
    "            episode_progress = episode / num_episodes\n",
    "            recent_trend = self._calculate_recent_trend()\n",
    "            complexity_estimate = 0.5  # Simplified\n",
    "            \n",
    "            state = {\n",
    "                \"performance_score\": current_performance,\n",
    "                \"steps_taken\": episode,\n",
    "                \"convergence_trend\": recent_trend,\n",
    "                \"program_complexity\": complexity_estimate\n",
    "            }\n",
    "            \n",
    "            # Choose hyperparameter combination\n",
    "            action = self.hp_agent.choose_action(state)\n",
    "            \n",
    "            if action < len(hyperparameter_combinations):\n",
    "                hyperparameters = hyperparameter_combinations[action]\n",
    "            else:\n",
    "                # Fallback to random combination\n",
    "                hyperparameters = random.choice(hyperparameter_combinations)\n",
    "            \n",
    "            # Evaluate hyperparameters\n",
    "            performance = self._evaluate_hyperparameters(hyperparameters)\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = self._calculate_hyperparameter_reward(performance, hyperparameters)\n",
    "            \n",
    "            # Update best performance\n",
    "            if performance > self.best_performance:\n",
    "                self.best_performance = performance\n",
    "                self.best_hyperparameters = hyperparameters.copy()\n",
    "            \n",
    "            # Record history\n",
    "            self.optimization_history.append({\n",
    "                \"episode\": episode,\n",
    "                \"hyperparameters\": hyperparameters,\n",
    "                \"performance\": performance,\n",
    "                \"reward\": reward,\n",
    "                \"is_best\": performance == self.best_performance\n",
    "            })\n",
    "            \n",
    "            # Learn from experience (simplified next state)\n",
    "            next_state = {\n",
    "                \"performance_score\": max(current_performance, performance),\n",
    "                \"steps_taken\": episode + 1,\n",
    "                \"convergence_trend\": self._calculate_recent_trend(),\n",
    "                \"program_complexity\": complexity_estimate\n",
    "            }\n",
    "            \n",
    "            done = episode == num_episodes - 1\n",
    "            self.hp_agent.learn(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Print progress\n",
    "            if episode % 10 == 0:\n",
    "                print(f\"Episode {episode}: Performance {performance:.3f}, Best {self.best_performance:.3f}\")\n",
    "                print(f\"  Hyperparameters: {hyperparameters}\")\n",
    "        \n",
    "        print_result(f\"Hyperparameter optimization completed!\")\n",
    "        print_result(f\"Best performance: {self.best_performance:.3f}\")\n",
    "        print_result(f\"Best hyperparameters: {self.best_hyperparameters}\")\n",
    "    \n",
    "    def _calculate_recent_trend(self):\n",
    "        \"\"\"Calculate recent performance trend.\"\"\"\n",
    "        if len(self.optimization_history) < 3:\n",
    "            return 0.0\n",
    "        \n",
    "        recent_performances = [h[\"performance\"] for h in self.optimization_history[-3:]]\n",
    "        return (recent_performances[-1] - recent_performances[0]) / 3\n",
    "    \n",
    "    def _calculate_hyperparameter_reward(self, performance, hyperparameters):\n",
    "        \"\"\"Calculate reward for hyperparameter choice.\"\"\"\n",
    "        \n",
    "        # Base reward from performance\n",
    "        reward = performance * 10\n",
    "        \n",
    "        # Bonus for improvement\n",
    "        if performance > self.best_performance:\n",
    "            improvement = performance - self.best_performance\n",
    "            reward += improvement * 20  # Bonus for improvement\n",
    "        \n",
    "        # Efficiency bonus for good configurations\n",
    "        if (hyperparameters[\"temperature\"] == 0.3 and \n",
    "            hyperparameters[\"num_examples\"] >= 3 and\n",
    "            hyperparameters[\"optimization_method\"] in [\"bootstrap\", \"ensemble\"]):\n",
    "            reward += 2.0\n",
    "        \n",
    "        # Penalty for extreme configurations\n",
    "        if hyperparameters[\"temperature\"] > 0.8 or hyperparameters[\"max_tokens\"] > 900:\n",
    "            reward -= 1.0\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def get_optimization_report(self):\n",
    "        \"\"\"Get comprehensive optimization report.\"\"\"\n",
    "        \n",
    "        if not self.optimization_history:\n",
    "            return {\"error\": \"No optimization history available\"}\n",
    "        \n",
    "        performances = [h[\"performance\"] for h in self.optimization_history]\n",
    "        \n",
    "        report = {\n",
    "            \"total_episodes\": len(self.optimization_history),\n",
    "            \"best_performance\": self.best_performance,\n",
    "            \"best_hyperparameters\": self.best_hyperparameters,\n",
    "            \"average_performance\": np.mean(performances),\n",
    "            \"performance_std\": np.std(performances),\n",
    "            \"improvement_episodes\": len([h for h in self.optimization_history if h[\"is_best\"]]),\n",
    "            \"final_epsilon\": self.hp_agent.epsilon,\n",
    "            \"hyperparameter_analysis\": self._analyze_hyperparameter_importance()\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _analyze_hyperparameter_importance(self):\n",
    "        \"\"\"Analyze which hyperparameters had the most impact.\"\"\"\n",
    "        \n",
    "        analysis = {}\n",
    "        \n",
    "        for param_name in self.hyperparameter_space.keys():\n",
    "            param_performance = defaultdict(list)\n",
    "            \n",
    "            for history_item in self.optimization_history:\n",
    "                param_value = history_item[\"hyperparameters\"][param_name]\n",
    "                param_performance[param_value].append(history_item[\"performance\"])\n",
    "            \n",
    "            # Calculate average performance for each parameter value\n",
    "            param_averages = {\n",
    "                value: np.mean(performances) \n",
    "                for value, performances in param_performance.items()\n",
    "            }\n",
    "            \n",
    "            analysis[param_name] = param_averages\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Test RL hyperparameter optimization\n",
    "print_step(\"Testing RL Hyperparameter Optimization\")\n",
    "\n",
    "hp_optimizer = RLHyperparameterOptimizer(\n",
    "    base_program_class=SimpleQA,\n",
    "    train_examples=train_data,\n",
    "    eval_examples=eval_data,\n",
    "    metric_func=qa_metric\n",
    ")\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "hp_optimizer.optimize_hyperparameters(num_episodes=30)\n",
    "\n",
    "# Get optimization report\n",
    "hp_report = hp_optimizer.get_optimization_report()\n",
    "\n",
    "print_step(\"Hyperparameter Optimization Report\")\n",
    "print(f\"  Best Performance: {hp_report['best_performance']:.3f}\")\n",
    "print(f\"  Best Hyperparameters: {hp_report['best_hyperparameters']}\")\n",
    "print(f\"  Average Performance: {hp_report['average_performance']:.3f}\")\n",
    "print(f\"  Improvement Episodes: {hp_report['improvement_episodes']}\")\n",
    "\n",
    "print_step(\"Hyperparameter Importance Analysis\")\n",
    "for param_name, param_analysis in hp_report[\"hyperparameter_analysis\"].items():\n",
    "    print(f\"  {param_name}:\")\n",
    "    sorted_values = sorted(param_analysis.items(), key=lambda x: x[1], reverse=True)\n",
    "    for value, avg_performance in sorted_values[:2]:  # Top 2 values\n",
    "        print(f\"    {value}: {avg_performance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa4b16",
   "metadata": {},
   "source": [
    "## Best Practices for RL Optimization in DSPy\n",
    "\n",
    "### Key Principles:\n",
    "\n",
    "1. **Environment Design**: Create meaningful state representations and reward functions\n",
    "2. **Action Space**: Define actions that correspond to actual DSPy optimizations\n",
    "3. **Exploration vs Exploitation**: Balance trying new approaches with using known good ones\n",
    "4. **Multi-Agent Coordination**: Use specialized agents for different optimization aspects\n",
    "5. **Hyperparameter Search**: Apply RL to automatically tune DSPy configurations\n",
    "\n",
    "### RL Algorithm Selection:\n",
    "\n",
    "- **Q-Learning**: Simple and effective for discrete action spaces\n",
    "- **Policy Gradient**: Better for continuous or complex action spaces\n",
    "- **Actor-Critic**: Combines benefits of value-based and policy-based methods\n",
    "- **Multi-Agent RL**: Coordinate multiple optimization objectives\n",
    "\n",
    "### Implementation Considerations:\n",
    "\n",
    "- **State Representation**: Include performance metrics, optimization history, and program characteristics\n",
    "- **Reward Engineering**: Design rewards that align with actual DSPy performance goals\n",
    "- **Sample Efficiency**: Use sample-efficient algorithms to minimize LM API calls\n",
    "- **Stability**: Implement safeguards against unstable optimization trajectories\n",
    "\n",
    "### Production Deployment:\n",
    "\n",
    "- **Online Learning**: Continue optimization during deployment\n",
    "- **Safety Constraints**: Ensure RL doesn't degrade critical performance metrics\n",
    "- **Monitoring**: Track RL agent behavior and intervention triggers\n",
    "- **Rollback Mechanisms**: Ability to revert to previous good configurations\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated experimental RL optimization techniques for DSPy:\n",
    "\n",
    "- **RL Environment**: Created environments for optimizing DSPy programs\n",
    "- **Q-Learning**: Implemented value-based RL for discrete optimization choices\n",
    "- **Policy Gradient**: Used policy-based methods for more complex optimization\n",
    "- **Multi-Agent RL**: Coordinated multiple agents for comprehensive optimization\n",
    "- **Hyperparameter Optimization**: Automated tuning of DSPy configurations\n",
    "\n",
    "These experimental techniques show promise for:\n",
    "- Automated optimization of complex DSPy programs\n",
    "- Dynamic adaptation to changing requirements\n",
    "- Multi-objective optimization (accuracy, speed, cost)\n",
    "- Continuous improvement during deployment\n",
    "\n",
    "While still experimental, RL-based optimization could become a powerful tool for automatically improving DSPy programs, especially for complex applications where manual optimization is challenging or time-consuming."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
